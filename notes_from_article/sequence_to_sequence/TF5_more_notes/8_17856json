"<pad> we found deep LSTMs to significantly outperform shallow LSTMs. each additional layer reduced perplexity by nearly 10%. the resulting LSTM has 384Mparameters of which 64M are pure recurrent connections. for each training batch, we computes = <unk> g<unk> 2, where g is the gradient divided by 128. if s > 5, we make sure all sentences in a minibatch are roughly of the same length, yielding a 2x speedup<extra_id_27>. a</s>"