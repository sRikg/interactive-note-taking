"<pad> in this paper, we present a general end-to-end approach to sequencelearning. our method uses a multilayered long short-term memory (LSTM) to map the input sequenceto a vector of a fixed dimensionality. the LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas-sive voice. reversing the order of the words in all sources (but not target sentences) improved the LSTM a a a</s>"