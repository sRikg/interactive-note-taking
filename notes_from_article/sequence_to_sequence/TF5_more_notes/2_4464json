"<pad> in this paper, we show that a straightforward application of the longshort-term memory (LSTM) architecture can solve general sequence to sequence problems. the idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation. a second LSTM is essentially a recurrent neural network language model except that it isconditioned on the input sequence. the output sequence is then mapped to the input sequence using the second LSTM.. a </s>"