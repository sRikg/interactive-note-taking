"<pad> in this section, we propose two new model architectures for learning distributed representations of words. most of the complexity is caused by the non-linear hidden layer in the model. the new architectures directly follow those proposed in our earlier work [13, 14]. the new models try to minimize computational complexity, but might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently, says daniel schmidt.</s>"