"<pad> the first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words. we call this architecture a bag-of-words model as the order of words in the history does not influence the projection. we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input. unlike standard bag-of-words model, it uses continuous distributed a a a a a a a a a a a a a a a a. a a a. a. a-a a a a a a...-a a a a. a....</s>"