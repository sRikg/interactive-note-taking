"<pad> to train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called distbelief. the framework allows us to run multiple replicas of the same model in parallel. each replica synchronizes its gradient updates through a centralized server that keeps all the parameters. for this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called adagrad.</s>"