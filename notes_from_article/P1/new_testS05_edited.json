"<pad> in this paper, we focus on distributed representations of words learned by neural networks. neural networks perform significantly better than LSA for preserving linear regularities among words. the training complexity is proportional to where is number of the training epochs, is the number of the words in the training set and is defined further for each model architecture. common choice is. up to one billion words in the training set. all models are trained using stochastic gradient descent and backpropagation.</s>"