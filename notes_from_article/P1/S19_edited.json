"<pad> in this paper, we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. using the distbelief distributed framework, it should be possible to train the CBOW and skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. results from machine translation experiments a a a a a a a en a a a a a. a. a a a a asa asase as as as as en a a a en a a ena a a sen</s>"