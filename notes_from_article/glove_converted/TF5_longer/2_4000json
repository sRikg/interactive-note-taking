"<pad> ning not the scalar distance between word vec- tors, but rather their various dimensions of dif- ference. currently, both main model families for learning word vectors suffer sig- nificant drawbacks. this work proposes a spe- cific weighted least squares model that trains on global word-word co-occurrence counts. the model pro- duces a word vector space with meaningful sub- structure, as evidenced by word analogy. a- a</s>"