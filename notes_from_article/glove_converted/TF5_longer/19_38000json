"<pad> the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus. this is likely due to the large number of city- and country-based analogies in the analogy dataset. a rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on per- formance. a single iteration takes 14 minutes for 300-dimensional vectors, and using all 32 cores of the above machine,  - - a</s>"