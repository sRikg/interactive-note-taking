"<pad> the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus. this is likely due to the large number of city- and country-based analogies in the analogy dataset. a rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on per- formance. for word2vec, the obvious choice would be the number of training epochs. unfortunately, the code for word2 a a a</s>"