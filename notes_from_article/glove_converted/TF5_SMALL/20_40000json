"<pad> in Fig. 4, we plot the overall performance on the word analogy task as a function of training time. overall accuracy is governed by the number of iterations for GloVe and by the number of negative samples for CBOW and skip-gram. in all cases, we train 300-dimensional vectors on the same 6B token corpus (wikipedia 2014 + Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10. in Fig. nn <extra_id_1> </s>"