"<pad> the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus. this is likely due to the large number of city- and country-based analogies in the analogy dataset. the total run-time is split between populating X and training the model. using a single thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric context window, a 400,000 word vocabulary.. a a</s>"