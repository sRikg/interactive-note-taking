"<pad> the Transformer is the first transduction model relying entirely on self-attention to compute representations of a sequence. it is the first transduction model relying entirely on self-attention to compute representations of a sequence. self-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. in the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions</s>"