"<pad> most competitive neural sequence transduction models have an encoder-decoder structure. the Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. the encoder maps an input sequence of symbol representations (x1,...,x_(n)) to a sequence of continuous representations z = (z1,...,z_(n)) the decoder then generates an output sequence (y1,...,y_(m)</s>"