"<pad> this paper compares various aspects of self-attention layers to recurrent and convolutional layers commonly used in sequence transduction. we consider three desiderata: total computational complexity per layer. another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. the shorter the paths between any two input and output positions, the easier it is to learn long-range dependencies in the network, we show in the paper.</s>"