"<pad> recurrent models typically factor computation along the symbol positions of the input and output sequences. this inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths. attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks. recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches.</s>"