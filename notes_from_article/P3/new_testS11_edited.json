"<pad> our model contains no recurrence and no convolution. to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. positional encodings have the same dimension d_(model) as the embeddings. in this work, we use sine and cosine functions of different frequencies: sin(pos/10000<unk> (2i/d_(model))) PE_((pos,2i + 1))</s>"