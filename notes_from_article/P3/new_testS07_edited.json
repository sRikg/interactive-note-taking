"<pad> multi-head attention consists of several attention layers running in parallel. instead of performing a single attention function, we linearly project the queries, keys and values h times with different, learned linear projections to d_(k), d_(k) and d_(v) dimensions. on each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d_(v)-dimensional output values.</s>"