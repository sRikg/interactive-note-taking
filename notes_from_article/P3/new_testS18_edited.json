"<pad> the Transformer is the first sequence transduction model based entirely on attention. it replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. for translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. the code we used to train and evaluate our models is available at https://github.com/tensor2tensor.</s>"