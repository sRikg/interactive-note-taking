"<pad> a total of 437,905<unk> ndiscrete features were generated for the conll-<unk> n2003 training dataset. we trained our model on five corpora of varying<unk> nsizes: a 2010 Wikipedia dump with 1 billion to-<unk> nkens; a 2014 Wikipedia dump with 1.6 billion to-<unk> nkens; Gigaword 5 which has 4.3 billion tokens; and the<unk> ncombination Gigaword5 + Wikipedia2014<unk> nthe analogy task.. a a</s>"