"<pad> 115 hours of meeting speech from ICSI, NIST, ISL and aMI training corpora were used to train the LMs. the RT05 LM was extended by additional CHIL and web data. to train the RNN LM, we selected in domain data that consists of meeting transcriptions and Switchboard corpus, for a total of 5.4M words. recurrent neural networks outperformed significantly state of the art backoff models in all our experiments, most notably even in a. a</s>"