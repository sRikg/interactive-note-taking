"<pad> we have used only up to 6.4M words for training RNN models (300K sentences) perplexity is evalu- ated on held-out data (230K words) in further experiments, we denote modified kneeser-ney smoothed 5-gram as KN5. to cor- rectly rescore n-best lists with backoff models that are trained on subset of data used by recognizer, we use open vocabulary language models. to improve results, outputs from a a a</s>"