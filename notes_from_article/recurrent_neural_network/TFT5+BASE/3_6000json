"<pad> recurrent neural networks do not use limited size of con- text. by using recurrent connections, information can cycle in- Copyright <unk> 2010 ISCA 1045 26 -30 September 2010, Makuhari, Chiba, japan. in our work, we have used an architecture that is usually called a simple recurrent neural network or Elman network. the network has an input layer x, hidden layer s (also called context layer s) and an output layer r (also called context layer<extra_id_27> re- a</s>"