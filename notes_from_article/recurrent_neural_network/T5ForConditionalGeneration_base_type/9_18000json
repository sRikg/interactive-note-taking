"<pad> Recurrent neural networks outperformed significantly state of the art backoff models in all our experiments. in WSJ experiments, word error rate reduction is around 18% for models trained on the same amount of data, and 12% when backoff model is trained on 5 times more data than RNN model. in WSJ experiments, word error rate reduction is around 18% for models trained on the same amount of data, and 12% when backoff model is trained on 5 times more data than RNN model. while WER is \u00ad\u00ad\u00ad\u00ad\u00ad</s>"