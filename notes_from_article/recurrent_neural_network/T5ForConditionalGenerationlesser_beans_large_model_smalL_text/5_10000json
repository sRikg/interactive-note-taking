"<pad> stan- dard backpropagation algorithm with stochastic gradient de- scent is used. learning rate <unk> is halved at start of each new epoch if no significant improvement is observed. network does not overtrain significantly, history is represented by hypothesis given by recognizer, and contains recognition errors. to overcome this sim- plity, backpropagation through time (BPTT) algorithm is commonly used. a major difference between feedforward neural net- works as used by Bengio - a a</s>"