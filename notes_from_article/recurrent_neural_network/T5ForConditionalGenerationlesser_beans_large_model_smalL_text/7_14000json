"<pad> we have used only up to 6.4M words for training RNN models (300K sentences) it takes several weeks to train the most complex models on large data. to improve results, outputs from various RNN LMs with different architectures can be linearly interpolated (diversity is also given by random weight initialization) the improvement keeps getting larger as the number of sentences is increased. the results are by no means among the largest improvements reported for the WSJ task ob- tained just by changing the a a a</s>"