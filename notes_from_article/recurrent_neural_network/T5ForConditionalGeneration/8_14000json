"<pad> we have used only up to 6.4M words for training RNN models (300K sentences) perplexity is evalu- ated on held-out data (230K words) in further experiments, we denote modified Kneser-Ney smoothed 5-gram as KN5. the results, reported in Tables 1 and 2, are by no means among the largest improvements reported for the WSJ task ob- tained just by changing the language modeling technique. but the improvement keeps getting larger with<extra_id_27> -<extra_id_27> -<extra_id_27> -</s>"