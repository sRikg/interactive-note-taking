"<pad> the training of deep networks can be posed as a non-convex optimization problem. given a sequence of learning rates, <unk> k, SGDiteratively takes steps of the formwk+1 = wk <unk> <unk> k<unk> f(wk), (1). for the specific task of neural language modeling, tradition-ally SGD without momentum has been found to outperformother algorithms by a sta-tistically significant margin. ASGD takes steps identical to equa-tion a a a</s>"