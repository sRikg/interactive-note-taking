"<pad> recurrent neural networks (RNNs), such as longshort-term memory networks (LSTMs), serve as a fundamental building block for many sequencelearning tasks. we propose the weight-droppedLSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regulariza-tion. using these and other reg-ularization strategies, we achieve state-of-the-artword level perplexities on two data sets: 57.3 onpenn treebank and 65. a a a</s>"