"<pad> training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs. we have implemented various models in a distributed framework called distbelief. we used 50 to 100 model replicas during the training. the number of CPU cores is an<unk> n8<unk> n<unk> nTable 6: Comparison of models trained using the distbelief distributed framework<unk> n<unk> nNote that training of NNLM with 1000- a a a</s>"