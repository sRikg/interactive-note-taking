"<pad> we consider the standard Trans-former architecture, as proposed invaswani et al. (2017). each layer of the Transformer containstwo primary sub-layers: anattention layer and a feedforwardlayer. the output of each sub-layer is fed directly into the following layernormalization. the output of theadapter is then passed directly into the following layernormalization. a bottle-neck architecture is proposed to limit the number of parameters added per task - in practice, we use around0.5 <unk> a a a</s>"