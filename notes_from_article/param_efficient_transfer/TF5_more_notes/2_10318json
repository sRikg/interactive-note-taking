"<pad> adapter-based tuning relates to multi-task and continuallearning. adapters differ in that thetasks do not interact and the sharedparameters are frozen. on the GLUEbenchmark, our strategy almost matches the performance of the fully fine-tunedbERT, but uses only 3% task-specificparameters. the key innovation is to design an effective adaptermodule and itsintegration with the base model. a simple yet effective, bottleneck architecture is proposed for tuning .. a</s>"