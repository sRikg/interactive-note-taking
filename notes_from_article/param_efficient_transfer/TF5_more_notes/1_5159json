"<pad> fine-tuning large pre-trained models is an effec-tive transfer mechanism in NLP. however, in thepresence of many downstream tasks, fine-tuningis parameter inefficient. adapter modules yield a compact and extensible model; they add only a few trainable parameters per task. on GLUE, we attain within 0.4% of the performanceof full fine-tuning, adding only 3.6% parameters per task - compared to fine-tuning which trains 100% a a a</s>"