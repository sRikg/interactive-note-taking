"<pad> dropout has a tunable hyperparameter p (the probability of retaining a unit in the network) we explore the effect of varying this hyperparameter in two situations. if the architecture is held constant, having a small p means very few units will turnon during training. if the architecture is changed so that the expected number of hidden unitswill be retained after dropout is held constant, the test error goes down. if the architecture is held constant, having a small p means a a a</s>"