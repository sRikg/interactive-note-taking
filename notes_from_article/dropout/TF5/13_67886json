"<pad> ow values of p mean more dropout. for real-valued inputs, a typical value is 0.8. for hidden layers, the choice of p is coupledwith the choice of number of hidden units n. smaller p requires big n which slows down the training and leads tounderfitting. large p may not produce enough dropout to prevent overfitting. for all architectures, p is a function of n, i.e., the number of hidden units in each a a n</s>"