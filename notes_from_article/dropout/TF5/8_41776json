"<pad> dropout prevents overfitting without the need to do dimensionality reduction. dropout combined with max-norm regularization gives thelowest generalization error. dropout affects the sparsity of hidden unit activations in a neural network. the dropout nets are very large (1000s of hidden units) compared to a few tens of units in the Bayesian network - this shows that dropout has a strong regularizing effect. dropout can be seen as another way a a a</s>"