"<pad> dropout neural nets can be trained using stochastic gradient descent. for each training case in a mini-batch, we sample a thinned network by dropping out units. any training case which does not use a parameter contributes a gradient of zero. the gradients for each parameter are averaged over the training cases in each mini-batch. a particular form of regularization was found to be especially useful for dropout - constraining the norm of the incoming weight vector at each a a a</s>"