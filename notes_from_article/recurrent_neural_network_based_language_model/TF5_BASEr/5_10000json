"<pad> in our experiments, networks do not overtrain significantly, history is represented by hypothesis given by recognizer, and contains recognition errors. one of major differences between feedforward neural net- works as used by Bengio [3] and Schwenk [4] is in amount of parameters that need to be tuned or selected ad hoc before training. for recurrent neural net- works, one needs to tune the size of hidden layer, the size of hidden layer and the context-length. to improve performance, we merge all words- a- a</s>"