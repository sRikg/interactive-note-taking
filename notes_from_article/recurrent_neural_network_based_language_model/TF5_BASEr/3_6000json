"<pad> recurrent neural networks do not use limited size of con- text. by using recurrent connections, information can cycle in- copyright <unk> 2010 ISCA 1045 26 -30 September 2010, Makuhari, chiba, japan. in our work, we have used an architecture that is usually called a simple recurrent neural network or Elman network. the network has an input layer x, hidden layer s (also called context layer or state) and output layer y. at each training step- a- a</s>"