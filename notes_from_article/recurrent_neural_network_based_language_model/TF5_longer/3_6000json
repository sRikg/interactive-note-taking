"<pad> recurrent neural networks do not use limited size of context. by using recurrent connections, information can cycle in- s(t <unk> 1) for arbitrarily long time. recurrent neural networks can be used to predict a wide range of words. they can also encode temporal information implicitly for contexts with arbitrary lengths. in our work, we have used an architecture that is usually called a simple recurrent neural network or Elman network, and very easy to implement and train. - a- - a- a- a- - a- a- a- na- na- a- a a a no- no- na na a a en- in-a-a- a- en- a- a- a-</s>"