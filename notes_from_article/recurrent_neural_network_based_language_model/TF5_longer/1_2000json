"<pad> a new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. speech recognition experiments show around 18% reduction of word error rate on the wall street journal task, and around 5% on the much harder NIST RT05 task. we provide ample empiri- cal evidence to a n- a a a- a- a- a- a- a a a a a a a a a a a a a- a a a a a a a a a a a a- a- en-ena- </s>"