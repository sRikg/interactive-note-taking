"<pad> o train the network, we use the stan- dard backpropagation algorithm with stochastic gradient de- scent. in our experiments, networks do not overtrain significantly, history is represented by hypothesis given by recognizer, and contains recognition errors. one of major differences between feedforward neural net- works as used by Bengio [3] and Schwenk [4] is in amount of parameters that need to be selected ad hoc before training. to improve performance, we merge all words that occur less often- a- a</s>"