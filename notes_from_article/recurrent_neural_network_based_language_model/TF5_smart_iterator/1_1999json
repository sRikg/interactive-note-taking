"<pad> a new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. speech recognition experiments show around 18% reduction of word error rate on the wall street journal task, and around 5% on the much harder NIST RT05 task. the goal of statistical language modeling is to predict the next word in<extra_id_27> - a a</s>"