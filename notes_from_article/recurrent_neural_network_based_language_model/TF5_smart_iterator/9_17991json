"<pad> recurrent neural networks outperformed significantly state of the art backoff models in all our experiments. word error rate reduction is around 18% for models trained on the same amount of data, and 12% when backoff model is trained on 5 times more data than RNN LMs. while word error rate reduction is around 18% for models trained on the same amount of data, and 12% when backoff model is trained on 5 times more data than RNN LMs. in WSJ experiments, word error rate reduction a a a</s>"