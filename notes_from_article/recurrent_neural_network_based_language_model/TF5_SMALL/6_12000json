"<pad> vocabulary growth increases the input and output layer sizes, and also the optimal hidden layer size increases with more training data. in some experiments we have achieved almost twice perplexity reduction over n-gram models by using a recurrent network instead of a feedforward network. for comparison, it takes around 6 hours for our basic implementation to train RNN model based on Brown corpus (800K words, 100 hidden units and vocab- ulary threshold 5), while Bengio reports 113 days for basic implementation and 26 \u00ad\u00ad\u00ad\u00ad\u00ad</s>"