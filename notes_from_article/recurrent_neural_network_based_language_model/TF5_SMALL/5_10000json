"<pad> ain the network, we use the stan- dard backpropagation algorithm with stochastic gradient de- scent. if log-likelihood of validation data increases, training continues in new epoch. if no significant improvement is observed, learning rate <unk> is halved at start of each new epoch. after there is again no signifi- cant improvement, training is finished. in our experiments, networks do not overtrain significantly, history is represented by. \u00ad\u00ad </s>"