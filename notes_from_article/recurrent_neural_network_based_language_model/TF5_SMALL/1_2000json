"<pad> a new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. re- sults indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. speech recognition experiments show around 18% reduction of word error rate when comparing models trained on the same amount of data, and around 5% on the harder NIST RT05 task... </s>"