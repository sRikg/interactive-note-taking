"<pad> configurations of neural network LMs, such as RNN 90/2, indicate that the hidden layer size is 90 and threshold for merging words to rare token is 2. to cor- rectly rescore n-best lists with backoff models that are trained on subset of data used by recognizer, we use open vocabulary language models (unknown words are assigned small probabil- ity) the results, reported in Tables 1 and 2, are by no means among the largest improvements reported for the WSJ task gra\u00ad \u00ad\u00ad\u00ad</s>"