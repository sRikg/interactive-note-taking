"<pad> n is the sequence length, d is the representation dimension, k is the kernelsize of convolutions and r is the size of the neighborhood in restricted self-attention. we use learned embeddings to convert the input and output tokens to vectors of dimension dmodel. we also use the usual learned linear transfor-mation and softmax function to convert the decoder output to predicted next-token probabilities. n is the maximum path length, d is the representation dimension,. a a</s>"