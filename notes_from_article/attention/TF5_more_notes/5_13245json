"<pad> multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions. the transformer uses multi-head attention in three different ways:\u2022 in \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\u2022 in a self-attention layer all of the keys, values and queries come from the output of the encoder.\u2022 in a decoder layer all of the keys, a a a</s>"