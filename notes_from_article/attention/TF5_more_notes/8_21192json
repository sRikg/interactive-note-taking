"<pad> separable convolutions decrease the complexity to O(k <unk> n <unk> d + n <unk> d2). as side benefit, self-attention could yield more interpretable models. we trained our models on one machine with 8 NVIDIAp100 GPUs. the big models were trained for 300,000 steps (3.5 days) and took about 3.5 hours to complete a training session. a previous version of this paper used a convolutional neural network (cnn) a  a a</s>"