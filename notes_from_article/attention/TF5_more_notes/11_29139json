"<pad> the Transformer is the first sequence transduction model based entirely onattention. it can be trained significantly faster thanarchitectures based on recurrent or convolutional neural networks. the Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences. for translation tasks, the Transformer can be trained significantly faster thanarchitectures based on recurrent or convolutional neural networks [8\u201310]. despite the lack of task-specific tuning our model perform a a a</s>"