"<pad> recurrent models typically factor computation along the symbol positions of the input and output sequences. this inherentlysequential nature precludes parallelization within training examples. in this work we propose the Transformer, a modelarchitecture eschewing recurrence. the Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs. hile at google brain.<unk> Work performed while at google.. a</s>"