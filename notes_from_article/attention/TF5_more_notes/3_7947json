"<pad> transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. self-attention has been used successfully in a variety of tasks including reading comprehension, textual entailment and learning task-independent sentence representations. the Transformer follows an overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder. the encoder is composed of a stack a a a</s>"