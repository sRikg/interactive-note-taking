"<pad> self-attention layers are faster than recurrent layers when the sequencelength n is smaller than the representation dimensionality d. a self-attention layer connects all positions with a constant number of sequentiallyexecuted operations, whereas a recurrent layer requires O(n) sequential operations. self-attention could be restricted toconsidering only a neighborhood of size r in the input sequence centered around the respective output position. this would increase the maximum path length to O(n/r) a. a</s>"