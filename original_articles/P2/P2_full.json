{"[1409.3215] Sequence to Sequence Learningwith Neural Networks\n\nSequence to Sequence Learning with Neural Networks\n\nIlya Sutskever\nGoogle\nilyasu@google.com\n&Oriol Vinyals\nGoogle\nvinyals@google.com\n&Quoc V. Le\nGoogle\nqvl@google.com\n\nAbstract\n\nDeep Neural Networks (DNNs) are powerful models that have achieved\nexcellent performance on difficult learning tasks. Although DNNs work\nwell whenever large labeled training sets are available, they cannot be\nused to map sequences to sequences. In this paper, we present a general\nend-to-end approach to sequence learning that makes minimal assumptions\non the sequence structure. Our method uses a multilayered Long\nShort-Term Memory (LSTM) to map the input sequence to a vector of a\nfixed dimensionality, and then another deep LSTM to decode the target\nsequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT-14 dataset, the translations\nproduced by the LSTM achieve a BLEU score of 34.8 on the entire test\nset, where the LSTM\u2019s BLEU score was penalized on out-of-vocabulary\nwords. Additionally, the LSTM did not have difficulty on long sentences.\nFor comparison, a phrase-based SMT system achieves a BLEU score of 33.3\non the same dataset. When we used the LSTM to rerank the 1000 hypotheses\nproduced by the aforementioned SMT system, its BLEU score increases to\n36.5, which is close to the previous state of the art. The LSTM also\nlearned sensible phrase and sentence representations that are sensitive\nto word order and are relatively invariant to the active and the passive\nvoice. Finally, we found that reversing the order of the words in all\nsource sentences (but not target sentences) improved the LSTM\u2019s\nperformance markedly, because doing so introduced many short term\ndependencies between the source and the target sentence which made the\noptimization problem easier.\n\n1 Introduction\n\nDeep Neural Networks (DNNs) are extremely powerful machine learning\nmodels that achieve excellent performance on difficult problems such as\nspeech recognition [13, 7] and visual object recognition [19, 6, 21,\n20]. DNNs are powerful because they can perform arbitrary parallel\ncomputation for a modest number of steps. A surprising example of the\npower of DNNs is their ability to sort N \u200b\u200b\u200b\u200b\u2003N-bit numbers using only 2\nhidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate\ncomputation. Furthermore, large DNNs can be trained with supervised\nbackpropagation whenever the labeled training set has enough information\nto specify the network\u2019s parameters. Thus, if there exists a parameter\nsetting of a large DNN that achieves good results (for example, because\nhumans can solve the task very rapidly), supervised backpropagation will\nfind these parameters and solve the problem.\n\nDespite their flexibility and power, DNNs can only be applied to\nproblems whose inputs and targets can be sensibly encoded with vectors\nof fixed dimensionality. It is a significant limitation, since many\nimportant problems are best expressed with sequences whose lengths are\nnot known a-priori. For example, speech recognition and machine\ntranslation are sequential problems. Likewise, question answering can\nalso be seen as mapping a sequence of words representing the question to\na sequence of words representing the answer. It is therefore clear that\na domain-independent method that learns to map sequences to sequences\nwould be useful.\n\nSequences pose a challenge for DNNs because they require that the\ndimensionality of the inputs and outputs is known and fixed. In this\npaper, we show that a straightforward application of the Long Short-Term\nMemory (LSTM) architecture [16] can solve general sequence to sequence\nproblems. The idea is to use one LSTM to read the input sequence, one\ntimestep at a time, to obtain large fixed-dimensional vector\nrepresentation, and then to use another LSTM to extract the output\nsequence from that vector (fig.\u00a01). The second LSTM is essentially a\nrecurrent neural network language model [28, 23, 30] except that it is\nconditioned on the input sequence. The LSTM\u2019s ability to successfully\nlearn on data with long range temporal dependencies makes it a natural\nchoice for this application due to the considerable time lag between the\ninputs and their corresponding outputs (fig.\u00a01).\n\nThere have been a number of related attempts to address the general\nsequence to sequence learning problem with neural networks. Our approach\nis closely related to Kalchbrenner and Blunsom [18] who were the first\nto map the entire input sentence to vector, and is very similar to Cho\net al.\u00a0[5]. Graves [10] introduced a novel differentiable attention\nmechanism that allows neural networks to focus on different parts of\ntheir input, and an elegant variant of this idea was successfully\napplied to machine translation by Bahdanau et al.\u00a0[2]. The Connectionist\nSequence Classification is another popular technique for mapping\nsequences to sequences with neural networks, although it assumes a\nmonotonic alignment between the inputs and the outputs [11].\n\n[Figure 1: Our model reads an input sentence \u201cABC\u201d and produces \u201cWXYZ\u201d\nas the output sentence. The model stops making predictions after\noutputting the end-of-sentence token. Note that the LSTM reads the input\nsentence in reverse, because doing so introduces many short term\ndependencies in the data that make the optimization problem much\neasier.]\n\nThe main result of this work is the following. On the WMT\u201914 English to\nFrench translation task, we obtained a BLEU score of 34.81 by directly\nextracting translations from an ensemble of 5 deep LSTMs (with 380M\nparameters each) using a simple left-to-right beam-search decoder. This\nis by far the best result achieved by direct translation with large\nneural networks. For comparison, the BLEU score of a SMT baseline on\nthis dataset is 33.30 [29]. The 34.81 BLEU score was achieved by an LSTM\nwith a vocabulary of 80k words, so the score was penalized whenever the\nreference translation contained a word not covered by these 80k. This\nresult shows that a relatively unoptimized neural network architecture\nwhich has much room for improvement outperforms a mature phrase-based\nSMT system.\n\nFinally, we used the LSTM to rescore the publicly available 1000-best\nlists of the SMT baseline on the same task [29]. By doing so, we\nobtained a BLEU score of 36.5, which improves the baseline by 3.2 BLEU\npoints and is close to the previous state-of-the-art (which is 37.0\n[9]).\n\nSurprisingly, the LSTM did not suffer on very long sentences, despite\nthe recent experience of other researchers with related architectures\n[26]. We were able to do well on long sentences because we reversed the\norder of words in the source sentence but not the target sentences in\nthe training and test set. By doing so, we introduced many short term\ndependencies that made the optimization problem much simpler (see sec.\u00a02\nand 3.3). As a result, SGD could learn LSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source\nsentence is one of the key technical contributions of this work.\n\nA useful property of the LSTM is that it learns to map an input sentence\nof variable length into a fixed-dimensional vector representation. Given\nthat translations tend to be paraphrases of the source sentences, the\ntranslation objective encourages the LSTM to find sentence\nrepresentations that capture their meaning, as sentences with similar\nmeanings are close to each other while different sentences meanings will\nbe far. A qualitative evaluation supports this claim, showing that our\nmodel is aware of word order and is fairly invariant to the active and\npassive voice.\n\n2 The model\n\nThe Recurrent Neural Network (RNN) [31, 28] is a natural generalization\nof feedforward neural networks to sequences. Given a sequence of inputs\n(x\u2081,\u2026,x_(T)), a standard RNN computes a sequence of outputs (y\u2081,\u2026,y_(T))\nby iterating the following equation:\n\n  -- ------- --- ----------------------------------- --\n     h_(t)   ": null, " accessed 03-September-2014].\n-   [30] M.\u00a0Sundermeyer, R.\u00a0Schluter, and H.\u00a0Ney. LSTM neural networks\n    for language modeling. In INTERSPEECH, 2010.\n-   [31] P.\u00a0Werbos. Backpropagation through time: what it does and how\n    to do it. Proceedings of IEEE, 1990.\n\n\u25c4 [ar5iv homepage] Feeling\nlucky? Conversion\nreport Report\nan issue View\u00a0original\non\u00a0arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Mon Dec 27 04:18:10 2021 by LaTeXML [[LOGO]]": null}