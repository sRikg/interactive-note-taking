Setting Winograd Winogrande (XL) - Fine-tuned SOTA 90.1^(a) 84.6^(b) GPT-3 Zero-Shot 88.3* 70.2 GPT-3 One-Shot 89.7* 73.2 GPT-3 Few-Shot 88.6* 77.7 Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4.1 for details on potential contamination of the Winograd test set. ^(a)[107] ^(b)[71] [Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large.] The Winograd Schemas Challenge [58] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions such as the adversarially-mined Winogrande dataset [107] still significantly lag human performance. We test GPT-3\u2019s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting. On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \u201cpartial evaluation\u201d method described in [106]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4.1). On the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves 70.2% in the zero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a fine-tuned RoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by [107] is 94.0%.