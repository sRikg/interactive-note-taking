Several lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters [44]. One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size: 213 million parameters [123] in the original paper, 300 million parameters [16], 1.5 billion parameters [106], 8 billion parameters [114], 11 billion parameters [105], and most recently 17 billion parameters [121]. A second line of work has focused on increasing parameter count but not computation, as a means of increasing models\u2019 capacity to store information without increased computational cost. These approaches rely on the conditional computation framework [7] and specifically, the mixture-of-experts method [113] has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models [3], though only a small fraction of the parameters are actually used on each forward pass. A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time [28] and the universal transformer [17]. Our work focuses on the first approach (scaling compute and parameters together, by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ this strategy. Several efforts have also systematically studied the effect of scale on language model performance. [49, 103, 69, 35], find a smooth power-law trend in loss as autoregressive language models are scaled up. This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure 3.1), and we also find relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling. Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language models that are as small as possible. This approach includes ALBERT [54] as well as general [37] and task-specific [110, 45, 51] approaches to distillation of language models. These architectures and techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint of giant models. As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering [50, 40, 11, 76], reading comprehension [13, 95], and adversarially constructed datasets designed to be difficult for existing language models [107, 86]. In this work we test our models on many of these datasets. Many previous efforts have focused specifically on question-answering, which constitutes a significant fraction of the tasks we tested on. Recent efforts include\u00a0[105, 104], which fine-tuned an 11 billion parameter language model, and\u00a0[27], which focused on attending over a large corpus of data at test time. Our work differs in focusing on in-context learning but could be combined in the future with those of [27, 68]. Metalearning in language models has been utilized in [106], though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including matching networks\u00a0[122], RL2 [20], learning to optimize\u00a0[98, 1, 66] and MAML [24]. Our approach of stuffing the model\u2019s context with previous examples is most structurally similar to RL2 and also resembles\u00a0[38], in that an inner loop of adaptation takes place through computation in the model\u2019s activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks defined at inference-time. Few-shot auto-regressive density estimation was explored in \u00a0[96] and\u00a0[31] studied low-resource NMT as a few-shot learning problem. While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning [115]. Another sub-field with similar goals is semi-supervised learning where approaches such as UDA [126] also explore methods of fine-tuning when very little labeled data is available. Giving multi-task models instructions in natural language was first formalized in a supervised setting with [79] and utilized for some tasks (such as summarizing) in a language model with [106]. The notion of presenting tasks in natural language was also explored in the text-to-text transformer [105], although there it was applied for multi-task fine-tuning rather than for in-context learning without weight updates. Another approach to increasing generality and transfer-learning capability in language models is multi-task learning [9], which fine-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample efficiency when updating the weights for a new task. Multi-task learning has shown some promising initial results\u00a0[60, 56] and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets [88] and pushed the boundaries on certain tasks [47], but is still limited by the need to manually curate collections of datasets and set up training curricula. By contrast pre-training at large enough scale appears to offer a \u201cnatural\u201d broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation [117], human interaction [132], or active learning [72]. Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality [16], prefixLM [19] and encoder-decoder architectures [65, 105], random permutations during training [128], architectures that improve the efficiency of sampling [22], improvements in data and training procedures [67], and efficiency increases in the embedding parameters [54]. Many of these techniques provide significant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3\u2019s performance on downstream tasks, especially in the fine-tuning setting, and combining GPT-3\u2019s scale with these algorithmic techniques is a promising direction for future work.evelopment of adaptable, general language systems.