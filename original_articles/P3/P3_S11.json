Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d_(model) as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (JonasFaceNet2017, ). In this work, we use sine and cosine functions of different frequencies:   -- --------------------------------------------------- --      PE_((pos,2i))\u2004=\u2004sin(pos/10000^(2i/d_(model)))             PE_((pos,2i\u2005+\u20051))\u2004=\u2004cos(pos/10000^(2i/d_(model)))    where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000\u2005\u22c5\u20052\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_(pos\u2005+\u2005k) can be represented as a linear function of PE_(pos). We also experimented with using learned positional embeddings (JonasFaceNet2017, ) instead, and found that the two versions produced nearly identical results (see Table\u00a03 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.