"[Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.] Instead of performing a single attention function with d_(model)-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d_(k), d_(k) and d_(v) dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d_(v)-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure\u00a02. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.   -- ------------------ ------------------------------------------------ --      MultiHead(Q,K,V)   \u2004=\u2004Concat(head\u2081,\u2026,head_(h))W^(O)                       wherehead_(i)      \u2004=\u2004Attention(QW_(i)^(Q),KW_(i)^(K),VW_(i)^(V))    Where the projections are parameter matrices W_(i)^(Q)\u2004\u2208\u2004\u211d^(d_(model)\u2005\u00d7\u2005d_(k)), W_(i)^(K)\u2004\u2208\u2004\u211d^(d_(model)\u2005\u00d7\u2005d_(k)), W_(i)^(V)\u2004\u2208\u2004\u211d^(d_(model)\u2005\u00d7\u2005d_(v)) and W^(O)\u2004\u2208\u2004\u211d^(hd_(v)\u2005\u00d7\u2005d_(model)). In this work we employ h\u2004=\u20048 parallel attention layers, or heads. For each of these we use d_(k)\u2004=\u2004d_(v)\u2004=\u2004d_(model)/h\u2004=\u200464. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality."
