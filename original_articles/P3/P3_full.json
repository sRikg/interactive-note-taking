{"[1706.03762] Attention Is All You Need": null, "Attention Is All You Need": null, "\\ANDAshish Vaswani": null, "Google Brain": null, "avaswani@google.com": null, "&Noam Shazeer\u00b9\u00b9footnotemark: 1": null, "noam@google.com": null, "&Niki Parmar\u00b9\u00b9footnotemark: 1": null, "Google Research": null, "nikip@google.com": null, "&Jakob Uszkoreit\u00b9\u00b9footnotemark: 1": null, "usz@google.com": null, "&Llion Jones\u00b9\u00b9footnotemark: 1": null, "llion@google.com": null, "&Aidan N. Gomez\u00b9\u00b9footnotemark: 1\u00a0\u00a0": null, "University of Toronto": null, "aidan@cs.toronto.edu &\u0141ukasz Kaiser\u00b9\u00b9footnotemark: 1": null, "lukaszkaiser@google.com": null, "&Illia Polosukhin\u00b9\u00b9footnotemark: 1\u00a0\u00a0": null, "illia.polosukhin@gmail.com": null, "Equal contribution. Listing order is random.Work performed while at": null, "Google Brain.Work performed while at Google Research.": null, "Abstract": null, "The dominant sequence transduction models are based on complex recurrent": null, "or convolutional neural networks that include an encoder and a decoder.": null, "The best performing models also connect the encoder and decoder through": null, "an attention mechanism. We propose a new simple network architecture,": null, "the Transformer, based solely on attention mechanisms, dispensing with": null, "recurrence and convolutions entirely. Experiments on two machine": null, "translation tasks show these models to be superior in quality while": null, "being more parallelizable and requiring significantly less time to": null, "train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German": null, "translation task, improving over the existing best results, including": null, "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation": null, "task, our model establishes a new single-model state-of-the-art BLEU": null, "score of 41.0 after training for 3.5 days on eight GPUs, a small": null, "fraction of the training costs of the best models from the literature.": null, "We show that the Transformer generalizes well to other tasks by applying": null, "it successfully to English constituency parsing both with large and": null, "limited training data. ^(\u2020)^(\u2020)Code available at": null, "https://github.com/tensorflow/tensor2tensor": null, "1 Introduction": null, "Recurrent neural networks, long short-term memory (hochreiter1997, ) and": null, "gated recurrent (gruEval14, ) neural networks in particular, have been": null, "firmly established as state of the art approaches in sequence modeling": null, "and transduction problems such as language modeling and machine": null, "translation (sutskever14, ; bahdanau2014neural, ; cho2014learning, ).": null, "Numerous efforts have since continued to push the boundaries of": null, "recurrent language models and encoder-decoder architectures": null, "(wu2016google, ; luong2015effective, ; jozefowicz2016exploring, ).": null, "Recurrent models typically factor computation along the symbol positions": null, "of the input and output sequences. Aligning the positions to steps in": null, "computation time, they generate a sequence of hidden states h_(t), as a": null, "function of the previous hidden state h_(t\u2005\u2212\u20051) and the input for": null, "position t. This inherently sequential nature precludes parallelization": null, "within training examples, which becomes critical at longer sequence": null, "lengths, as memory constraints limit batching across examples. Recent": null, "work has achieved significant improvements in computational efficiency": null, "through factorization tricks (Kuchaiev2017Factorization, ) and": null, "conditional computation (shazeer2017outrageously, ), while also": null, "improving model performance in case of the latter. The fundamental": null, "constraint of sequential computation, however, remains.": null, "Attention mechanisms have become an integral part of compelling sequence": null, "modeling and transduction models in various tasks, allowing modeling of": null, "dependencies without regard to their distance in the input or output": null, "sequences (bahdanau2014neural, ; structuredAttentionNetworks, ). In all": null, "but a few cases (decomposableAttnModel, ), however, such attention": null, "mechanisms are used in conjunction with a recurrent network.": null, "In this work we propose the Transformer, a model architecture eschewing": null, "recurrence and instead relying entirely on an attention mechanism to": null, "draw global dependencies between input and output. The Transformer": null, "allows for significantly more parallelization and can reach a new state": null, "of the art in translation quality after being trained for as little as": null, "twelve hours on eight P100 GPUs.": null, "2 Background": null, "The goal of reducing sequential computation also forms the foundation of": null, "the Extended Neural GPU (extendedngpu, ), ByteNet (NalBytenet2017, ) and": null, "ConvS2S (JonasFaceNet2017, ), all of which use convolutional neural": null, "networks as basic building block, computing hidden representations in": null, "parallel for all input and output positions. In these models, the number": null, "of operations required to relate signals from two arbitrary input or": null, "output positions grows in the distance between positions, linearly for": null, "ConvS2S and logarithmically for ByteNet. This makes it more difficult to": null, "learn dependencies between distant positions (hochreiter2001gradient, ).": null, "In the Transformer this is reduced to a constant number of operations,": null, "albeit at the cost of reduced effective resolution due to averaging": null, "attention-weighted positions, an effect we counteract with Multi-Head": null, "Attention as described in section\u00a03.2.": null, "Self-attention, sometimes called intra-attention is an attention": null, "mechanism relating different positions of a single sequence in order to": null, "compute a representation of the sequence. Self-attention has been used": null, "successfully in a variety of tasks including reading comprehension,": null, "abstractive summarization, textual entailment and learning": null, "task-independent sentence representations (cheng2016long, ;": null, "decomposableAttnModel, ; paulus2017deep, ; lin2017structured, ).": null, "To the best of our knowledge, however, the Transformer is the first": null, "transduction model relying entirely on self-attention to compute": null, "representations of its input and output without using RNNs or": null, "convolution. In the following sections, we will describe the": null, "Transformer, motivate self-attention and discuss its advantages over": null, "models such as (neural_gpu, ; NalBytenet2017, ) and (JonasFaceNet2017,": null, ").": null, "3 Model Architecture": null, "[Figure 1: The Transformer - model architecture.]": null, "Most competitive neural sequence transduction models have an": null, "encoder-decoder structure (cho2014learning, ; bahdanau2014neural, ;": null, "sutskever14, ). Here, the encoder maps an input sequence of symbol": null, "representations (x\u2081,\u2026,x_(n)) to a sequence of continuous representations": null, "z\u2004=\u2004(z\u2081,\u2026,z_(n)). Given z, the decoder then generates an output sequence": null, "(y\u2081,\u2026,y_(m)) of symbols one element at a time. At each step the model is": null, "auto-regressive (graves2013generating, ), consuming the previously": null, "generated symbols as additional input when generating the next.": null, "The Transformer follows this overall architecture using stacked": null, "self-attention and point-wise, fully connected layers for both the": null, "encoder and decoder, shown in the left and right halves of Figure\u00a01,": null, "respectively.": null, "3.1 Encoder and Decoder Stacks": null, "Encoder:": null, "The encoder is composed of a stack of N\u2004=\u20046 identical layers. Each layer": null, "has two sub-layers. The first is a multi-head self-attention mechanism,": null, "and the second is a simple, position-wise fully connected feed-forward": null, "network. We employ a residual connection (he2016deep, ) around each of": null, "the two sub-layers, followed by layer normalization layernorm2016 . That": null, "is, the output of each sub-layer is LayerNorm(x+Sublayer(x)), where": null, "Sublayer(x) is the function implemented by the sub-layer itself. To": null, "facilitate these residual connections, all sub-layers in the model, as": null, "well as the embedding layers, produce outputs of dimension": null, "d_(model)\u2004=\u2004512.": null, "Decoder:": null, "The decoder is also composed of a stack of N\u2004=\u20046 identical layers. In": null, "addition to the two sub-layers in each encoder layer, the decoder": null, "inserts a third sub-layer, which performs multi-head attention over the": null, "output of the encoder stack. Similar to the encoder, we employ residual": null, "connections around each of the sub-layers, followed by layer": null, "normalization. We also modify the self-attention sub-layer in the": null, "decoder stack to prevent positions from attending to subsequent": null, "positions. This masking, combined with fact that the output embeddings": null, "are offset by one position, ensures that the predictions for position i": null, "can depend only on the known outputs at positions less than i.": null, "3.2 Attention": null, "An attention function can be described as mapping a query and a set of": null, "key-value pairs to an output, where the query, keys, values, and output": null, "are all vectors. The output is computed as a weighted sum of the values,": null, "where the weight assigned to each value is computed by a compatibility": null, "function of the query with the corresponding key.": null, "3.2.1 Scaled Dot-Product Attention": null, "We call our particular attention \"Scaled Dot-Product Attention\"": null, "(Figure\u00a02). The input consists of queries and keys of dimension d_(k),": null, "and values of dimension d_(v). We compute the dot products of the query": null, "with all keys, divide each by $\\sqrt{d_{k}}$, and apply a softmax": null, "function to obtain the weights on the values.": null, "In practice, we compute the attention function on a set of queries": null, "simultaneously, packed together into a matrix Q. The keys and values are": null, "also packed together into matrices K and V. We compute the matrix of": null, "outputs as:": null, "  -- -------------------------------------------------------------------------- -- -----": null, "     $${{Attention}{(Q,K,V)}} = {{softmax}{(\\frac{QK^{T}}{\\sqrt{d_{k}}})}V}$$      (1)": null, "The two most commonly used attention functions are additive attention": null, "(bahdanau2014neural, ), and dot-product (multiplicative) attention.": null, "Dot-product attention is identical to our algorithm, except for the": null, "scaling factor of $\\frac{1}{\\sqrt{d_{k}}}$. Additive attention computes": null, "the compatibility function using a feed-forward network with a single": null, "hidden layer. While the two are similar in theoretical complexity,": null, "dot-product attention is much faster and more space-efficient in": null, "practice, since it can be implemented using highly optimized matrix": null, "multiplication code.": null, "While for small values of d_(k) the two mechanisms perform similarly,": null, "additive attention outperforms dot product attention without scaling for": null, "larger values of d_(k) (DBLP:journals/corr/BritzGLL17, ). We suspect": null, "that for large values of d_(k), the dot products grow large in": null, "magnitude, pushing the softmax function into regions where it has": null, "extremely small gradients \u00b9\u00b91To illustrate why the dot products get": null, "large, assume that the components of q and k are independent random": null, "variables with mean 0 and variance 1. Then their dot product,": null, "${q \\cdot k} = {\\sum_{i = 1}^{d_{k}}{q_{i}k_{i}}}$, has mean 0 and": null, "variance d_(k).. To counteract this effect, we scale the dot products by": null, "$\\frac{1}{\\sqrt{d_{k}}}$.": null, "3.2.2 Multi-Head Attention": null, "[Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head": null, "Attention consists of several attention layers running in parallel.]": null, "Instead of performing a single attention function with": null, "d_(model)-dimensional keys, values and queries, we found it beneficial": null, "to linearly project the queries, keys and values h times with different,": null, "learned linear projections to d_(k), d_(k) and d_(v) dimensions,": null, "respectively. On each of these projected versions of queries, keys and": null, "values we then perform the attention function in parallel, yielding": null, "d_(v)-dimensional output values. These are concatenated and once again": null, "projected, resulting in the final values, as depicted in Figure\u00a02.": null, "Multi-head attention allows the model to jointly attend to information": null, "from different representation subspaces at different positions. With a": null, "single attention head, averaging inhibits this.": null, "  -- ------------------ ------------------------------------------------ --": null, "     MultiHead(Q,K,V)   \u2004=\u2004Concat(head\u2081,\u2026,head_(h))W^(O)                 ": null, "     wherehead_(i)      \u2004=\u2004Attention(QW_(i)^(Q),KW_(i)^(K),VW_(i)^(V))   ": null, "Where the projections are parameter matrices": null, "W_(i)^(Q)\u2004\u2208\u2004\u211d^(d_(model)\u2005\u00d7\u2005d_(k)), W_(i)^(K)\u2004\u2208\u2004\u211d^(d_(model)\u2005\u00d7\u2005d_(k)),": null, "W_(i)^(V)\u2004\u2208\u2004\u211d^(d_(model)\u2005\u00d7\u2005d_(v)) and W^(O)\u2004\u2208\u2004\u211d^(hd_(v)\u2005\u00d7\u2005d_(model)).": null, "In this work we employ h\u2004=\u20048 parallel attention layers, or heads. For": null, "each of these we use d_(k)\u2004=\u2004d_(v)\u2004=\u2004d_(model)/h\u2004=\u200464. Due to the": null, "reduced dimension of each head, the total computational cost is similar": null, "to that of single-head attention with full dimensionality.": null, "3.2.3 Applications of Attention in our Model": null, "The Transformer uses multi-head attention in three different ways:": null, "-   \u2022": null, "    In \"encoder-decoder attention\" layers, the queries come from the": null, "    previous decoder layer, and the memory keys and values come from the": null, "    output of the encoder. This allows every position in the decoder to": null, "    attend over all positions in the input sequence. This mimics the": null, "    typical encoder-decoder attention mechanisms in sequence-to-sequence": null, "    models such as (wu2016google, ; bahdanau2014neural, ;": null, "    JonasFaceNet2017, ).": null, "    The encoder contains self-attention layers. In a self-attention": null, "    layer all of the keys, values and queries come from the same place,": null, "    in this case, the output of the previous layer in the encoder. Each": null, "    position in the encoder can attend to all positions in the previous": null, "    layer of the encoder.": null, "    Similarly, self-attention layers in the decoder allow each position": null, "    in the decoder to attend to all positions in the decoder up to and": null, "    including that position. We need to prevent leftward information": null, "    flow in the decoder to preserve the auto-regressive property. We": null, "    implement this inside of scaled dot-product attention by masking out": null, "    (setting to \u2005\u2212\u2005\u221e) all values in the input of the softmax which": null, "    correspond to illegal connections. See Figure\u00a02.": null, "3.3 Position-wise Feed-Forward Networks": null, "In addition to attention sub-layers, each of the layers in our encoder": null, "and decoder contains a fully connected feed-forward network, which is": null, "applied to each position separately and identically. This consists of": null, "two linear transformations with a ReLU activation in between.": null, "  -- ---------------------------------- -- -----": null, "     FFN(x)\u2004=\u2004max\u2006(0,xW\u2081\u2005+\u2005b\u2081)W\u2082\u2005+\u2005b\u2082      (2)": null, "While the linear transformations are the same across different": null, "positions, they use different parameters from layer to layer. Another": null, "way of describing this is as two convolutions with kernel size 1. The": null, "dimensionality of input and output is d_(model)\u2004=\u2004512, and the": null, "inner-layer has dimensionality d_(ff)\u2004=\u20042048.": null, "3.4 Embeddings and Softmax": null, "Similarly to other sequence transduction models, we use learned": null, "embeddings to convert the input tokens and output tokens to vectors of": null, "dimension d_(model). We also use the usual learned linear transformation": null, "and softmax function to convert the decoder output to predicted": null, "next-token probabilities. In our model, we share the same weight matrix": null, "between the two embedding layers and the pre-softmax linear": null, "transformation, similar to (press2016using, ). In the embedding layers,": null, "we multiply those weights by $\\sqrt{d_{\\text{model}}}$.": null, "3.5 Positional Encoding": null, "Since our model contains no recurrence and no convolution, in order for": null, "the model to make use of the order of the sequence, we must inject some": null, "information about the relative or absolute position of the tokens in the": null, "sequence. To this end, we add \"positional encodings\" to the input": null, "embeddings at the bottoms of the encoder and decoder stacks. The": null, "positional encodings have the same dimension d_(model) as the": null, "embeddings, so that the two can be summed. There are many choices of": null, "positional encodings, learned and fixed (JonasFaceNet2017, ).": null, "In this work, we use sine and cosine functions of different frequencies:": null, "  -- --------------------------------------------------- --": null, "     PE_((pos,2i))\u2004=\u2004sin(pos/10000^(2i/d_(model)))       ": null, "     PE_((pos,2i\u2005+\u20051))\u2004=\u2004cos(pos/10000^(2i/d_(model)))   ": null, "where pos is the position and i is the dimension. That is, each": null, "dimension of the positional encoding corresponds to a sinusoid. The": null, "wavelengths form a geometric progression from 2\u03c0 to 10000\u2005\u22c5\u20052\u03c0. We chose": null, "this function because we hypothesized it would allow the model to easily": null, "learn to attend by relative positions, since for any fixed offset k,": null, "PE_(pos\u2005+\u2005k) can be represented as a linear function of PE_(pos).": null, "We also experimented with using learned positional embeddings": null, "(JonasFaceNet2017, ) instead, and found that the two versions produced": null, "nearly identical results (see Table\u00a03 row (E)). We chose the sinusoidal": null, "version because it may allow the model to extrapolate to sequence": null, "lengths longer than the ones encountered during training.": null, "4 Why Self-Attention": null, "In this section we compare various aspects of self-attention layers to": null, "the recurrent and convolutional layers commonly used for mapping one": null, "variable-length sequence of symbol representations (x\u2081,\u2026,x_(n)) to": null, "another sequence of equal length (z\u2081,\u2026,z_(n)), with": null, "x_(i),\u2006z_(i)\u2004\u2208\u2004\u211d^(d), such as a hidden layer in a typical sequence": null, "transduction encoder or decoder. Motivating our use of self-attention we": null, "consider three desiderata.": null, "One is the total computational complexity per layer. Another is the": null, "amount of computation that can be parallelized, as measured by the": null, "minimum number of sequential operations required.": null, "The third is the path length between long-range dependencies in the": null, "network. Learning long-range dependencies is a key challenge in many": null, "sequence transduction tasks. One key factor affecting the ability to": null, "learn such dependencies is the length of the paths forward and backward": null, "signals have to traverse in the network. The shorter these paths between": null, "any combination of positions in the input and output sequences, the": null, "easier it is to learn long-range dependencies (hochreiter2001gradient,": null, "). Hence we also compare the maximum path length between any two input": null, "and output positions in networks composed of the different layer types.": null, "Table 1: Maximum path lengths, per-layer complexity and minimum number": null, "of sequential operations for different layer types. n is the sequence": null, "length, d is the representation dimension, k is the kernel size of": null, "convolutions and r the size of the neighborhood in restricted": null, "self-attention.": null, "  Layer Type                    Complexity per Layer   Sequential   Maximum Path Length": null, "  ----------------------------- ---------------------- ------------ ---------------------": null, "                                                       Operations   ": null, "  Self-Attention                O(n\u00b2\u22c5d)                O(1)         O(1)": null, "  Recurrent                     O(n\u22c5d\u00b2)                O(n)         O(n)": null, "  Convolutional                 O(k\u22c5n\u22c5d\u00b2)              O(1)         O(log_(k)(n))": null, "  Self-Attention (restricted)   O(r\u22c5n\u22c5d)               O(1)         O(n/r)": null, "As noted in Table 1, a self-attention layer connects all positions with": null, "a constant number of sequentially executed operations, whereas a": null, "recurrent layer requires O(n) sequential operations. In terms of": null, "computational complexity, self-attention layers are faster than": null, "recurrent layers when the sequence length n is smaller than the": null, "representation dimensionality d, which is most often the case with": null, "sentence representations used by state-of-the-art models in machine": null, "translations, such as word-piece (wu2016google, ) and byte-pair": null, "(sennrich2015neural, ) representations. To improve computational": null, "performance for tasks involving very long sequences, self-attention": null, "could be restricted to considering only a neighborhood of size r in the": null, "input sequence centered around the respective output position. This": null, "would increase the maximum path length to O(n/r). We plan to investigate": null, "this approach further in future work.": null, "A single convolutional layer with kernel width k\u2004<\u2004n does not connect": null, "all pairs of input and output positions. Doing so requires a stack of": null, "O(n/k) convolutional layers in the case of contiguous kernels, or": null, "O(log_(k)(n)) in the case of dilated convolutions (NalBytenet2017, ),": null, "increasing the length of the longest paths between any two positions in": null, "the network. Convolutional layers are generally more expensive than": null, "recurrent layers, by a factor of k. Separable convolutions": null, "(xception2016, ), however, decrease the complexity considerably, to": null, "O(k\u2005\u22c5\u2005n\u2005\u22c5\u2005d+n\u2005\u22c5\u2005d\u00b2). Even with k\u2004=\u2004n, however, the complexity of a": null, "separable convolution is equal to the combination of a self-attention": null, "layer and a point-wise feed-forward layer, the approach we take in our": null, "model.": null, "As side benefit, self-attention could yield more interpretable models.": null, "We inspect attention distributions from our models and present and": null, "discuss examples in the appendix. Not only do individual attention heads": null, "clearly learn to perform different tasks, many appear to exhibit": null, "behavior related to the syntactic and semantic structure of the": null, "sentences.": null, "5 Training": null, "This section describes the training regime for our models.": null, "5.1 Training Data and Batching": null, "We trained on the standard WMT 2014 English-German dataset consisting of": null, "about 4.5 million sentence pairs. Sentences were encoded using byte-pair": null, "encoding (DBLP:journals/corr/BritzGLL17, ), which has a shared": null, "source-target vocabulary of about 37000 tokens. For English-French, we": null, "used the significantly larger WMT 2014 English-French dataset consisting": null, "of 36M sentences and split tokens into a 32000 word-piece vocabulary": null, "(wu2016google, ). Sentence pairs were batched together by approximate": null, "sequence length. Each training batch contained a set of sentence pairs": null, "containing approximately 25000 source tokens and 25000 target tokens.": null, "5.2 Hardware and Schedule": null, "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our": null, "base models using the hyperparameters described throughout the paper,": null, "each training step took about 0.4 seconds. We trained the base models": null, "for a total of 100,000 steps or 12 hours. For our big models,(described": null, "on the bottom line of table 3), step time was 1.0 seconds. The big": null, "models were trained for 300,000 steps (3.5 days).": null, "5.3 Optimizer": null, "We used the Adam optimizer\u00a0(kingma2014adam, ) with \u03b2\u2081\u2004=\u20040.9, \u03b2\u2082\u2004=\u20040.98": null, "and \u03f5\u2004=\u200410^(\u2005\u2212\u20059). We varied the learning rate over the course of": null, "training, according to the formula:": null, "  -- --------------------------------------------------------------------------------------- -- -----": null, "     lrate\u2004=\u2004d_(model)^(\u2005\u2212\u20050.5)\u2005\u22c5\u2005min\u2006(step_num^(\u2005\u2212\u20050.5),step_num\u2005\u22c5\u2005warmup_steps^(\u2005\u2212\u20051.5))      (3)": null, "This corresponds to increasing the learning rate linearly for the first": null, "warmup_steps training steps, and decreasing it thereafter proportionally": null, "to the inverse square root of the step number. We used": null, "warmup_steps\u2004=\u20044000.": null, "5.4 Regularization": null, "We employ three types of regularization during training:": null, "Residual Dropout": null, "We apply dropout (srivastava2014dropout, ) to the output of each": null, "sub-layer, before it is added to the sub-layer input and normalized. In": null, "addition, we apply dropout to the sums of the embeddings and the": null, "positional encodings in both the encoder and decoder stacks. For the": null, "base model, we use a rate of P_(drop)\u2004=\u20040.1.": null, "Attention Dropout": null, "Query to key attentions are structurally similar to hidden-to-hidden": null, "weights in a feed-forward network, albeit across positions. The softmax": null, "activations yielding attention weights can then be seen as the analogue": null, "of hidden layer activations. A natural possibility is to extend dropout": null, "(srivastava2014dropout, ) to attention. We implement attention dropout": null, "by dropping out attention weights as,": null, "  -- ------------------------------------------------------------------------------------- --": null, "     $${{Attention}{(Q,K,V)}} = {{dropout}{({{softmax}{(\\frac{QK^{T}}{\\sqrt{d}})}})}V}$$   ": null, "In addition to residual dropout, we found attention dropout to be": null, "beneficial for our parsing experiments.": null, "Label Smoothing": null, "During training, we employed label smoothing of value \u03f5_(ls)\u2004=\u20040.1": null, "(DBLP:journals/corr/SzegedyVISW15, ). This hurts perplexity, as the": null, "model learns to be more unsure, but improves accuracy and BLEU score.": null, "6 Results": null, "6.1 Machine Translation": null, "Table 2: The Transformer achieves better BLEU scores than previous": null, "state-of-the-art models on the English-to-German and English-to-French": null, "newstest2014 tests at a fraction of the training cost.": null, "  -------------------------------------------------------------- ------- ------- ----------------------- ------------": null, "  Model                                                          BLEU            Training Cost (FLOPs)   ": null, "                                                                 EN-DE   EN-FR   EN-DE                   EN-FR": null, "  ByteNet (NalBytenet2017, )                                     23.75                                   ": null, "  Deep-Att + PosUnk (DBLP:journals/corr/ZhouCWLX16, )                    39.2                            1.0\u2005\u22c5\u200510\u00b2\u2070": null, "  GNMT + RL (wu2016google, )                                     24.6    39.92   2.3\u2005\u22c5\u200510\u00b9\u2079              1.4\u2005\u22c5\u200510\u00b2\u2070": null, "  ConvS2S (JonasFaceNet2017, )                                   25.16   40.46   9.6\u2005\u22c5\u200510\u00b9\u2078              1.5\u2005\u22c5\u200510\u00b2\u2070": null, "  MoE (shazeer2017outrageously, )                                26.03   40.56   2.0\u2005\u22c5\u200510\u00b9\u2079              1.2\u2005\u22c5\u200510\u00b2\u2070": null, "  Deep-Att + PosUnk Ensemble (DBLP:journals/corr/ZhouCWLX16, )           40.4                            8.0\u2005\u22c5\u200510\u00b2\u2070": null, "  GNMT + RL Ensemble (wu2016google, )                            26.30   41.16   1.8\u2005\u22c5\u200510\u00b2\u2070              1.1\u2005\u22c5\u200510\u00b2\u00b9": null, "  ConvS2S Ensemble (JonasFaceNet2017, )                          26.36   41.29   7.7\u2005\u22c5\u200510\u00b9\u2079              1.2\u2005\u22c5\u200510\u00b2\u00b9": null, "  Transformer (base model)                                       27.3    38.1    3.3\u2005\u22c5\u200510\u00b9\u2078              ": null, "  Transformer (big)                                              28.4    41.0    2.3\u2005\u22c5\u200510\u00b9\u2079              ": null, "On the WMT 2014 English-to-German translation task, Our big transformer": null, "model (Transformer (big) in Table\u00a02) outperforms the best previously": null, "reported models (including ensembles) by more than 2.0 BLEU,": null, "establishing a new state-of-the-art BLEU score of 28.4. The": null, "configuration of this model is listed in the bottom line of Table\u00a03.": null, "Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all": null, "previously published models and ensembles, at a fraction of the training": null, "cost of any of the previous best models.": null, "On the WMT 2014 English-to-French translation task, our big model": null, "achieves a BLEU score of 41.0, outperforming all of the previously": null, "published single models, at less than 1/4 the training cost of the": null, "previous state-of-the-art model. The Transformer (big) model trained for": null, "English-to-French used dropout rate P_(drop)\u2004=\u20040.1, instead of 0.3.": null, "For the base models, we used a single model obtained by averaging the": null, "last 5 checkpoints, which were written at 10-minute intervals. For the": null, "big models, we averaged the last 20 checkpoints. We used beam search": null, "with a beam size of 4 and length penalty \u03b1\u2004=\u20040.6 (wu2016google, ). These": null, "hyperparameters were chosen after experimentation on the development": null, "set. We set the maximum output length during inference to input length +": null, "50, but terminate early when possible (wu2016google, ).": null, "Table 2 summarizes our results and compares our translation quality and": null, "training costs to other model architectures from the literature. We": null, "estimate the number of floating point operations used to train a model": null, "by multiplying the training time, the number of GPUs used, and an": null, "estimate of the sustained single-precision floating-point capacity of": null, "each GPU \u00b2\u00b22We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40,": null, "M40 and P100, respectively..": null, "6.2 Model Variations": null, "Table 3: Variations on the Transformer architecture. Unlisted values are": null, "identical to those of the base model. All metrics are on the": null, "English-to-German translation development set, newstest2013. Listed": null, "perplexities are per-wordpiece, according to our byte-pair encoding, and": null, "should not be compared to per-word perplexities.": null, "  ------ --- ------------------------------------------- -------- ---- ------- ------- ---------- -------- ------- ------- ------- --------": null, "         N   d_(model)                                   d_(ff)   h    d_(k)   d_(v)   P_(drop)   \u03f5_(ls)   train   PPL     BLEU    params": null, "                                                                                                           steps   (dev)   (dev)   \u2005\u00d7\u200510\u2076": null, "  base   6   512                                         2048     8    64      64      0.1        0.1      100K    4.92    25.8    65": null, "  (A)                                                             1    512     512                                 5.29    24.9    ": null, "                                                                  4    128     128                                 5.00    25.5    ": null, "                                                                  16   32      32                                  4.91    25.8    ": null, "                                                                  32   16      16                                  5.01    25.4    ": null, "  (B)                                                                  16                                          5.16    25.1    58": null, "                                                                       32                                          5.01    25.4    60": null, "  (C)    2                                                                                                         6.11    23.7    36": null, "         4                                                                                                         5.19    25.3    50": null, "         8                                                                                                         4.88    25.5    80": null, "             256                                                       32      32                                  5.75    24.5    28": null, "             1024                                                      128     128                                 4.66    26.0    168": null, "                                                         1024                                                      5.12    25.4    53": null, "                                                         4096                                                      4.75    26.2    90": null, "  (D)                                                                                  0.0                         5.77    24.6    ": null, "                                                                                       0.2                         4.95    25.5    ": null, "                                                                                                  0.0              4.67    25.3    ": null, "                                                                                                  0.2              5.47    25.7    ": null, "  (E)        positional embedding instead of sinusoids                                                             4.92    25.7    ": null, "  big    6   1024                                        4096     16                   0.3                 300K    4.33    26.4    213": null, "To evaluate the importance of different components of the Transformer,": null, "we varied our base model in different ways, measuring the change in": null, "performance on English-to-German translation on the development set,": null, "newstest2013. We used beam search as described in the previous section,": null, "but no checkpoint averaging. We present these results in Table\u00a03.": null, "In Table\u00a03 rows (A), we vary the number of attention heads and the": null, "attention key and value dimensions, keeping the amount of computation": null, "constant, as described in Section 3.2.2. While single-head attention is": null, "0.9 BLEU worse than the best setting, quality also drops off with too": null, "many heads.": null, "In Table\u00a03 rows (B), we observe that reducing the attention key size": null, "d_(k) hurts model quality. This suggests that determining compatibility": null, "is not easy and that a more sophisticated compatibility function than": null, "dot product may be beneficial. We further observe in rows (C) and (D)": null, "that, as expected, bigger models are better, and dropout is very helpful": null, "in avoiding over-fitting. In row (E) we replace our sinusoidal": null, "positional encoding with learned positional embeddings": null, "(JonasFaceNet2017, ), and observe nearly identical results to the base": null, "6.3 English Constituency Parsing": null, "Table 4: The Transformer generalizes well to English constituency": null, "parsing (Results are on Section 23 of WSJ)": null, "  ------------------------------------------------- -------------------------- -----------": null, "  Parser                                            Training                   WSJ 23 F1": null, "  Vinyals & Kaiser el al. (2014) KVparse15          WSJ only, discriminative   88.3": null, "  Petrov et al. (2006) petrov-EtAl:2006:ACL         WSJ only, discriminative   90.4": null, "  Zhu et al. (2013) zhu-EtAl:2013:ACL               WSJ only, discriminative   90.4": null, "  Dyer et al. (2016) dyer-rnng:16                   WSJ only, discriminative   91.7": null, "  Transformer (4 layers)                            WSJ only, discriminative   91.3": null, "  Zhu et al. (2013) zhu-EtAl:2013:ACL               semi-supervised            91.3": null, "  Huang & Harper (2009) huang-harper:2009:EMNLP     semi-supervised            91.3": null, "  McClosky et al. (2006) mcclosky-etAl:2006:NAACL   semi-supervised            92.1": null, "  Vinyals & Kaiser el al. (2014) KVparse15          semi-supervised            92.1": null, "  Transformer (4 layers)                            semi-supervised            92.7": null, "  Dyer et al. (2016) dyer-rnng:16                   generative                 93.3": null, "To evaluate if the Transformer can generalize to other tasks we": null, "performed experiments on English constituency parsing. This task": null, "presents specific challenges: the output is subject to strong structural": null, "constraints and is significantly longer than the input. Furthermore, RNN": null, "sequence-to-sequence models have not been able to attain": null, "state-of-the-art results in small-data regimes KVparse15 .": null, "We trained a 4-layer transformer with d_(model)\u2004=\u20041024 on the Wall": null, "Street Journal (WSJ) portion of the Penn Treebank (marcus1993building,": null, "), about 40K training sentences. We also trained it in a semi-supervised": null, "setting, using the larger high-confidence and BerkleyParser corpora from": null, "with approximately 17M sentences (KVparse15, ). We used a vocabulary of": null, "16K tokens for the WSJ only setting and a vocabulary of 32K tokens for": null, "the semi-supervised setting.": null, "We performed only a small number of experiments to select the dropout,": null, "both attention and residual (section\u00a05.4), learning rates and beam size": null, "on the Section 22 development set, all other parameters remained": null, "unchanged from the English-to-German base translation model. During": null, "inference, we increased the maximum output length to input length + 300.": null, "We used a beam size of 21 and \u03b1\u2004=\u20040.3 for both WSJ only and the": null, "semi-supervised setting.": null, "Our results in Table\u00a04 show that despite the lack of task-specific": null, "tuning our model performs surprisingly well, yielding better results": null, "than all previously reported models with the exception of the Recurrent": null, "Neural Network Grammar dyer-rnng:16 .": null, "In contrast to RNN sequence-to-sequence models (KVparse15, ), the": null, "Transformer outperforms the BerkeleyParser petrov-EtAl:2006:ACL even": null, "when training only on the WSJ training set of 40K sentences.": null, "7 Conclusion": null, "In this work, we presented the Transformer, the first sequence": null, "transduction model based entirely on attention, replacing the recurrent": null, "layers most commonly used in encoder-decoder architectures with": null, "multi-headed self-attention.": null, "For translation tasks, the Transformer can be trained significantly": null, "faster than architectures based on recurrent or convolutional layers. On": null, "both WMT 2014 English-to-German and WMT 2014 English-to-French": null, "translation tasks, we achieve a new state of the art. In the former task": null, "our best model outperforms even all previously reported ensembles. We": null, "also provide an indication of the broader applicability of our models": null, "through experiments on English constituency parsing.": null, "We are excited about the future of attention-based models and plan to": null, "apply them to other tasks. We plan to extend the Transformer to problems": null, "involving input and output modalities other than text and to investigate": null, "local, restricted attention mechanisms to efficiently handle large": null, "inputs and outputs such as images, audio and video. Making generation": null, "less sequential is another research goals of ours.": null, "The code we used to train and evaluate our models is available at": null, "https://github.com/tensorflow/tensor2tensor.": null, "Acknowledgement.": null, "We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful": null, "comments, corrections and inspiration.": null, "References": null, "-   [1] Jimmy\u00a0Lei Ba, Jamie\u00a0Ryan Kiros, and Geoffrey\u00a0E Hinton. Layer": null, "    normalization. arXiv preprint arXiv:1607.06450, 2016.": null, "-   [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural": null, "    machine translation by jointly learning to align and translate.": null, "    CoRR, abs/1409.0473, 2014.": null, "-   [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc\u00a0V. Le.": null, "    Massive exploration of neural machine translation architectures.": null, "    CoRR, abs/1703.03906, 2017.": null, "-   [4] Jianpeng Cheng, Li\u00a0Dong, and Mirella Lapata. Long short-term": null, "    memory-networks for machine reading. arXiv preprint": null, "    arXiv:1601.06733, 2016.": null, "-   [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi": null, "    Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase": null, "    representations using rnn encoder-decoder for statistical machine": null, "    translation. CoRR, abs/1406.1078, 2014.": null, "-   [6] Francois Chollet. Xception: Deep learning with depthwise": null, "    separable convolutions. arXiv preprint arXiv:1610.02357, 2016.": null, "-   [7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua": null, "    Bengio. Empirical evaluation of gated recurrent neural networks on": null, "    sequence modeling. CoRR, abs/1412.3555, 2014.": null, "-   [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah\u00a0A.": null, "    Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.": null, "-   [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and": null, "    Yann\u00a0N. Dauphin. Convolutional sequence to sequence learning. arXiv": null, "    preprint arXiv:1705.03122v2, 2017.": null, "-   [10] Alex Graves. Generating sequences with recurrent neural": null, "    networks. arXiv preprint arXiv:1308.0850, 2013.": null, "-   [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep": null, "    residual learning for image recognition. In Proceedings of the IEEE": null, "    Conference on Computer Vision and Pattern Recognition, pages": null, "    770\u2013778, 2016.": null, "-   [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen": null, "    Schmidhuber. Gradient flow in recurrent nets: the difficulty of": null, "    learning long-term dependencies, 2001.": null, "-   [13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory.": null, "    Neural computation, 9(8):1735\u20131780, 1997.": null, "-   [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars": null, "    with latent annotations across languages. In Proceedings of the 2009": null, "    Conference on Empirical Methods in Natural Language Processing,": null, "    pages 832\u2013841. ACL, August 2009.": null, "-   [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer,": null, "    and Yonghui Wu. Exploring the limits of language modeling. arXiv": null, "    preprint arXiv:1602.02410, 2016.": null, "-   [16] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms.": null, "    In International Conference on Learning Representations (ICLR),": null, "    2016.": null, "-   [17] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van\u00a0den": null, "    Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation": null, "    in linear time. arXiv preprint arXiv:1610.10099v2, 2017.": null, "-   [18] Yoon Kim, Carl Denton, Luong Hoang, and Alexander\u00a0M. Rush.": null, "    Structured attention networks. In International Conference on": null, "    Learning Representations, 2017.": null, "-   [19] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic": null, "    optimization. In ICLR, 2015.": null, "-   [20] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for": null, "    LSTM networks. arXiv preprint arXiv:1703.10722, 2017.": null, "-   [21] Zhouhan Lin, Minwei Feng, Cicero Nogueira\u00a0dos Santos, Mo\u00a0Yu,": null, "    Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured": null, "    self-attentive sentence embedding. arXiv preprint arXiv:1703.03130,": null, "    2017.": null, "-   [22] Samy\u00a0Bengio \u0141ukasz\u00a0Kaiser. Can active memory replace attention?": null, "    In Advances in Neural Information Processing Systems, (NIPS), 2016.": null, "-   [23] Minh-Thang Luong, Hieu Pham, and Christopher\u00a0D Manning.": null, "    Effective approaches to attention-based neural machine translation.": null, "    arXiv preprint arXiv:1508.04025, 2015.": null, "-   [24] Mitchell\u00a0P Marcus, Mary\u00a0Ann Marcinkiewicz, and Beatrice": null, "    Santorini. Building a large annotated corpus of english: The penn": null, "    treebank. Computational linguistics, 19(2):313\u2013330, 1993.": null, "-   [25] David McClosky, Eugene Charniak, and Mark Johnson. Effective": null, "    self-training for parsing. In Proceedings of the Human Language": null, "    Technology Conference of the NAACL, Main Conference, pages 152\u2013159.": null, "    ACL, June 2006.": null, "-   [26] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob": null, "    Uszkoreit. A decomposable attention model. In Empirical Methods in": null, "    Natural Language Processing, 2016.": null, "-   [27] Romain Paulus, Caiming Xiong, and Richard Socher. A deep": null, "    reinforced model for abstractive summarization. arXiv preprint": null, "    arXiv:1705.04304, 2017.": null, "-   [28] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.": null, "    Learning accurate, compact, and interpretable tree annotation. In": null, "    Proceedings of the 21st International Conference on Computational": null, "    Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL,": null, "    July 2006.": null, "-   [29] Ofir Press and Lior Wolf. Using the output embedding to improve": null, "    language models. arXiv preprint arXiv:1608.05859, 2016.": null, "-   [30] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural": null, "    machine translation of rare words with subword units. arXiv preprint": null, "    arXiv:1508.07909, 2015.": null, "-   [31] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,": null, "    Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural": null, "    networks: The sparsely-gated mixture-of-experts layer. arXiv": null, "    preprint arXiv:1701.06538, 2017.": null, "-   [32] Nitish Srivastava, Geoffrey\u00a0E Hinton, Alex Krizhevsky, Ilya": null, "    Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to": null, "    prevent neural networks from overfitting. Journal of Machine": null, "    Learning Research, 15(1):1929\u20131958, 2014.": null, "-   [33] Ilya Sutskever, Oriol Vinyals, and Quoc\u00a0VV Le. Sequence to": null, "    sequence learning with neural networks. In Advances in Neural": null, "    Information Processing Systems, pages 3104\u20133112, 2014.": null, "-   [34] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon": null, "    Shlens, and Zbigniew Wojna. Rethinking the inception architecture": null, "    for computer vision. CoRR, abs/1512.00567, 2015.": null, "-   [35] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar": null, "    as a foreign language. In Advances in Neural Information Processing": null, "    Systems, 2015.": null, "-   [36] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc\u00a0V Le, Mohammad": null, "    Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus": null, "    Macherey, et\u00a0al. Google\u2019s neural machine translation system:": null, "    Bridging the gap between human and machine translation. arXiv": null, "    preprint arXiv:1609.08144, 2016.": null, "-   [37] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep": null, "    recurrent models with fast-forward connections for neural machine": null, "    translation. CoRR, abs/1606.04199, 2016.": null, "-   [38] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.": null, "    Fast and accurate shift-reduce constituent parsing. In Proceedings": null, "    of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages": null, "    434\u2013443. ACL, August 2013.": null, "Attention Visualizations": null, "[Figure 3: An example of the attention mechanism following long-distance": null, "dependencies in the encoder self-attention in layer 5 of 6. Many of the": null, "attention heads attend to a distant dependency of the verb \u2018making\u2019,": null, "completing the phrase \u2018making\u2026more difficult\u2019. Attentions here shown": null, "only for the word \u2018making\u2019. Different colors represent different heads.": null, "Best viewed in color.]": null, "[Figure 4: Two attention heads, also in layer 5 of 6, apparently": null, "involved in anaphora resolution. Top: Full attentions for head 5.": null, "Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads": null, "5 and 6. Note that the attentions are very sharp for this word.]": null, "[Figure 5: Many of the attention heads exhibit behaviour that seems": null, "related to the structure of the sentence. We give two such examples": null, "above, from two different heads from the encoder self-attention at layer": null, "5 of 6. The heads clearly learned to perform different tasks.]": null, "\u25c4 [ar5iv homepage] Feeling": null, "lucky? Conversion": null, "report Report": null, "an issue View\u00a0original": null, "on\u00a0arXiv\u25ba": null, "Copyright Privacy Policy": null, "Generated on Sun Jan 30 22:05:37 2022 by LaTeXML [[LOGO]]": null}