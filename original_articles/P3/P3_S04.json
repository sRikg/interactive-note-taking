"[Figure 1: The Transformer - model architecture.] Most competitive neural sequence transduction models have an encoder-decoder structure (cho2014learning, ; bahdanau2014neural, ; sutskever14, ). Here, the encoder maps an input sequence of symbol representations (x\u2081,\u2026,x_(n)) to a sequence of continuous representations z\u2004=\u2004(z\u2081,\u2026,z_(n)). Given z, the decoder then generates an output sequence (y\u2081,\u2026,y_(m)) of symbols one element at a time. At each step the model is auto-regressive (graves2013generating, ), consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure\u00a01, respectively."
