"For evaluating the impact of these approaches, we perform language modeling over a preprocessed version of the Penn Treebank (PTB) (Mikolov et\u00a0al., 2010) and the WikiText-2 (WT2) data set (Merity et\u00a0al., 2016). PTB: The Penn Treebank data set has long been a central data set for experimenting with language modeling. The data set is heavily preprocessed and does not contain capital letters, numbers, or punctuation. The vocabulary is also capped at 10,000 unique words, quite small in comparison to most modern datasets, which results in a large number of out of vocabulary (OoV) tokens. WT2: WikiText-2 is sourced from curated Wikipedia articles and is approximately twice the size of the PTB data set. The text is tokenized and processed using the Moses tokenizer (Koehn et\u00a0al., 2007), frequently used for machine translation, and features a vocabulary of over 30,000 words. Capitalization, punctuation, and numbers are retained in this data set. All experiments use a three-layer LSTM model with 1150 units in the hidden layer and an embedding of size 400. The loss was averaged over all examples and timesteps. All embedding weights were uniformly initialized in the interval [\u2005\u2212\u20050.1,\u20060.1] and all other weights were initialized between $\\lbrack{- \\frac{1}{\\sqrt{H}}},\\frac{1}{\\sqrt{H}}\\rbrack$, where H is the hidden size. For training the models, we use the NT-ASGD algorithm discussed in the previous section for 750 epochs with L equivalent to one epoch and n\u2004=\u20045. We use a batch size of 80 for WT2 and 40 for PTB. Empirically, we found relatively large batch sizes (e.g., 40-80) performed better than smaller sizes (e.g., 10-20) for NT-ASGD. After completion, we run ASGD with T\u2004=\u20040 and hot-started w\u2080 as a fine-tuning step to further improve the solution. For this fine-tuning step, we terminate the run using the same non-monotonic criterion detailed in Algorithm 1. We carry out gradient clipping with maximum norm 0.25 and use an initial learning rate of 30 for all experiments. We use a random BPTT length which is \ud835\udca9(70,5) with probability 0.95 and \ud835\udca9(35,5) with probability 0.05. The values used for dropout on the word vectors, the output between LSTM layers, the output of the final LSTM layer, and embedding dropout where (0.4,0.3,0.4,0.1) respectively. For the weight-dropped LSTM, a dropout of 0.5 was applied to the recurrent weight matrices. For WT2, we increase the input dropout to 0.65 to account for the increased vocabulary size. For all experiments, we use AR and TAR values of 2 and 1 respectively, and tie the embedding and softmax weights. These hyperparameters were chosen through trial and error and we expect further improvements may be possible if a fine-grained hyperparameter search were to be conducted. In the results, we abbreviate our approach as AWD-LSTM for ASGD Weight-Dropped LSTM.  Model Parameters Validation Test  - - - -  Mikolov & Zweig (2012) - KN-5 2M^(\u2021) \u2212 141.2  Mikolov & Zweig (2012) - KN5 + cache 2M^(\u2021) \u2212 125.7  Mikolov & Zweig (2012) - RNN 6M^(\u2021) \u2212 124.7  Mikolov & Zweig (2012) - RNN-LDA 7M^(\u2021) \u2212 113.7  Mikolov & Zweig (2012) - RNN-LDA + KN-5 + cache 9M^(\u2021) \u2212 92.0  Zaremba et\u00a0al. (2014) - LSTM (medium) 20M 86.2 82.7  Zaremba et\u00a0al. (2014) - LSTM (large) 66M 82.2 78.4  Gal & Ghahramani (2016) - Variational LSTM (medium) 20M 81.9\u2005\u00b1\u20050.2 79.7\u2005\u00b1\u20050.1  Gal & Ghahramani (2016) - Variational LSTM (medium, MC) 20M \u2212 78.6\u2005\u00b1\u20050.1  Gal & Ghahramani (2016) - Variational LSTM (large) 66M 77.9\u2005\u00b1\u20050.3 75.2\u2005\u00b1\u20050.2  Gal & Ghahramani (2016) - Variational LSTM (large, MC) 66M \u2212 73.4\u2005\u00b1\u20050.0  Kim et\u00a0al. (2016) - CharCNN 19M \u2212 78.9  Merity et\u00a0al. (2016) - Pointer Sentinel-LSTM 21M 72.4 70.9  Grave et\u00a0al. (2016) - LSTM \u2212 \u2212 82.3  Grave et\u00a0al. (2016) - LSTM + continuous cache pointer \u2212 \u2212 72.1  Inan et\u00a0al. (2016) - Variational LSTM (tied) + augmented loss 24M 75.7 73.2  Inan et\u00a0al. (2016) - Variational LSTM (tied) + augmented loss 51M 71.1 68.5  Zilly et\u00a0al. (2016) - Variational RHN (tied) 23M 67.9 65.4  Zoph & Le (2016) - NAS Cell (tied) 25M \u2212 64.0  Zoph & Le (2016) - NAS Cell (tied) 54M \u2212 62.4  Melis et\u00a0al. (2017) - 4-layer skip connection LSTM (tied) 24M 60.9 58.3  AWD-LSTM - 3-layer LSTM (tied) 24M 60.0 57.3  AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer 24M 53.9 52.8 Table 1: Single model perplexity on validation and test sets for the Penn Treebank language modeling task. Parameter numbers with \u2021 are estimates based upon our understanding of the model and with reference to Merity et\u00a0al. (2016). Models noting tied use weight tying on the embedding and softmax weights. Our model, AWD-LSTM, stands for ASGD Weight-Dropped LSTM.  Model Parameters Validation Test  - - - -  Inan et\u00a0al. (2016) - Variational LSTM (tied) (h\u2004=\u2004650) 28M 92.3 87.7  Inan et\u00a0al. (2016) - Variational LSTM (tied) (h\u2004=\u2004650) + augmented loss 28M 91.5 87.0  Grave et\u00a0al. (2016) - LSTM \u2212 \u2212 99.3  Grave et\u00a0al. (2016) - LSTM + continuous cache pointer \u2212 \u2212 68.9  Melis et\u00a0al. (2017) - 1-layer LSTM (tied) 24M 69.3 65.9  Melis et\u00a0al. (2017) - 2-layer skip connection LSTM (tied) 24M 69.1 65.9  AWD-LSTM - 3-layer LSTM (tied) 33M 68.6 65.8  AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer 33M 53.8 52.0 Table 2: Single model perplexity over WikiText-2. Models noting tied use weight tying on the embedding and softmax weights. Our model, AWD-LSTM, stands for ASGD Weight-Dropped LSTM."
