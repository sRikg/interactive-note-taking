L\u2082-regularization is often used on the weights of the network to control the norm of the resulting model and reduce overfitting. In addition, L\u2082 decay can be used on the individual unit activations and on the difference in outputs of an RNN at different time steps; these strategies labeled as activation regularization (AR) and temporal activation regularization (TAR) respectively (Merity et\u00a0al., 2017). AR penalizes activations that are significantly larger than 0 as a means of regularizing the network. Concretely, AR is defined as  - - -  \u03b1L\u2082(m\u2299h_(t))  where m is the dropout mask, L\u2082(\u22c5)\u2004=\u2004\u2004\u2225\u2004\u2005\u22c5\u2005\u2225\u2082, h_(t) is the output of the RNN at timestep t, and \u03b1 is a scaling coefficient. TAR falls under the broad category of slowness regularizers (Hinton, 1989; F\u00f6ldi\u00e1k, 1991; Luciw & Schmidhuber, 2012; Jonschkowski & Brock, 2015) which penalize the model from producing large changes in the hidden state. Using the notation from AR, TAR is defined as  - - -  \u03b2L\u2082(h_(t)\u2212h_(t\u2005+\u20051))  where \u03b2 is a scaling coefficient. As in Merity et\u00a0al. (2017), the AR and TAR loss are only applied to the output of the final RNN layer as opposed to being applied to all layers.