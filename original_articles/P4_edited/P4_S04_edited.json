SGD is among the most popular methods for training deep learning models across various modalities including computer vision, natural language processing, and reinforcement learning. The training of deep networks can be posed as a non-convex optimization problem  - - -  ${\\min\\limits_{w}\\quad{\\frac{1}{N}{\\sum\\limits_{i = 1}^{N}{f_{i}{(w)}}}}},$  where f_(i) is the loss function for the i^(th) data point, w are the weights of the network, and the expectation is taken over the data. Given a sequence of learning rates, \u03b3_(k), SGD iteratively takes steps of the form  - - - -  ${w_{k + 1} = {w_{k} - {\\gamma_{k}\\hat{\\nabla}f{(w_{k})}}}},$ (1) where the subscript denotes the iteration number and the $\\hat{\\nabla}$ denotes a stochastic gradient that may be computed on a minibatch of data points. SGD demonstrably performs well in practice and also possesses several attractive theoretical properties such as linear convergence (Bottou et\u00a0al., 2016), saddle point avoidance (Panageas & Piliouras, 2016) and better generalization performance (Hardt et\u00a0al., 2015). For the specific task of neural language modeling, traditionally SGD without momentum has been found to outperform other algorithms such as momentum SGD (Sutskever et\u00a0al., 2013), Adam (Kingma & Ba, 2014), Adagrad (Duchi et\u00a0al., 2011) and RMSProp (Tieleman & Hinton, 2012) by a statistically significant margin. Motivated by this observation, we investigate averaged SGD (ASGD) to further improve the training process. ASGD has been analyzed in depth theoretically and many surprising results have been shown including its asymptotic second-order convergence (Polyak & Juditsky, 1992; Mandt et\u00a0al., 2017). ASGD takes steps identical to equation (1) but instead of returning the last iterate as the solution, returns $\\frac{1}{({{K - T} + 1})}{\\sum_{i = T}^{K}w_{i}}$, where K is the total number of iterations and T\u2004<\u2004K is a user-specified averaging trigger. Algorithm 1 Non-monotonically Triggered ASGD (NT-ASGD) Inputs: Initial point w\u2080, learning rate \u03b3, logging interval L, non-monotone interval n. 1:\u00a0\u00a0Initialize k\u21900, t\u21900, T\u21900, logs \u2190 [] 2:\u00a0\u00a0while\u00a0stopping criterion not met\u00a0do 3:\u00a0\u00a0\u00a0\u00a0\u00a0Compute stochastic gradient $\\hat{\\nabla}f{(w_{k})}$ and take SGD step (1). 4:\u00a0\u00a0\u00a0\u00a0\u00a0if\u00a0 mod(k,L)\u2004=\u20040 and T\u2004=\u20040\u00a0then 5:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Compute validation perplexity v. 6:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if\u00a0t\u2004>\u2004n and $v > \\min\\limits_{l \\in {\\{{t - n},\\cdots,t\\}}}$ logs[l]\u00a0then 7:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set T\u2190k 8:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0end\u00a0if 9:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Append v to logs 10:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0t\u2190t\u2005+\u20051 11:\u00a0\u00a0\u00a0\u00a0\u00a0end\u00a0if 12:\u00a0\u00a0end\u00a0while return $\\frac{\\sum_{i = T}^{k}w_{i}}{({{k - T} + 1})}$ Despite its theoretical appeal, ASGD has found limited practical use in training of deep networks. This may be in part due to unclear tuning guidelines for the learning-rate schedule \u03b3_(k) and averaging trigger T. If the averaging is triggered too soon, the efficacy of the method is impacted, and if it is triggered too late, many additional iterations may be needed to converge to the solution. In this section, we describe a non-monotonically triggered variant of ASGD (NT-ASGD), which obviates the need for tuning T. Further, the algorithm uses a constant learning rate throughout the experiment and hence no further tuning is necessary for the decay scheduling. Ideally, averaging needs to be triggered when the SGD iterates converge to a steady-state distribution (Mandt et\u00a0al., 2017). This is roughly equivalent to the convergence of SGD to a neighborhood around a solution. In the case of SGD, certain learning-rate reduction strategies such as the step-wise strategy analogously reduce the learning rate by a fixed quantity at such a point. A common strategy employed in language modeling is to reduce the learning rates by a fixed proportion when the performance of the model\u2019s primary metric (such as perplexity) worsens or stagnates. Along the same lines, one could make a triggering decision based on the performance of the model on the validation set. However, instead of averaging immediately after the validation metric worsens, we propose a non-monotonic criterion that conservatively triggers the averaging when the validation metric fails to improve for multiple cycles; see Algorithm 1. Given that the choice of triggering is irreversible, this conservatism ensures that the randomness of training does not play a major role in the decision. Analogous strategies have also been proposed for learning-rate reduction in SGD (Keskar & Saon, 2015). While the algorithm introduces two additional hyperparameters, the logging interval L and non-monotone interval n, we found that setting L to be the number of iterations in an epoch and n\u2004=\u20045 worked well across various models and data sets. As such, we use this setting in all of our NT-ASGD experiments in the following section and demonstrate that it achieves better training outcomes as compared to SGD.