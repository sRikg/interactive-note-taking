In past work, pointer based attention models have been shown to be highly effective in improving language modeling (Merity et\u00a0al., 2016; Grave et\u00a0al., 2016). Given such substantial improvements to the underlying neural language model, it remained an open question as to how effective pointer augmentation may be, especially when improvements such as weight tying may act in mutually exclusive ways. The neural cache model (Grave et\u00a0al., 2016) can be added on top of a pre-trained language model at negligible cost. The neural cache stores the previous hidden states in memory cells and then uses a simple convex combination of the probability distributions suggested by the cache and the language model for prediction. The cache model has three hyperparameters: the memory size (window) for the cache, the coefficient of the combination (which determines how the two distributions are mixed), and the flatness of the cache distribution. All of these are tuned on the validation set once a trained language model has been obtained and require no training by themselves, making it quite inexpensive to use. The tuned values for these hyperparameters were (2000,0.1,1.0) for PTB and (3785,0.1279,0.662) for WT2 respectively. In Tables 1 and 2, we show that the model further improves the perplexity of the language model by as much as 6 perplexity points for PTB and 11 points for WT2. While this is smaller than the gains reported in Grave et\u00a0al. (2016), which used an LSTM without weight tying, this is still a substantial drop. Given the simplicity of the neural cache model, and the lack of any trained components, these results suggest that existing neural language models remain fundamentally lacking, failing to capture long term dependencies or remember recently seen words effectively. To understand the impact the pointer had on the model, specifically the validation set perplexity, we detail the contribution that each word has on the cache model\u2019s overall perplexity in Table 3. We compute the sum of the total difference in the loss function value (i.e., log perplexity) between the LSTM-only and LSTM-with-cache models for the target words in the validation portion of the WikiText-2 data set. We present results for the sum of the difference as opposed to the mean since the latter undesirably overemphasizes infrequently occurring words for which the cache helps significantly and ignores frequently occurring words for which the cache provides modest improvements that cumulatively make a strong contribution. The largest cumulative gain is in improving the handling of <unk> tokens, though this is over 11540 instances. The second best improvement, approximately one fifth the gain given by the <unk> tokens, is for Meridian, yet this word only occurs 161 times. This indicates the cache still helps significantly even for relatively rare words, further demonstrated by Churchill, Blythe, or Sonic. The cache is not beneficial when handling frequent word categories, such as punctuation or stop words, for which the language model is likely well suited. These observations motivate the design of a cache framework that is more aware of the relative strengths of the two models.  Word Count \u0394loss Word Count \u0394loss  - - - - - -  . 7632 -696.45 <unk> 11540 5047.34  , 9857 -687.49 Meridian 161 1057.78  of 5816 -365.21 Churchill 137 849.43  = 2884 -342.01 - 67 682.15  to 4048 -283.10 Blythe 97 554.95  in 4178 -222.94 Sonic 75 543.85  <eos> 3690 -216.42 Richmond 101 429.18  and 5251 -215.38 Starr 74 416.52  the 12481 -209.97 Australian 234 366.36  a 3381 -149.78 Pagan 54 365.19  \" 2540 -127.99 Asahi 39 316.24  that 1365 -118.09 Japanese 181 295.97  by 1252 -113.05 Hu 43 285.58  was 2279 -107.95 Hedgehog 29 266.48  ) 1101 -94.74 Burma 35 263.65  with 1176 -93.01 29 92 260.88  for 1215 -87.68 Mississippi 72 241.59  on 1485 -81.55 German 108 241.23  as 1338 -77.05 mill 67 237.76  at 879 -59.86 Cooke 33 231.11 Table 3: The sum total difference in loss (log perplexity) that a given word results in over all instances in the validation data set of WikiText-2 when the continuous cache pointer is introduced. The right column contains the words with the twenty best improvements (i.e., where the cache was advantageous), and the left column the twenty most deteriorated (i.e., where the cache was disadvantageous).