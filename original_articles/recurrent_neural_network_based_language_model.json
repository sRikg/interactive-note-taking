"\nRecurrent neural network based language model\nToma\u00b4s\u02c7 Mikolov1,2, Martin Karafia\u00b4t1, Luka\u00b4s\u02c7 Burget1, Jan \u201cHonza\u201d C\u02c7ernocky\u00b41, Sanjeev Khudanpur2 1Speech@FIT, Brno University of Technology, Czech Republic\n2 Department of Electrical and Computer Engineering, Johns Hopkins University, USA\n{imikolov,karafiat,burget,cernocky}@fit.vutbr.cz, khudanpur@jhu.edu\n\n\nAbstract\nA new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Re- sults indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empiri- cal evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high com- putational (training) complexity.\nIndex Terms: language modeling, recurrent neural networks, speech recognition\n\nIntroduction\nSequential data prediction is considered by many as a key prob- lem in machine learning and artificial intelligence (see for ex- ample [1]). The goal of statistical language modeling is to predict the next word in textual data given context; thus we  are dealing with sequential data prediction problem when con- structing language models. Still, many attempts to obtain such statistical models involve approaches that are very specific for language domain - for example, assumption that natural lan- guage sentences can be described by parse trees, or that we need to consider morphology of words, syntax and semantics. Even the most widely used and general models, based on n- gram statistics, assume that language consists of sequences of atomic symbols - words - that form sentences, and where the end of sentence symbol plays important and very special role.\nIt is questionable if there has been any significant progress in language modeling over simple n-gram models (see for ex- ample [2] for review of advanced techniques). If we would mea- sure this progress by ability of models to better predict sequen- tial data, the answer would be that considerable improvement has been achieved - namely by introduction of cache models and class-based models. While many other techniques have been proposed, their effect is almost always similar to cache models (that describe long context information) or class-based models (that improve parameter estimation for short contexts by sharing parameters between similar words).\nIf we would measure success of advanced language model- ing techniques by their application in practice, we would have to be much more skeptical. Language models for real-world speech recognition or machine translation systems are built on huge amounts of data, and popular belief says that more data  is all we need. Models coming from research tend to be com-\n\n\n\nplex and often work well only for systems based on very limited amounts of training data. In fact, most of the proposed advanced language modeling techniques provide only tiny improvements over simple baselines, and are rarely used in practice.\n\nModel description\nWe have decided to investigate recurrent neural networks for modeling sequential data. Using artificial neural networks in statistical language modeling has been already proposed by Bengio [3], who used feedforward neural networks with fixed- length context. This approach was exceptionally successful and further investigation by Goodman [2] shows that this sin- gle model performs better than mixture of several other models based on other techniques, including class-based model. Later, Schwenk [4] has shown that neural network based models pro- vide significant improvements in speech recognition for several tasks against good baseline systems.\nA major deficiency of Bengio\u2019s approach is that a feedfor- ward network has to use fixed length context that needs to be specified ad hoc before training. Usually this means that neural networks see only five to ten preceding words when predicting the next one. It is well known that humans can exploit longer context with great success. Also, cache models provide comple- mentary information to neural network models, so it is natural to think about a model that would encode temporal information implicitly for contexts with arbitrary lengths.\nRecurrent neural networks do not use limited size of con- text. By using recurrent connections, information can cycle in-\n\n\n\n\n\n\n\nCopyright \u2665 2010 ISCA\n1045\n26 -30 September 2010, Makuhari, Chiba, Japan\n\nside these networks for arbitrarily long time (see [5]). However,\n\ns(t \u2212 1).  Softmax ensures that this probPability distribution is\n\nstochastic gradient descent can be quite difficult [6].\nIn our work, we have used an architecture that is usually called a simple recurrent neural network or Elman network [7]. This is probably the simplest possible version of recurrent neu- ral network, and very easy to implement and train. The network has an input layer x, hidden layer s (also called context layer or state) and output layer y. Input to the network in time t is x(t), output is denoted as y(t), and s(t) is state of the network (hidden layer). Input vector x(t) is formed by concatenating vector w representing current word, and output from neurons in context layer s at time t 1. Input, hidden and output layers are then computed as follows:\nAt each training step, error vector is computed according to cross entropy criterion and weights are updated with the stan- dard backpropagation algorithm:\nwhere desired is a vector using 1-of-N coding representing the word that should have been predicted in a particular context and y(t) is the actual output from the network.\nNote that training phase and testing phase in statistical lan- guage modeling usually differs in the fact that models do not get updated as test data are being processed. So, if a new person- name occurs repeatedly in the test set, it will repeatedly get a very small probability, even if it is composed of known words. It can be assumed that such long term memory should not re- side in activation of context units (as these change very rapidly), but rather in synapses themselves - that the network should con- tinue training even during testing phase. We refer to such model as dynamic. For dynamic model, we use fixed learning rate\n\n\nwhere f (z) is sigmoid activation function:\n\n\n\n\nonce as it processes testing data. This is of course not optimal solution, but as we shall see, it is enough to obtain large perplex- ity reductions against static models. Note that such modification is very similar to cache techniques for backoff models, with the difference that neural networks learn in continuous space, so if \u2019dog\u2019 and \u2019cat\u2019 are related, frequent occurrence of \u2019dog\u2019 in test- ing data will also trigger increased probability of \u2019cat\u2019.\n\n\n\ngto new domains. However, in speech recognition experiments,\n\nFor initialization, s(0) can be set to vector of small values, like\n0.1 - when processing a large amount of data, initialization is not crucial. In the next time steps, s(t + 1) is a copy of s(t). In- put vector x(t) represents word in time t encoded using 1-of-N coding and previous context layer - size of vector x is equal to size of vocabulary V (this can be in practice 30 000 200 000) plus size of context layer. Size of context (hidden) layer s is usually 30 500 hidden units. Based on our experiments, size of hidden layer should reflect amount of training data - for large amounts of data, large hidden layer is needed1.\nNetworks are trained in several epochs, in which all data from training corpus are sequentially presented. Weights are initialized to small values (random Gaussian noise with zero mean and 0.1 variance). To train the network, we use the stan- dard backpropagation algorithm with stochastic gradient de- scent. Starting learning rate is \u03b1  =  0.1.  After each epoch,  the network is tested on validation data. If log-likelihood of validation data increases, training continues in new epoch. If no significant improvement is observed, learning rate \u03b1 is halved at start of each new epoch.  After there is again no signifi-  cant improvement, training is finished. Convergence is usually achieved after 10-20 epochs.\nIn our experiments, networks do not overtrain significantly,\nhistory is represented by hypothesis given by recognizer, and contains recognition errors. This generally results in poor per- formance of cache n-gram models in ASR [2].\nThe training algorithm described here is also referred to as truncated backpropagation through time with \u03c4 = 1. It is not optimal, as weights of network are updated based on error vec- tor computed only for current time step. To overcome this sim- plification, backpropagation through time (BPTT) algorithm is commonly used (see Boden [5] for details).\nOne of major differences between feedforward neural net- works as used by Bengio [3] and Schwenk [4] and recurrent neural networks is in amount of parameters that need to be tuned or selected ad hoc before training. For RNN LM, only size of hidden (context) layer needs to be selected. For feedfor- ward networks, one needs to tune the size of layer that projects words to low dimensional space, the size of hidden layer and the context-length2.\n\n2.1.  Optimization\nTo improve performance, we merge all words that occur less often than a threshold (in the training text) into a special rare token. Word-probabilities are then computed as\n\neven if very large hidden layers are used - regularization of net- works to penalize large weights did not provide any significant\nP (wi(t+1)|w(t), s(t\u22121)) =\nyrare(t) Crare\nif wi(t + 1) is rare,\n\nimprovements. Output layer y(t) represents probability dis- tribution of next word given previous word w(t) and context\nyi(t)\totherwise\n(7)\n\n\n\n1Consequently, time needed to train optimal network increases faster than just linearly with increased amount of training data: vocabulary growth increases the input and output layer sizes, and also the optimal hidden layer size increases with more training data.\n\n\n2It is out of scope of this paper to provide a detailed comparison of feedforward and recurrent networks. However, in some experiments we have achieved almost twice perplexity reduction over n-gram models by using a recurrent network instead of a feedforward network.\n\nTable 1: Performance of models on WSJ DEV set when increas- ing size of training data.\n\nwhere Crare is number of words in the vocabulary that occur less often than the threshold. All rare words are thus treated equally, ie. probability is distributed uniformly between them.\nSchwenk [4] describes several possible approaches that can be used for further performance improvements. Additional pos- sibilities are also discussed in [10][11][12] and most of them can be applied also to RNNs. For comparison, it takes around 6 hours for our basic implementation to train RNN model based on Brown corpus (800K words, 100 hidden units and vocab- ulary threshold 5), while Bengio reports 113 days for basic implementation and 26 hours with importance sampling [10], when using similar data and size of neural network. We use only BLAS library to speed up computation.\n\nWSJ experiments\nTo evaluate performance of simple recurrent neural network based language model, we have selected several standard speech recognition tasks. First we report results after rescor- ing 100-best lists from DARPA WSJ\u201992 and WSJ\u201993 data sets\n- the same data sets were used by Xu [8] and Filimonov [9]. Oracle WER is 6.1% for dev set and 9.5% for eval set. Training data for language model are the same as used by Xu [8].\nThe training corpus consists of 37M words from NYT sec- tion of English Gigaword. As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models (300K sentences) - it takes several weeks to train the most complex models. Perplexity is evalu- ated on held-out data (230K words). Also, we report results for combined models - linear interpolation with weight 0.75 for RNN LM and 0.25 for backoff LM is used in all these experi- ments. In further experiments, we denote modified Kneser-Ney smoothed 5-gram as KN5. Configurations of neural network LMs, such as RNN 90/2, indicate that the hidden layer size is 90 and threshold for merging words to rare token is 2. To cor- rectly rescore n-best lists with backoff models that are trained on subset of data used by recognizer, we use open vocabulary language models (unknown words are assigned small probabil- ity). To improve results, outputs from various RNN LMs with different architectures can be linearly interpolated (diversity is also given by random weight initialization).\nThe results, reported in Tables 1 and 2, are by no means among the largest improvements reported for the WSJ task ob- tained just by changing the language modeling technique. The improvement keeps getting larger with increasing training data, suggesting that even larger improvements may be achieved sim- ply by using more data. As shown in Table 2, WER reduc-  tion when using mixture of 3 dynamic RNN LMs against 5- gram with modified Kneser-Ney smoothing is about 18%. Also, perplexity reductions are one of the largest ever reported, al- most 50% when comparing KN 5gram and mixture of 3 dy-\nTable 2: Comparison of various configurations of RNN LMs and combinations with backoff models while using 6.4M words in training data (WSJ DEV).\n\n\nTable 3: Comparison of WSJ results obtained with various mod- els. Note that RNN models are trained just on 6.4M words.\n\n\nnamic RNN LMs - actually, by mixing static and dynamic RNN LMs with larger learning rate used when processing testing data (\u03b1 = 0.3), the best perplexity result was 112.\nAll LMs in the preceding experiments were trained on only 6.4M words, which is much less than the amount of data used by others for this task. To provide a comparison with Xu [8] and Filimonov [9], we have used 37M words based backoff model (the same data were used by Xu, Filimonov used 70M words). Results are reported in Table 3, and we can conclude that RNN based models can reduce WER by around 12% relatively, com- pared to backoff model trained on 5x more data3.\n\nNIST RT05 experiments\nWhile previous experiments show very interesting improve- ments over a fair baseline, a valid criticism would be that the acoustic models used in those experiments are far from state of the art, and perhaps  obtaining improvements in such cases is easier than improving well tuned system. Even more crucial is the fact that 37M or 70M words used for training baseline backoff models is by far less than what is possible for the task. To show that it is possible to obtain meaningful improve- ments in state of the art system, we experimented with lattices generated by AMI system used for NIST RT05 evaluation [13].\nTest data set was NIST RT05 evaluation on independent headset condition.\nThe acoustic HMMs are based on cross-word tied-states tri- phones trained discriminatively using MPE criteria. Feature ex-\n\n3We have also tried to combine RNN models and discriminatively trained LMs [8], with no significant improvement.\n4Apparently strange result obtained with dynamic models on eval- uation set is probably due to the fact that sentences in eval set do not follow each other. As dynamic changes in model try to capture longer context information between sentences, sentences must be presented consecutively to dynamic models.\n\n\nTable 4: Comparison of very large back-off LMs and RNN LMs trained only on limited in-domain data (5.4M words).\n\n\n\ntraction use 13 Mel-PLP\u2019s features with deltas, double and triple deltas reduced by HLDA to 39-dimension feature vector. VTLN warping factors were applied to the outputs of Mel filterbanks. The amount of training data was 115 hours of meeting speech from ICSI, NIST, ISL and AMI training corpora.\nFour gram LM used in AMI system was trained on vari- ous data sources, see description in [13]. Total amount of LM training data was more than 1.3G words. This LM is denoted as RT05 LM in table 4. The RT09 LM was extended by additional CHIL and web data. Next change was in lowering cut-offs, e.g. the minimum count for 4-grams was set to 3 instead of 4. To train the RNN LM, we selected in domain data that consists  of meeting transcriptions and Switchboard corpus, for a total of 5.4M words \u2013 RNN training was too time consuming with more data.  This means that RNNs are trained on tiny subset  of the data that are used to construct the RT05 and RT09 LMs. Table 4 compares the performance of these LMs on RT05.\n\nConclusion and future work\nRecurrent neural networks outperformed significantly state of the art backoff models in all our experiments, most notably even in case when backoff models were trained on much more data than RNN LMs. In WSJ experiments, word error rate reduction is around 18% for models trained on the same amount of data, and 12% when backoff model is trained on 5 times more data than RNN model. For NIST RT05, we can conclude that models trained on just 5.4M words of in-domain data can outperform big backoff models, which are trained on hundreds times more data. Obtained results are breaking myth that language model- ing is just about counting n-grams, and that the only reasonable way how to improve results is by acquiring new training data.\nPerplexity improvements reported in Table 2 are one of the largest ever reported on similar data set, with very significant effect of on-line learning (also called dynamic models in this paper, and in context of speech recognition very similar to un- supervised LM training techniques). While WER is affected just slightly and requires correct ordering of testing data, on- line learning should be further investigated as it provides natural way how to obtain cache-like and trigger-like information (note that for data compression, on-line techniques for training pre- dictive neural networks have been already studied for example by Mahoney [14]). If we want to build models that can really learn language, then on-line learning is crucial - acquiring new information is definitely important.\nIt is possible that further investigation into backpropagation through time algorithm for learning recurrent neural networks\nwill provide additional improvements. Preliminary results on toy tasks are promising. However, it does not seem that simple recurrent neural networks can capture truly long context infor- mation, as cache models still provide complementary informa- tion even to dynamic models trained with BPTT. Explanation is discussed in [6].\nAs we did not make any task or language specific assump- tion in our work, it is easy to use RNN based models almost ef- fortlessly in any kind of application that uses backoff language models, like machine translation or OCR. Especially tasks in- volving inflectional languages or languages with large vocabu- lary might benefit from using NN based models, as was already shown in [12].\nBesides very good results reported in our work, we find pro- posed recurrent neural network model interesting also because it connects language modeling more closely to machine learn- ing, data compression and cognitive sciences research. We hope that these connections will be better understood in the future.\n\nAcknowledgements\nWe would like to thank Puyang Xu for providing WSJ data, and Stefan Kombrink for help with additional NIST RT experi- ments. The work was also partly supported by European project DIRAC (FP6-027787), Czech Ministry of Interior project No. VD20072010B16, Grant Agency of Czech Republic project No. 102/08/0707, Czech Ministry of Education project No. MSM0021630528 and by BUT FIT grant No. FIT-10-S-2.\n"