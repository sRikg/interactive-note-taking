"arXiv:1409.3215v3  [cs.CL]  14 Dec 2014arXiv:1409.3215v3  [cs.CL]  14 Dec 2014Sequence to Sequence Learningwith Neural NetworksIlya SutskeverGoogleilyasu@google.comOriol VinyalsGooglevinyals@google.comQuoc V. LeGoogleqvl@google.comAbstractDeep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well wheneverlarge labeled training sets are available, they cannot be used to map sequences tosequences. In this paper, we present a general end-to-end approach to sequencelearning that makes minimal assumptions on the sequence structure. Our methoduses a multilayered Long Short-Term Memory (LSTM) to map the input sequenceto a vector of a fixed dimensionality, and then another deep LSTM to decode thetarget sequence from the vector. Our main result is that on an English to Frenchtranslation task from the WMT\u201914 dataset, the translations produced by the LSTMachieve a BLEU score of 34.8 on the entire test set, where the LSTM\u2019s BLEUscore was penalized on out-of-vocabulary words. Additionally, the LSTM did nothave difficulty on long sentences. For comparison, a phrase-based SMT systemachieves a BLEU score of 33.3 on the same dataset. When we used the LSTMto rerank the 1000 hypotheses produced by the aforementioned SMT system, itsBLEU score increases to 36.5, which is close to the previous best result on thistask. The LSTM also learned sensible phrase and sentence representations thatare sensitive to word order and are relatively invariant to the active and the pas-sive voice. Finally, we found that reversing the order of the words in all sourcesentences (but not target sentences) improved the LSTM\u2019s performance markedly,because doing so introduced many short term dependencies between the sourceand the target sentence which made the optimization problem easier.1 IntroductionDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-cellent performance on difficult problems such as speech recognition [13, 7] and visual object recog-nition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computationfor a modest number of steps. A surprising example of the power of DNNs is their ability to sortN N -bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks arerelated to conventional statistical models, they learn an intricate computation. Furthermore, largeDNNs can be trained with supervised backpropagation whenever the labeled training set has enoughinformation to specify the network\u2019s parameters. Thus, if there exists a parameter setting of a largeDNN that achieves good results (for example, because humans can solve the task very rapidly),supervised backpropagation will find these parameters and solve the problem.Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targetscan be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, sincemany important problems are best expressed with sequences whose lengths are not known a-priori.For example, speech recognition and machine translation are sequential problems. Likewise, ques-tion answering can also be seen as mapping a sequence of words representing the question to a1http://arxiv.org/abs/1409.3215v3sequence of words representing the answer. It is therefore clear that a domain-independent methodthat learns to map sequences to sequences would be useful.Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs andoutputs is known and fixed. In this paper, we show that a straightforward application of the LongShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation, and then to use another LSTM to extract the output sequencefrom that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model[28, 23, 30] except that it is conditioned on the input sequence. The LSTM\u2019s ability to successfullylearn on data with long range temporal dependencies makes it a natural choice for this applicationdue to the considerable time lag between the inputs and their corresponding outputs (fig. 1).There have been a number of related attempts to address the general sequence to sequence learningproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]who were the first to map the entire input sentence to vector, and is related to Cho et al. [5] althoughthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]introduced a novel differentiable attention mechanism that allows neural networks to focus on dif-ferent parts of their input, and an elegant variant of this idea was successfully applied to machinetranslation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another populartechnique for mapping sequences to sequences with neural networks, but it assumes a monotonicalignment between the inputs and the outputs [11].Figure 1: Our model reads an input sentence \u201cABC\u201d and produces \u201cWXYZ\u201d as the output sentence. Themodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads theinput sentence in reverse, because doing so introduces many short term dependencies in the data that make theoptimization problem much easier.The main result of this work is the following. On the WMT\u201914 English to French translation task,we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deepLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-search decoder. This is by far the best result achieved by direct translation with large neural net-works. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalizedwhenever the reference translation contained a word not covered by these 80k. This result showsthat a relatively unoptimized small-vocabulary neural network architecture which has much roomfor improvement outperforms a phrase-based SMT system.Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline onthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of otherresearchers with related architectures [26]. We were able to do well on long sentences because wereversed the order of words in the source sentence but not the target sentences in the training and testset. By doing so, we introduced many short term dependencies that made the optimization problemmuch simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble withlong sentences. The simple trick of reversing the words in the source sentence is one of the keytechnical contributions of this work.A useful property of the LSTM is that it learns to map an input sentence of variable length intoa fixed-dimensional vector representation. Given that translations tend to be paraphrases of thesource sentences, the translation objective encourages the LSTM to find sentence representationsthat capture their meaning, as sentences with similar meanings are close to each other while different2sentences meanings will be far. A qualitative evaluation supports this claim, showing that our modelis aware of word order and is fairly invariant to the active and passive voice.2 The modelThe Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neuralnetworks to sequences. Given a sequence of inputs (x1, . . . , xT ), a standard RNN computes asequence of outputs (y1, . . . , yT ) by iterating the following equation:ht = sigm(W hxxt +Whhht\u22121)yt = WyhhtThe RNN can easily map sequences to sequences whenever the alignment between the inputs theoutputs is known ahead of time. However, it is not clear how to apply an RNN to problems whoseinput and the output sequences have different lengths with complicated and non-monotonic relation-ships.The simplest strategy for general sequence learning is to map the input sequence to a fixed-sizedvector using one RNN, and then to map the vector to the target sequence with another RNN (thisapproach has also been taken by Cho et al. [5]). While it could work in principle since the RNN isprovided with all the relevant information, it would be difficult to train the RNNs due to the resultinglong term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeedin this setting.The goal of the LSTM is to estimate the conditional probability p(y1, . . . , yT \u2032 |x1, . . . , xT ) where(x1, . . . , xT ) is an input sequence and y1, . . . , yT \u2032 is its corresponding output sequence whose lengthT \u2032 may differ from T . The LSTM computes this conditional probability by first obtaining the fixed-dimensional representation v of the input sequence (x1, . . . , xT ) given by the last hidden state of theLSTM, and then computing the probability of y1, . . . , yT \u2032 with a standard LSTM-LM formulationwhose initial hidden state is set to the representation v of x1, . . . , xT :p(y1, . . . , yT \u2032 |x1, . . . , xT ) =T\u2032\u220ft=1p(yt|v, y1, . . . , yt\u22121) (1)In this equation, each p(yt|v, y1, . . . , yt\u22121) distribution is represented with a softmax over all thewords in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require thateach sentence ends with a special end-of-sentence symbol \u201c<EOS>\u201d, which enables the model todefine a distribution over sequences of all possible lengths. The overall scheme is outlined in figure1, where the shown LSTM computes the representation of \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201c<EOS>\u201d and then usesthis representation to compute the probability of \u201cW\u201d, \u201cX\u201d, \u201cY\u201d, \u201cZ\u201d, \u201c<EOS>\u201d.Our actual models differ from the above description in three important ways. First, we used twodifferent LSTMs: one for the input sequence and another for the output sequence, because doingso increases the number model parameters at negligible computational cost and makes it natural totrain the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMssignificantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we foundit extremely valuable to reverse the order of the words of the input sentence. So for example, insteadof mapping the sentence a, b, c to the sentence \u03b1, \u03b2, \u03b3, the LSTM is asked to map c, b, a to \u03b1, \u03b2, \u03b3,where \u03b1, \u03b2, \u03b3 is the translation of a, b, c. This way, a is in close proximity to \u03b1, b is fairly close to \u03b2,and so on, a fact that makes it easy for SGD to \u201cestablish communication\u201d between the input and theoutput. We found this simple data transformation to greatly improve the performance of the LSTM.3 ExperimentsWe applied our method to the WMT\u201914 English to French MT task in two ways. We used it todirectly translate the input sentence without using a reference SMT system and we it to rescore then-best lists of an SMT baseline. We report the accuracy of these translation methods, present sampletranslations, and visualize the resulting sentence representation.33.1 Dataset detailsWe used the WMT\u201914 English to French dataset. We trained our models on a subset of 12M sen-tences consisting of 348M French words and 304M English words, which is a clean \u201cselected\u201dsubset from [29]. We chose this translation task and this specific training set subset because of thepublic availability of a tokenized training and test set together with 1000-best lists from the baselineSMT [29].As typical neural language models rely on a vector representation for each word, we used a fixedvocabulary for both languages. We used 160,000 of the most frequent words for the source languageand 80,000 of the most frequent words for the target language. Every out-of-vocabulary word wasreplaced with a special \u201cUNK\u201d token.3.2 Decoding and RescoringThe core of our experiments involved training a large deep LSTM on many sentence pairs. Wetrained it by maximizing the log probability of a correct translation T given the source sentence S,so the training objective is1/|S|\u2211(T,S)\u2208Slog p(T |S)where S is the training set. Once training is complete, we produce translations by finding the mostlikely translation according to the LSTM:T\u0302 = argmaxTp(T |S) (2)We search for the most likely translation using a simple left-to-right beam search decoder whichmaintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of sometranslation. At each timestep we extend each partial hypothesis in the beam with every possibleword in the vocabulary. This greatly increases the number of the hypotheses so we discard all butthe B most likely hypotheses according to the model\u2019s log probability. As soon as the \u201c<EOS>\u201dsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of completehypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our systemperforms well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beamsearch (Table 1).We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. Torescore an n-best list, we computed the log probability of every hypothesis with our LSTM and tookan even average with their score and the LSTM\u2019s score.3.3 Reversing the Source SentencesWhile the LSTM is capable of solving problems with long term dependencies, we discovered thatthe LSTM learns much better when the source sentences are reversed (the target sentences are notreversed). By doing so, the LSTM\u2019s test perplexity dropped from 5.8 to 4.7, and the test BLEUscores of its decoded translations increased from 25.9 to 30.6.While we do not have a complete explanation to this phenomenon, we believe that it is caused bythe introduction of many short term dependencies to the dataset. Normally, when we concatenate asource sentence with a target sentence, each word in the source sentence is far from its correspondingword in the target sentence. As a result, the problem has a large \u201cminimal time lag\u201d [17]. Byreversing the words in the source sentence, the average distance between corresponding words inthe source and target language is unchanged. However, the first few words in the source languageare now very close to the first few words in the target language, so the problem\u2019s minimal time lag isgreatly reduced. Thus, backpropagation has an easier time \u201cestablishing communication\u201d betweenthe source sentence and the target sentence, which in turn results in substantially improved overallperformance.Initially, we believed that reversing the input sentences would only lead to more confident predic-tions in the early parts of the target sentence and to less confident predictions in the later parts. How-ever, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs4trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentencesresults in LSTMs with better memory utilization.3.4 Training detailsWe found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabularyof 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers torepresent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, whereeach additional layer reduced perplexity by nearly 10%, possibly due to their much larger hiddenstate. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384Mparameters of which 64M are pure recurrent connections (32M for the \u201cencoder\u201d LSTM and 32Mfor the \u201cdecoder\u201d LSTM). The complete training details are given below:\u2022 We initialized all of the LSTM\u2019s parameters with the uniform distribution between -0.08and 0.08\u2022 We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.After 5 epochs, we begun halving the learning rate every half epoch. We trained our modelsfor a total of 7.5 epochs.\u2022 We used batches of 128 sequences for the gradient and divided it the size of the batch(namely, 128).\u2022 Although LSTMs tend to not suffer from the vanishing gradient problem, they can haveexploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,25] by scaling it when its norm exceeded a threshold. For each training batch, we computes = \u2016g\u20162, where g is the gradient divided by 128. If s > 5, we set g =5gs.\u2022 Different sentences have different lengths. Most sentences are short (e.g., length 20-30)but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosentraining sentences will have many short sentences and few long sentences, and as a result,much of the computation in the minibatch is wasted. To address this problem, we made surethat all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.3.5 ParallelizationA C++ implementation of deep LSTM with the configuration from the previous section on a sin-gle GPU processes a speed of approximately 1,700 words per second. This was too slow for ourpurposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM wasexecuted on a different GPU and communicated its activations to the next GPU / layer as soon asthey were computed. Our models have 4 layers of LSTMs, each of which resides on a separateGPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsiblefor multiplying by a 1000\u00d7 20000 matrix. The resulting implementation achieved a speed of 6,300(both English and French) words per second with a minibatch size of 128. Training took about a tendays with this implementation.3.6 Experimental ResultsWe used the cased BLEU score [24] to evaluate the quality of our translations. We computed ourBLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth. This wayof evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].However, if we evaluate the best WMT\u201914 system [9] (whose predictions can be downloaded fromstatmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported bystatmt.org\\matrix.The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMsthat differ in their random initializations and in the random order of minibatches. While the decodedtranslations of the LSTM ensemble do not outperform the best WMT\u201914 system, it is the first timethat a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT1There several variants of the BLEU score, and each variant is defined with a perl script.5Method test BLEU score (ntst14)Bahdanau et al. [2] 28.45Baseline System [29] 33.30Single forward LSTM, beam size 12 26.17Single reversed LSTM, beam size 12 30.59Ensemble of 5 reversed LSTMs, beam size 1 33.00Ensemble of 2 reversed LSTMs, beam size 12 33.27Ensemble of 5 reversed LSTMs, beam size 2 34.50Ensemble of 5 reversed LSTMs, beam size 12 34.81Table 1: The performance of the LSTM on WMT\u201914 English to French test set (ntst14). Note thatan ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam ofsize 12.Method test BLEU score (ntst14)Baseline System [29] 33.30Cho et al. [5] 34.54Best WMT\u201914 result [9] 37.0Rescoring the baseline 1000-best with a single forward LSTM 35.61Rescoring the baseline 1000-best with a single reversed LSTM 35.85Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs 36.5Oracle Rescoring of the Baseline 1000-best lists \u223c45Table 2: Methods that use neural networks together with an SMT system on the WMT\u201914 Englishto French test set (ntst14).task by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM iswithin 0.5 BLEU points of the best WMT\u201914 result if it is used to rescore the 1000-best list of thebaseline system.3.7 Performance on long sentencesWe were surprised to discover that the LSTM did well on long sentences, which is shown quantita-tively in figure 3. Table 3 presents several examples of long sentences and their translations.3.8 Model Analysis\u22128 \u22126 \u22124 \u22122 0 2 4 6 8 10\u22126\u22125\u22124\u22123\u22122\u2212101234John respects MaryMary respects JohnJohn admires MaryMary admires JohnMary is in love with JohnJohn is in love with Mary\u221215 \u221210 \u22125 0 5 10 15 20\u221220\u221215\u221210\u22125051015I gave her a card in the gardenIn the garden , I gave her a cardShe was given a card by me in the gardenShe gave me a card in the gardenIn the garden , she gave me a cardI was given a card by her in the gardenFigure 2: The figure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtainedafter processing the phrases in the figures. The phrases are clustered by meaning, which in these examples isprimarily a function of word order, which would be difficult to capture with a bag-of-words model. Notice thatboth clusters have similar internal structure.One of the attractive features of our model is its ability to turn a sequence of words into a vectorof fixed dimensionality. Figure 2 visualizes some of the learned representations. The figure clearlyshows that the representations are sensitive to the order of words, while being fairly insensitive to the6Type SentenceOur model Ulrich UNK , membre du conseil d\u2019 administration du constructeur automobile Audi ,affirme qu\u2019 il s\u2019 agit d\u2019 une pratique courante depuis des anne\u0301es pour que les te\u0301le\u0301phonesportables puissent e\u0302tre collecte\u0301s avant les re\u0301unions du conseil d\u2019 administration afin qu\u2019 ilsne soient pas utilise\u0301s comme appareils d\u2019 e\u0301coute a\u0300 distance .Truth Ulrich Hackenberg , membre du conseil d\u2019 administration du constructeur automobile Audi ,de\u0301clare que la collecte des te\u0301le\u0301phones portables avant les re\u0301unions du conseil , afin qu\u2019 ilsne puissent pas e\u0302tre utilise\u0301s comme appareils d\u2019 e\u0301coute a\u0300 distance , est une pratique courantedepuis des anne\u0301es .Our model \u201c Les te\u0301le\u0301phones cellulaires , qui sont vraiment une question , non seulement parce qu\u2019 ilspourraient potentiellement causer des interfe\u0301rences avec les appareils de navigation , maisnous savons , selon la FCC , qu\u2019 ils pourraient interfe\u0301rer avec les tours de te\u0301le\u0301phone cellulairelorsqu\u2019 ils sont dans l\u2019 air \u201d , dit UNK .Truth \u201c Les te\u0301le\u0301phones portables sont ve\u0301ritablement un proble\u0300me , non seulement parce qu\u2019 ilspourraient e\u0301ventuellement cre\u0301er des interfe\u0301rences avec les instruments de navigation , maisparce que nous savons , d\u2019 apre\u0300s la FCC , qu\u2019 ils pourraient perturber les antennes-relais dete\u0301le\u0301phonie mobile s\u2019 ils sont utilise\u0301s a\u0300 bord \u201d , a de\u0301clare\u0301 Rosenker .Our model Avec la cre\u0301mation , il y a un \u201c sentiment de violence contre le corps d\u2019 un e\u0302tre cher \u201d ,qui sera \u201c re\u0301duit a\u0300 une pile de cendres \u201d en tre\u0300s peu de temps au lieu d\u2019 un processus dede\u0301composition \u201c qui accompagnera les e\u0301tapes du deuil \u201d .Truth Il y a , avec la cre\u0301mation , \u201c une violence faite au corps aime\u0301 \u201d ,qui va e\u0302tre \u201c re\u0301duit a\u0300 un tas de cendres \u201d en tre\u0300s peu de temps , et non apre\u0300s un processus dede\u0301composition , qui \u201c accompagnerait les phases du deuil \u201d .Table 3: A few examples of long translations produced by the LSTM alongside the ground truthtranslations. The reader can verify that the translations are sensible using Google translate.4 7 8 12 17 22 28 35 79test sentences sorted by their length2025303540BLEU scoreLSTM  (34.8)baseline (33.3)0 500 1000 1500 2000 2500 3000 3500test sentences sorted by average word frequency rank2025303540BLEU scoreLSTM  (34.8)baseline (33.3)Figure 3: The left plot shows the performance of our system as a function of sentence length, where thex-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.There is no degradation on sentences with less than 35 words, there is only a minor degradation on the longestsentences. The right plot shows the LSTM\u2019s performance on sentences with progressively more rare words,where the x-axis corresponds to the test sentences sorted by their \u201caverage word frequency rank\u201d.replacement of an active voice with a passive voice. The two-dimensional projections are obtainedusing PCA.4 Related workThere is a large body of work on applications of neural networks to machine translation. So far,the simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a7Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the n-best lists of a strong MT baseline [22], which reliably improves translation quality.More recently, researchers have begun to look into ways of including information about the sourcelanguage into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLMwith a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8]followed a similar approach, but they incorporated their NNLM into the decoder of an MT systemand used the decoder\u2019s alignment information to provide the NNLM with the most useful words inthe input sentence. Their approach was highly successful and it achieved large improvements overtheir baseline.Our work is closely related to Kalchbrenner and Blunsom [18], who were the first to map the inputsentence into a vector and then back to a sentence, although they map sentences to vectors usingconvolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho etal. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although theirprimary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] alsoattempted direct translations with a neural network that used an attention mechanism to overcomethe poor performance on long sentences experienced by Cho et al. [5] and achieved encouragingresults. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho etal. [5] by translating pieces of the source sentence in way that produces smooth translations, whichis similar to a phrase-based approach. We suspect that they could achieve similar improvements bysimply training their networks on reversed source sentences.End-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs andoutputs by feedforward networks, and map them to similar points in space. However, their approachcannot generate translations directly: to get a translation, they need to do a look up for closest vectorin the pre-computed database of sentences, or to rescore a sentence.5 ConclusionIn this work, we showed that a large deep LSTM, that has a limited vocabulary and that makesalmost no assumption about problem structure can outperform a standard SMT-based system whosevocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approachon MT suggests that it should do well on many other sequence learning problems, provided theyhave enough training data.We were surprised by the extent of the improvement obtained by reversing the words in the sourcesentences. We conclude that it is important to find a problem encoding that has the greatest numberof short term dependencies, as they make the learning problem much simpler. In particular, whilewe were unable to train a standard RNN on the non-reversed translation problem (shown in fig. 1),we believe that a standard RNN should be easily trainable when the source sentences are reversed(although we did not verify it experimentally).We were also surprised by the ability of the LSTM to correctly translate very long sentences. Wewere initially convinced that the LSTM would fail on long sentences due to its limited memory,and other researchers reported poor performance on long sentences with a model similar to ours[5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little difficulty translating longsentences.Most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized ap-proach can outperform an SMT system, so further work will likely lead to even greater translationaccuracies. These results suggest that our approach will likely do well on other challenging sequenceto sequence problems.6 AcknowledgmentsWe thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolf-gang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain teamfor useful comments and discussions.8References[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrentneural networks. In EMNLP, 2013.[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.arXiv preprint arXiv:1409.0473, 2014.[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In Journal ofMachine Learning Research, pages 1137\u20131155, 2003.[4] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult.IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.[5] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase represen-tations using RNN encoder-decoder for statistical machine translation. In Arxiv preprint arXiv:1406.1078,2014.[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.In CVPR, 2012.[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for largevocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - SpecialIssue on Deep Learning for Speech and Language Processing, 2012.[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural networkjoint models for statistical machine translation. In ACL, 2014.[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh\u2019s phrase-based machinetranslation systems for wmt-14. In WMT, 2014.[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,2013.[11] A. Graves, S. Ferna\u0301ndez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: labellingunsegmented sequence data with recurrent neural networks. In ICML, 2006.[12] K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. InICLR, 2014.[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEESignal Processing Magazine, 2012.[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master\u2019s thesis, Institut fur Infor-matik, Technische Universitat, Munchen, 1991.[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the difficultyof learning long-term dependencies, 2001.[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997.[18] N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, 2013.[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neuralnetworks. In NIPS, 2012.[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Buildinghigh-level features using large scale unsupervised learning. In ICML, 2012.[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.Proceedings of the IEEE, 1998.[22] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University ofTechnology, 2012.[23] T. Mikolov, M. Karafia\u0301t, L. Burget, J. Cernocky\u0300, and S. Khudanpur. Recurrent neural network basedlanguage model. In INTERSPEECH, pages 1045\u20131048, 2010.[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machinetranslation. In ACL, 2002.[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. arXivpreprint arXiv:1211.5063, 2012.[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio. Overcoming thecurse of sentence length for neural machine translation using automatic segmentation. arXiv preprintarXiv:1409.1257, 2014.[27] A. Razborov. On small depth threshold circuits. In Proc. 3rd Scandinavian Workshop on AlgorithmTheory, 1992.[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.Nature, 323(6088):533\u2013536, 1986.[29] H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/\u02dcschwenk/cslm_joint_paper/, 2014. [Online; accessed 03-September-2014].[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTER-SPEECH, 2010.[31] P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of IEEE, 1990.9"