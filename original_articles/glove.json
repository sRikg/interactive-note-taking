"\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGlove: Global Vectors for Word Representation\\n\\n\\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\\u20131543,\\nOctober 25-29, 2014, Doha, Qatar. c\\u00a92014 Association for Computational Linguistics\\n\\nGloVe: Global Vectors for Word Representation\\n\\nJeffrey Pennington, Richard Socher, Christopher D. Manning\\nComputer Science Department, Stanford University, Stanford, CA 94305\\n\\njpennin@stanford.edu, richard@socher.org, manning@stanford.edu\\n\\nAbstract\\n\\nRecent methods for learning vector space\\nrepresentations of words have succeeded\\nin capturing fine-grained semantic and\\nsyntactic regularities using vector arith-\\nmetic, but the origin of these regularities\\nhas remained opaque. We analyze and\\nmake explicit the model properties needed\\nfor such regularities to emerge in word\\nvectors. The result is a new global log-\\nbilinear regression model that combines\\nthe advantages of the two major model\\nfamilies in the literature: global matrix\\nfactorization and local context window\\nmethods. Our model efficiently leverages\\nstatistical information by training only on\\nthe nonzero elements in a word-word co-\\noccurrence matrix, rather than on the en-\\ntire sparse matrix or on individual context\\nwindows in a large corpus. The model pro-\\nduces a vector space with meaningful sub-\\nstructure, as evidenced by its performance\\nof 75% on a recent word analogy task. It\\nalso outperforms related models on simi-\\nlarity tasks and named entity recognition.\\n\\n1 Introduction\\n\\nSemantic vector space models of language repre-\\nsent each word with a real-valued vector. These\\nvectors can be used as features in a variety of ap-\\nplications, such as information retrieval (Manning\\net al., 2008), document classification (Sebastiani,\\n2002), question answering (Tellex et al., 2003),\\nnamed entity recognition (Turian et al., 2010), and\\nparsing (Socher et al., 2013).\\n\\nMost word vector methods rely on the distance\\nor angle between pairs of word vectors as the pri-\\nmary method for evaluating the intrinsic quality\\nof such a set of word representations. Recently,\\nMikolov et al. (2013c) introduced a new evalua-\\ntion scheme based on word analogies that probes\\n\\nthe finer structure of the word vector space by ex-\\namining not the scalar distance between word vec-\\ntors, but rather their various dimensions of dif-\\nference. For example, the analogy \\u201cking is to\\nqueen as man is to woman\\u201d should be encoded\\nin the vector space by the vector equation king \\u2212\\nqueen = man \\u2212 woman. This evaluation scheme\\nfavors models that produce dimensions of mean-\\ning, thereby capturing the multi-clustering idea of\\ndistributed representations (Bengio, 2009).\\n\\nThe two main model families for learning word\\nvectors are: 1) global matrix factorization meth-\\nods, such as latent semantic analysis (LSA) (Deer-\\nwester et al., 1990) and 2) local context window\\nmethods, such as the skip-gram model of Mikolov\\net al. (2013c). Currently, both families suffer sig-\\nnificant drawbacks. While methods like LSA ef-\\nficiently leverage statistical information, they do\\nrelatively poorly on the word analogy task, indi-\\ncating a sub-optimal vector space structure. Meth-\\nods like skip-gram may do better on the analogy\\ntask, but they poorly utilize the statistics of the cor-\\npus since they train on separate local context win-\\ndows instead of on global co-occurrence counts.\\n\\nIn this work, we analyze the model properties\\nnecessary to produce linear directions of meaning\\nand argue that global log-bilinear regression mod-\\nels are appropriate for doing so. We propose a spe-\\ncific weighted least squares model that trains on\\nglobal word-word co-occurrence counts and thus\\nmakes efficient use of statistics. The model pro-\\nduces a word vector space with meaningful sub-\\nstructure, as evidenced by its state-of-the-art per-\\nformance of 75% accuracy on the word analogy\\ndataset. We also demonstrate that our methods\\noutperform other current methods on several word\\nsimilarity tasks, and also on a common named en-\\ntity recognition (NER) benchmark.\\n\\nWe provide the source code for the model as\\nwell as trained word vectors at http://nlp.\\nstanford.edu/projects/glove/.\\n\\n1532\\n\\n\\n\\n2 Related Work\\n\\nMatrix Factorization Methods. Matrix factor-\\nization methods for generating low-dimensional\\nword representations have roots stretching as far\\nback as LSA. These methods utilize low-rank ap-\\nproximations to decompose large matrices that\\ncapture statistical information about a corpus. The\\nparticular type of information captured by such\\nmatrices varies by application. In LSA, the ma-\\ntrices are of \\u201cterm-document\\u201d type, i.e., the rows\\ncorrespond to words or terms, and the columns\\ncorrespond to different documents in the corpus.\\nIn contrast, the Hyperspace Analogue to Language\\n(HAL) (Lund and Burgess, 1996), for example,\\nutilizes matrices of \\u201cterm-term\\u201d type, i.e., the rows\\nand columns correspond to words and the entries\\ncorrespond to the number of times a given word\\noccurs in the context of another given word.\\n\\nA main problem with HAL and related meth-\\nods is that the most frequent words contribute a\\ndisproportionate amount to the similarity measure:\\nthe number of times two words co-occur with the\\nor and, for example, will have a large effect on\\ntheir similarity despite conveying relatively little\\nabout their semantic relatedness. A number of\\ntechniques exist that addresses this shortcoming of\\nHAL, such as the COALS method (Rohde et al.,\\n2006), in which the co-occurrence matrix is first\\ntransformed by an entropy- or correlation-based\\nnormalization. An advantage of this type of trans-\\nformation is that the raw co-occurrence counts,\\nwhich for a reasonably sized corpus might span\\n8 or 9 orders of magnitude, are compressed so as\\nto be distributed more evenly in a smaller inter-\\nval. A variety of newer models also pursue this\\napproach, including a study (Bullinaria and Levy,\\n2007) that indicates that positive pointwise mu-\\ntual information (PPMI) is a good transformation.\\nMore recently, a square root type transformation\\nin the form of Hellinger PCA (HPCA) (Lebret and\\nCollobert, 2014) has been suggested as an effec-\\ntive way of learning word representations.\\n\\nShallow Window-Based Methods. Another\\napproach is to learn word representations that aid\\nin making predictions within local context win-\\ndows. For example, Bengio et al. (2003) intro-\\nduced a model that learns word vector representa-\\ntions as part of a simple neural network architec-\\nture for language modeling. Collobert and Weston\\n(2008) decoupled the word vector training from\\nthe downstream training objectives, which paved\\n\\nthe way for Collobert et al. (2011) to use the full\\ncontext of a word for learning the word represen-\\ntations, rather than just the preceding context as is\\nthe case with language models.\\n\\nRecently, the importance of the full neural net-\\nwork structure for learning useful word repre-\\nsentations has been called into question. The\\nskip-gram and continuous bag-of-words (CBOW)\\nmodels of Mikolov et al. (2013a) propose a sim-\\nple single-layer architecture based on the inner\\nproduct between two word vectors. Mnih and\\nKavukcuoglu (2013) also proposed closely-related\\nvector log-bilinear models, vLBL and ivLBL, and\\nLevy et al. (2014) proposed explicit word embed-\\ndings based on a PPMI metric.\\n\\nIn the skip-gram and ivLBL models, the objec-\\ntive is to predict a word\\u2019s context given the word\\nitself, whereas the objective in the CBOW and\\nvLBL models is to predict a word given its con-\\ntext. Through evaluation on a word analogy task,\\nthese models demonstrated the capacity to learn\\nlinguistic patterns as linear relationships between\\nthe word vectors.\\n\\nUnlike the matrix factorization methods, the\\nshallow window-based methods suffer from the\\ndisadvantage that they do not operate directly on\\nthe co-occurrence statistics of the corpus. Instead,\\nthese models scan context windows across the en-\\ntire corpus, which fails to take advantage of the\\nvast amount of repetition in the data.\\n\\n3 The GloVe Model\\n\\nThe statistics of word occurrences in a corpus is\\nthe primary source of information available to all\\nunsupervised methods for learning word represen-\\ntations, and although many such methods now ex-\\nist, the question still remains as to how meaning\\nis generated from these statistics, and how the re-\\nsulting word vectors might represent that meaning.\\nIn this section, we shed some light on this ques-\\ntion. We use our insights to construct a new model\\nfor word representation which we call GloVe, for\\nGlobal Vectors, because the global corpus statis-\\ntics are captured directly by the model.\\n\\nFirst we establish some notation. Let the matrix\\nof word-word co-occurrence counts be denoted by\\nX , whose entries Xi j tabulate the number of times\\nword j occurs in the context of word i. Let Xi =\\u2211\\n\\nk Xik be the number of times any word appears\\nin the context of word i. Finally, let Pi j = P( j |i) =\\nXi j/Xi be the probability that word j appear in the\\n\\n1533\\n\\n\\n\\nTable 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6\\nbillion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion\\ncancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and\\nsmall values (much less than 1) correlate well with properties specific of steam.\\n\\nProbability and Ratio k = solid k = gas k = water k = fashion\\n\\nP(k |ice) 1.9 \\u00d7 10\\u22124 6.6 \\u00d7 10\\u22125 3.0 \\u00d7 10\\u22123 1.7 \\u00d7 10\\u22125\\nP(k |steam) 2.2 \\u00d7 10\\u22125 7.8 \\u00d7 10\\u22124 2.2 \\u00d7 10\\u22123 1.8 \\u00d7 10\\u22125\\nP(k |ice)/P(k |steam) 8.9 8.5 \\u00d7 10\\u22122 1.36 0.96\\n\\ncontext of word i.\\nWe begin with a simple example that showcases\\n\\nhow certain aspects of meaning can be extracted\\ndirectly from co-occurrence probabilities. Con-\\nsider two words i and j that exhibit a particular as-\\npect of interest; for concreteness, suppose we are\\ninterested in the concept of thermodynamic phase,\\nfor which we might take i = ice and j = steam.\\nThe relationship of these words can be examined\\nby studying the ratio of their co-occurrence prob-\\nabilities with various probe words, k. For words\\nk related to ice but not steam, say k = solid, we\\nexpect the ratio Pik/Pjk will be large. Similarly,\\nfor words k related to steam but not ice, say k =\\ngas, the ratio should be small. For words k like\\nwater or fashion, that are either related to both ice\\nand steam, or to neither, the ratio should be close\\nto one. Table 1 shows these probabilities and their\\nratios for a large corpus, and the numbers confirm\\nthese expectations. Compared to the raw probabil-\\nities, the ratio is better able to distinguish relevant\\nwords (solid and gas) from irrelevant words (water\\nand fashion) and it is also better able to discrimi-\\nnate between the two relevant words.\\n\\nThe above argument suggests that the appropri-\\nate starting point for word vector learning should\\nbe with ratios of co-occurrence probabilities rather\\nthan the probabilities themselves. Noting that the\\nratio Pik/Pjk depends on three words i, j, and k,\\nthe most general model takes the form,\\n\\nF (wi ,w j , w\\u0303k ) =\\nPik\\nPjk\\n\\n, (1)\\n\\nwhere w \\u2208 Rd are word vectors and w\\u0303 \\u2208 Rd\\nare separate context word vectors whose role will\\nbe discussed in Section 4.2. In this equation, the\\nright-hand side is extracted from the corpus, and\\nF may depend on some as-of-yet unspecified pa-\\nrameters. The number of possibilities for F is vast,\\nbut by enforcing a few desiderata we can select a\\nunique choice. First, we would like F to encode\\n\\nthe information present the ratio Pik/Pjk in the\\nword vector space. Since vector spaces are inher-\\nently linear structures, the most natural way to do\\nthis is with vector differences. With this aim, we\\ncan restrict our consideration to those functions F\\nthat depend only on the difference of the two target\\nwords, modifying Eqn. (1) to,\\n\\nF (wi \\u2212 w j , w\\u0303k ) =\\nPik\\nPjk\\n\\n. (2)\\n\\nNext, we note that the arguments of F in Eqn. (2)\\nare vectors while the right-hand side is a scalar.\\nWhile F could be taken to be a complicated func-\\ntion parameterized by, e.g., a neural network, do-\\ning so would obfuscate the linear structure we are\\ntrying to capture. To avoid this issue, we can first\\ntake the dot product of the arguments,\\n\\nF\\n(\\n(wi \\u2212 w j )T w\\u0303k\\n\\n)\\n=\\n\\nPik\\nPjk\\n\\n, (3)\\n\\nwhich prevents F from mixing the vector dimen-\\nsions in undesirable ways. Next, note that for\\nword-word co-occurrence matrices, the distinction\\nbetween a word and a context word is arbitrary and\\nthat we are free to exchange the two roles. To do so\\nconsistently, we must not only exchange w \\u2194 w\\u0303\\nbut also X \\u2194 XT . Our final model should be in-\\nvariant under this relabeling, but Eqn. (3) is not.\\nHowever, the symmetry can be restored in two\\nsteps. First, we require that F be a homomorphism\\nbetween the groups (R,+) and (R>0,\\u00d7 ), i.e.,\\n\\nF\\n(\\n(wi \\u2212 w j )T w\\u0303k\\n\\n)\\n=\\n\\nF (wT\\ni\\nw\\u0303k )\\n\\nF (wT\\nj\\nw\\u0303k )\\n\\n, (4)\\n\\nwhich, by Eqn. (3), is solved by,\\n\\nF (wTi w\\u0303k ) = Pik =\\nXik\\nXi\\n\\n. (5)\\n\\nThe solution to Eqn. (4) is F = exp, or,\\n\\nwTi w\\u0303k = log(Pik ) = log(Xik ) \\u2212 log(Xi ) . (6)\\n\\n1534\\n\\n\\n\\nNext, we note that Eqn. (6) would exhibit the ex-\\nchange symmetry if not for the log(Xi ) on the\\nright-hand side. However, this term is indepen-\\ndent of k so it can be absorbed into a bias bi for\\nwi . Finally, adding an additional bias b\\u0303k for w\\u0303k\\nrestores the symmetry,\\n\\nwTi w\\u0303k + bi + b\\u0303k = log(Xik ) . (7)\\n\\nEqn. (7) is a drastic simplification over Eqn. (1),\\nbut it is actually ill-defined since the logarithm di-\\nverges whenever its argument is zero. One reso-\\nlution to this issue is to include an additive shift\\nin the logarithm, log(Xik ) \\u2192 log(1 + Xik ), which\\nmaintains the sparsity of X while avoiding the di-\\nvergences. The idea of factorizing the log of the\\nco-occurrence matrix is closely related to LSA and\\nwe will use the resulting model as a baseline in\\nour experiments. A main drawback to this model\\nis that it weighs all co-occurrences equally, even\\nthose that happen rarely or never. Such rare co-\\noccurrences are noisy and carry less information\\nthan the more frequent ones \\u2014 yet even just the\\nzero entries account for 75\\u201395% of the data in X ,\\ndepending on the vocabulary size and corpus.\\n\\nWe propose a new weighted least squares re-\\ngression model that addresses these problems.\\nCasting Eqn. (7) as a least squares problem and\\nintroducing a weighting function f (Xi j ) into the\\ncost function gives us the model\\n\\nJ =\\nV\\u2211\\n\\ni, j=1\\n\\nf\\n(\\nXi j\\n\\n) (\\nwTi w\\u0303 j + bi + b\\u0303j \\u2212 log Xi j\\n\\n)2\\n,\\n\\n(8)\\nwhere V is the size of the vocabulary. The weight-\\ning function should obey the following properties:\\n\\n1. f (0) = 0. If f is viewed as a continuous\\nfunction, it should vanish as x \\u2192 0 fast\\nenough that the limx\\u21920 f (x) log\\n\\n2 x is finite.\\n\\n2. f (x) should be non-decreasing so that rare\\nco-occurrences are not overweighted.\\n\\n3. f (x) should be relatively small for large val-\\nues of x, so that frequent co-occurrences are\\nnot overweighted.\\n\\nOf course a large number of functions satisfy these\\nproperties, but one class of functions that we found\\nto work well can be parameterized as,\\n\\nf (x) =\\n{\\n\\n(x/xmax)\\u03b1 if x < xmax\\n1 otherwise .\\n\\n(9)\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\n0.0\\n\\nFigure 1: Weighting function f with \\u03b1 = 3/4.\\n\\nThe performance of the model depends weakly on\\nthe cutoff, which we fix to xmax = 100 for all our\\nexperiments. We found that \\u03b1 = 3/4 gives a mod-\\nest improvement over a linear version with \\u03b1 = 1.\\nAlthough we offer only empirical motivation for\\nchoosing the value 3/4, it is interesting that a sim-\\nilar fractional power scaling was found to give the\\nbest performance in (Mikolov et al., 2013a).\\n\\n3.1 Relationship to Other Models\\n\\nBecause all unsupervised methods for learning\\nword vectors are ultimately based on the occur-\\nrence statistics of a corpus, there should be com-\\nmonalities between the models. Nevertheless, cer-\\ntain models remain somewhat opaque in this re-\\ngard, particularly the recent window-based meth-\\nods like skip-gram and ivLBL. Therefore, in this\\nsubsection we show how these models are related\\nto our proposed model, as defined in Eqn. (8).\\n\\nThe starting point for the skip-gram or ivLBL\\nmethods is a model Qi j for the probability that\\nword j appears in the context of word i. For con-\\ncreteness, let us assume that Qi j is a softmax,\\n\\nQi j =\\nexp(wT\\n\\ni\\nw\\u0303 j )\\u2211V\\n\\nk=1 exp(w\\nT\\ni\\nw\\u0303k )\\n\\n. (10)\\n\\nMost of the details of these models are irrelevant\\nfor our purposes, aside from the the fact that they\\nattempt to maximize the log probability as a con-\\ntext window scans over the corpus. Training pro-\\nceeds in an on-line, stochastic fashion, but the im-\\nplied global objective function can be written as,\\n\\nJ = \\u2212\\n\\u2211\\n\\ni\\u2208corpus\\nj\\u2208context(i)\\n\\nlog Qi j . (11)\\n\\nEvaluating the normalization factor of the soft-\\nmax for each term in this sum is costly. To al-\\nlow for efficient training, the skip-gram and ivLBL\\nmodels introduce approximations to Qi j . How-\\never, the sum in Eqn. (11) can be evaluated much\\n\\n1535\\n\\n\\n\\nmore efficiently if we first group together those\\nterms that have the same values for i and j,\\n\\nJ = \\u2212\\nV\\u2211\\ni=1\\n\\nV\\u2211\\nj=1\\n\\nXi j log Qi j , (12)\\n\\nwhere we have used the fact that the number of\\nlike terms is given by the co-occurrence matrix X .\\n\\nRecalling our notation for Xi =\\n\\u2211\\n\\nk Xik and\\nPi j = Xi j/Xi , we can rewrite J as,\\n\\nJ = \\u2212\\nV\\u2211\\ni=1\\n\\nXi\\nV\\u2211\\nj=1\\n\\nPi j log Qi j =\\nV\\u2211\\ni=1\\n\\nXiH (Pi ,Qi ) ,\\n\\n(13)\\nwhere H (Pi ,Qi ) is the cross entropy of the dis-\\ntributions Pi and Qi , which we define in analogy\\nto Xi . As a weighted sum of cross-entropy error,\\nthis objective bears some formal resemblance to\\nthe weighted least squares objective of Eqn. (8).\\nIn fact, it is possible to optimize Eqn. (13) directly\\nas opposed to the on-line training methods used in\\nthe skip-gram and ivLBL models. One could inter-\\npret this objective as a \\u201cglobal skip-gram\\u201d model,\\nand it might be interesting to investigate further.\\nOn the other hand, Eqn. (13) exhibits a number of\\nundesirable properties that ought to be addressed\\nbefore adopting it as a model for learning word\\nvectors.\\n\\nTo begin, cross entropy error is just one among\\nmany possible distance measures between prob-\\nability distributions, and it has the unfortunate\\nproperty that distributions with long tails are of-\\nten modeled poorly with too much weight given\\nto the unlikely events. Furthermore, for the mea-\\nsure to be bounded it requires that the model dis-\\ntribution Q be properly normalized. This presents\\na computational bottleneck owing to the sum over\\nthe whole vocabulary in Eqn. (10), and it would be\\ndesirable to consider a different distance measure\\nthat did not require this property of Q. A natural\\nchoice would be a least squares objective in which\\nnormalization factors in Q and P are discarded,\\n\\nJ\\u0302 =\\n\\u2211\\ni, j\\n\\nXi\\n(\\nP\\u0302i j \\u2212 Q\\u0302i j\\n\\n)2 (14)\\nwhere P\\u0302i j = Xi j and Q\\u0302i j = exp(wTi w\\u0303 j ) are the\\nunnormalized distributions. At this stage another\\nproblem emerges, namely that Xi j often takes very\\nlarge values, which can complicate the optimiza-\\ntion. An effective remedy is to minimize the\\n\\nsquared error of the logarithms of P\\u0302 and Q\\u0302 instead,\\n\\nJ\\u0302 =\\n\\u2211\\ni, j\\n\\nXi\\n(\\n\\nlog P\\u0302i j \\u2212 log Q\\u0302i j\\n)2\\n\\n=\\n\\u2211\\ni, j\\n\\nXi\\n(\\nwTi w\\u0303 j \\u2212 log Xi j\\n\\n)2\\n. (15)\\n\\nFinally, we observe that while the weighting factor\\nXi is preordained by the on-line training method\\ninherent to the skip-gram and ivLBL models, it is\\nby no means guaranteed to be optimal. In fact,\\nMikolov et al. (2013a) observe that performance\\ncan be increased by filtering the data so as to re-\\nduce the effective value of the weighting factor for\\nfrequent words. With this in mind, we introduce\\na more general weighting function, which we are\\nfree to take to depend on the context word as well.\\nThe result is,\\n\\nJ\\u0302 =\\n\\u2211\\ni, j\\n\\nf (Xi j )\\n(\\nwTi w\\u0303 j \\u2212 log Xi j\\n\\n)2\\n, (16)\\n\\nwhich is equivalent1 to the cost function of\\nEqn. (8), which we derived previously.\\n\\n3.2 Complexity of the model\\nAs can be seen from Eqn. (8) and the explicit form\\nof the weighting function f (X ), the computational\\ncomplexity of the model depends on the number of\\nnonzero elements in the matrix X . As this num-\\nber is always less than the total number of en-\\ntries of the matrix, the model scales no worse than\\nO( |V |2). At first glance this might seem like a sub-\\nstantial improvement over the shallow window-\\nbased approaches, which scale with the corpus\\nsize, |C |. However, typical vocabularies have hun-\\ndreds of thousands of words, so that |V |2 can be in\\nthe hundreds of billions, which is actually much\\nlarger than most corpora. For this reason it is im-\\nportant to determine whether a tighter bound can\\nbe placed on the number of nonzero elements of\\nX .\\n\\nIn order to make any concrete statements about\\nthe number of nonzero elements in X , it is neces-\\nsary to make some assumptions about the distribu-\\ntion of word co-occurrences. In particular, we will\\nassume that the number of co-occurrences of word\\ni with word j, Xi j , can be modeled as a power-law\\nfunction of the frequency rank of that word pair,\\nri j :\\n\\nXi j =\\nk\\n\\n(ri j )\\u03b1\\n. (17)\\n\\n1We could also include bias terms in Eqn. (16).\\n\\n1536\\n\\n\\n\\nThe total number of words in the corpus is pro-\\nportional to the sum over all elements of the co-\\noccurrence matrix X ,\\n\\n|C | \\u223c\\n\\u2211\\ni j\\n\\nXi j =\\n|X |\\u2211\\nr=1\\n\\nk\\nr\\u03b1\\n= kH|X |,\\u03b1 , (18)\\n\\nwhere we have rewritten the last sum in terms of\\nthe generalized harmonic number Hn,m . The up-\\nper limit of the sum, |X |, is the maximum fre-\\nquency rank, which coincides with the number of\\nnonzero elements in the matrix X . This number is\\nalso equal to the maximum value of r in Eqn. (17)\\nsuch that Xi j \\u2265 1, i.e., |X | = k1/\\u03b1 . Therefore we\\ncan write Eqn. (18) as,\\n\\n|C | \\u223c |X |\\u03b1 H|X |,\\u03b1 . (19)\\nWe are interested in how |X | is related to |C | when\\nboth numbers are large; therefore we are free to\\nexpand the right hand side of the equation for large\\n|X |. For this purpose we use the expansion of gen-\\neralized harmonic numbers (Apostol, 1976),\\n\\nHx,s =\\nx1\\u2212s\\n\\n1 \\u2212 s + \\u03b6 (s) + O(x\\n\\u2212s ) if s > 0, s , 1 ,\\n\\n(20)\\ngiving,\\n\\n|C | \\u223c |X |\\n1 \\u2212 \\u03b1 + \\u03b6 (\\u03b1) |X |\\n\\n\\u03b1 + O(1) , (21)\\n\\nwhere \\u03b6 (s) is the Riemann zeta function. In the\\nlimit that X is large, only one of the two terms on\\nthe right hand side of Eqn. (21) will be relevant,\\nand which term that is depends on whether \\u03b1 > 1,\\n\\n|X | =\\n{ O(|C |) if \\u03b1 < 1,\\nO(|C |1/\\u03b1 ) if \\u03b1 > 1. (22)\\n\\nFor the corpora studied in this article, we observe\\nthat Xi j is well-modeled by Eqn. (17) with \\u03b1 =\\n1.25. In this case we have that |X | = O(|C |0.8).\\nTherefore we conclude that the complexity of the\\nmodel is much better than the worst case O(V 2),\\nand in fact it does somewhat better than the on-line\\nwindow-based methods which scale like O(|C |).\\n4 Experiments\\n\\n4.1 Evaluation methods\\nWe conduct experiments on the word analogy\\ntask of Mikolov et al. (2013a), a variety of word\\nsimilarity tasks, as described in (Luong et al.,\\n2013), and on the CoNLL-2003 shared benchmark\\n\\nTable 2: Results on the word analogy task, given\\nas percent accuracy. Underlined scores are best\\nwithin groups of similarly-sized models; bold\\nscores are best overall. HPCA vectors are publicly\\navailable2; (i)vLBL results are from (Mnih et al.,\\n2013); skip-gram (SG) and CBOW results are\\nfrom (Mikolov et al., 2013a,b); we trained SG\\u2020\\n\\nand CBOW\\u2020 using the word2vec tool3. See text\\nfor details and a description of the SVD models.\\n\\nModel Dim. Size Sem. Syn. Tot.\\nivLBL 100 1.5B 55.9 50.1 53.2\\nHPCA 100 1.6B 4.2 16.4 10.8\\nGloVe 100 1.6B 67.5 54.3 60.3\\n\\nSG 300 1B 61 61 61\\nCBOW 300 1.6B 16.1 52.6 36.1\\nvLBL 300 1.5B 54.2 64.8 60.0\\nivLBL 300 1.5B 65.2 63.0 64.0\\nGloVe 300 1.6B 80.8 61.5 70.3\\nSVD 300 6B 6.3 8.1 7.3\\n\\nSVD-S 300 6B 36.7 46.6 42.1\\nSVD-L 300 6B 56.6 63.0 60.1\\nCBOW\\u2020 300 6B 63.6 67.4 65.7\\n\\nSG\\u2020 300 6B 73.0 66.0 69.1\\nGloVe 300 6B 77.4 67.0 71.7\\nCBOW 1000 6B 57.3 68.9 63.7\\n\\nSG 1000 6B 66.1 65.1 65.6\\nSVD-L 300 42B 38.4 58.2 49.2\\nGloVe 300 42B 81.9 69.3 75.0\\n\\ndataset for NER (Tjong Kim Sang and De Meul-\\nder, 2003).\\n\\nWord analogies. The word analogy task con-\\nsists of questions like, \\u201ca is to b as c is to ?\\u201d\\nThe dataset contains 19,544 such questions, di-\\nvided into a semantic subset and a syntactic sub-\\nset. The semantic questions are typically analogies\\nabout people or places, like \\u201cAthens is to Greece\\nas Berlin is to ?\\u201d. The syntactic questions are\\ntypically analogies about verb tenses or forms of\\nadjectives, for example \\u201cdance is to dancing as fly\\nis to ?\\u201d. To correctly answer the question, the\\nmodel should uniquely identify the missing term,\\nwith only an exact correspondence counted as a\\ncorrect match. We answer the question \\u201ca is to b\\nas c is to ?\\u201d by finding the word d whose repre-\\nsentation wd is closest to wb \\u2212 wa + wc according\\nto the cosine similarity.4\\n\\n2http://lebret.ch/words/\\n3http://code.google.com/p/word2vec/\\n4Levy et al. (2014) introduce a multiplicative analogy\\n\\nevaluation, 3COSMUL, and report an accuracy of 68.24% on\\n\\n1537\\n\\n\\n\\n0 100 200 300 400 500 600\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\nVector Dimension\\n\\nA\\ncc\\n\\nur\\nac\\n\\ny \\n[%\\n\\n]\\n\\n \\n\\nSemantic\\nSyntactic\\nOverall\\n\\n(a) Symmetric context\\n\\n2 4 6 8 10\\n40\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n45\\n\\nWindow Size\\n\\nA\\ncc\\n\\nur\\nac\\n\\ny \\n[%\\n\\n]\\n\\n \\n\\n \\n\\n \\n\\nSemantic\\nSyntactic\\nOverall\\n\\n(b) Symmetric context\\n\\n2 4 6 8 10\\n40\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n45\\n\\nWindow Size\\n\\nA\\ncc\\n\\nur\\nac\\n\\ny \\n[%\\n\\n]\\n\\n \\n\\n \\n\\n \\n\\nSemantic\\nSyntactic\\nOverall\\n\\n(c) Asymmetric context\\n\\nFigure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are\\ntrained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100.\\n\\nWord similarity. While the analogy task is our\\nprimary focus since it tests for interesting vector\\nspace substructures, we also evaluate our model on\\na variety of word similarity tasks in Table 3. These\\ninclude WordSim-353 (Finkelstein et al., 2001),\\nMC (Miller and Charles, 1991), RG (Rubenstein\\nand Goodenough, 1965), SCWS (Huang et al.,\\n2012), and RW (Luong et al., 2013).\\nNamed entity recognition. The CoNLL-2003\\nEnglish benchmark dataset for NER is a collec-\\ntion of documents from Reuters newswire articles,\\nannotated with four entity types: person, location,\\norganization, and miscellaneous. We train mod-\\nels on CoNLL-03 training data on test on three\\ndatasets: 1) ConLL-03 testing data, 2) ACE Phase\\n2 (2001-02) and ACE-2003 data, and 3) MUC7\\nFormal Run test set. We adopt the BIO2 annota-\\ntion standard, as well as all the preprocessing steps\\ndescribed in (Wang and Manning, 2013). We use a\\ncomprehensive set of discrete features that comes\\nwith the standard distribution of the Stanford NER\\nmodel (Finkel et al., 2005). A total of 437,905\\ndiscrete features were generated for the CoNLL-\\n2003 training dataset. In addition, 50-dimensional\\nvectors for each word of a five-word context are\\nadded and used as continuous features. With these\\nfeatures as input, we trained a conditional random\\nfield (CRF) with exactly the same setup as the\\nCRFjoin model of (Wang and Manning, 2013).\\n\\n4.2 Corpora and training details\\n\\nWe trained our model on five corpora of varying\\nsizes: a 2010 Wikipedia dump with 1 billion to-\\nkens; a 2014 Wikipedia dump with 1.6 billion to-\\nkens; Gigaword 5 which has 4.3 billion tokens; the\\ncombination Gigaword5 + Wikipedia2014, which\\n\\nthe analogy task. This number is evaluated on a subset of the\\ndataset so it is not included in Table 2. 3COSMUL performed\\nworse than cosine similarity in almost all of our experiments.\\n\\nhas 6 billion tokens; and on 42 billion tokens of\\nweb data, from Common Crawl5. We tokenize\\nand lowercase each corpus with the Stanford to-\\nkenizer, build a vocabulary of the 400,000 most\\nfrequent words6, and then construct a matrix of co-\\noccurrence counts X . In constructing X , we must\\nchoose how large the context window should be\\nand whether to distinguish left context from right\\ncontext. We explore the effect of these choices be-\\nlow. In all cases we use a decreasing weighting\\nfunction, so that word pairs that are d words apart\\ncontribute 1/d to the total count. This is one way\\nto account for the fact that very distant word pairs\\nare expected to contain less relevant information\\nabout the words\\u2019 relationship to one another.\\n\\nFor all our experiments, we set xmax = 100,\\n\\u03b1 = 3/4, and train the model using AdaGrad\\n(Duchi et al., 2011), stochastically sampling non-\\nzero elements from X , with initial learning rate of\\n0.05. We run 50 iterations for vectors smaller than\\n300 dimensions, and 100 iterations otherwise (see\\nSection 4.6 for more details about the convergence\\nrate). Unless otherwise noted, we use a context of\\nten words to the left and ten words to the right.\\n\\nThe model generates two sets of word vectors,\\nW and W\\u0303 . When X is symmetric, W and W\\u0303 are\\nequivalent and differ only as a result of their ran-\\ndom initializations; the two sets of vectors should\\nperform equivalently. On the other hand, there is\\nevidence that for certain types of neural networks,\\ntraining multiple instances of the network and then\\ncombining the results can help reduce overfitting\\nand noise and generally improve results (Ciresan\\net al., 2012). With this in mind, we choose to use\\n\\n5To demonstrate the scalability of the model, we also\\ntrained it on a much larger sixth corpus, containing 840 bil-\\nlion tokens of web data, but in this case we did not lowercase\\nthe vocabulary, so the results are not directly comparable.\\n\\n6For the model trained on Common Crawl data, we use a\\nlarger vocabulary of about 2 million words.\\n\\n1538\\n\\n\\n\\nthe sum W +W\\u0303 as our word vectors. Doing so typ-\\nically gives a small boost in performance, with the\\nbiggest increase in the semantic analogy task.\\n\\nWe compare with the published results of a va-\\nriety of state-of-the-art models, as well as with\\nour own results produced using the word2vec\\ntool and with several baselines using SVDs. With\\nword2vec, we train the skip-gram (SG\\u2020) and\\ncontinuous bag-of-words (CBOW\\u2020) models on the\\n6 billion token corpus (Wikipedia 2014 + Giga-\\nword 5) with a vocabulary of the top 400,000 most\\nfrequent words and a context window size of 10.\\nWe used 10 negative samples, which we show in\\nSection 4.6 to be a good choice for this corpus.\\n\\nFor the SVD baselines, we generate a truncated\\nmatrix Xtrunc which retains the information of how\\nfrequently each word occurs with only the top\\n10,000 most frequent words. This step is typi-\\ncal of many matrix-factorization-based methods as\\nthe extra columns can contribute a disproportion-\\nate number of zero entries and the methods are\\notherwise computationally expensive.\\n\\nThe singular vectors of this matrix constitute\\nthe baseline \\u201cSVD\\u201d. We also evaluate two related\\nbaselines: \\u201cSVD-S\\u201d in which we take the SVD of\\u221a\\n\\nXtrunc, and \\u201cSVD-L\\u201d in which we take the SVD\\nof log(1+ Xtrunc). Both methods help compress the\\notherwise large range of values in X .7\\n\\n4.3 Results\\nWe present results on the word analogy task in Ta-\\nble 2. The GloVe model performs significantly\\nbetter than the other baselines, often with smaller\\nvector sizes and smaller corpora. Our results us-\\ning the word2vec tool are somewhat better than\\nmost of the previously published results. This is\\ndue to a number of factors, including our choice to\\nuse negative sampling (which typically works bet-\\nter than the hierarchical softmax), the number of\\nnegative samples, and the choice of the corpus.\\n\\nWe demonstrate that the model can easily be\\ntrained on a large 42 billion token corpus, with a\\nsubstantial corresponding performance boost. We\\nnote that increasing the corpus size does not guar-\\nantee improved results for other models, as can be\\nseen by the decreased performance of the SVD-\\n\\n7We also investigated several other weighting schemes for\\ntransforming X ; what we report here performed best. Many\\nweighting schemes like PPMI destroy the sparsity of X and\\ntherefore cannot feasibly be used with large vocabularies.\\nWith smaller vocabularies, these information-theoretic trans-\\nformations do indeed work well on word similarity measures,\\nbut they perform very poorly on the word analogy task.\\n\\nTable 3: Spearman rank correlation on word simi-\\nlarity tasks. All vectors are 300-dimensional. The\\nCBOW\\u2217 vectors are from the word2vec website\\nand differ in that they contain phrase vectors.\\n\\nModel Size WS353 MC RG SCWS RW\\nSVD 6B 35.3 35.1 42.5 38.3 25.6\\n\\nSVD-S 6B 56.5 71.5 71.0 53.6 34.7\\nSVD-L 6B 65.7 72.7 75.1 56.5 37.0\\nCBOW\\u2020 6B 57.2 65.6 68.2 57.0 32.5\\n\\nSG\\u2020 6B 62.8 65.2 69.7 58.1 37.2\\nGloVe 6B 65.8 72.7 77.8 53.9 38.1\\nSVD-L 42B 74.0 76.4 74.1 58.3 39.9\\nGloVe 42B 75.9 83.6 82.9 59.6 47.8\\n\\nCBOW\\u2217 100B 68.4 79.6 75.4 59.4 45.5\\n\\nL model on this larger corpus. The fact that this\\nbasic SVD model does not scale well to large cor-\\npora lends further evidence to the necessity of the\\ntype of weighting scheme proposed in our model.\\n\\nTable 3 shows results on five different word\\nsimilarity datasets. A similarity score is obtained\\nfrom the word vectors by first normalizing each\\nfeature across the vocabulary and then calculat-\\ning the cosine similarity. We compute Spearman\\u2019s\\nrank correlation coefficient between this score and\\nthe human judgments. CBOW\\u2217 denotes the vec-\\ntors available on the word2vec website that are\\ntrained with word and phrase vectors on 100B\\nwords of news data. GloVe outperforms it while\\nusing a corpus less than half the size.\\n\\nTable 4 shows results on the NER task with the\\nCRF-based model. The L-BFGS training termi-\\nnates when no improvement has been achieved on\\nthe dev set for 25 iterations. Otherwise all config-\\nurations are identical to those used by Wang and\\nManning (2013). The model labeled Discrete is\\nthe baseline using a comprehensive set of discrete\\nfeatures that comes with the standard distribution\\nof the Stanford NER model, but with no word vec-\\ntor features. In addition to the HPCA and SVD\\nmodels discussed previously, we also compare to\\nthe models of Huang et al. (2012) (HSMN) and\\nCollobert and Weston (2008) (CW). We trained\\nthe CBOW model using the word2vec tool8.\\nThe GloVe model outperforms all other methods\\non all evaluation metrics, except for the CoNLL\\ntest set, on which the HPCA method does slightly\\nbetter. We conclude that the GloVe vectors are\\nuseful in downstream NLP tasks, as was first\\n\\n8We use the same parameters as above, except in this case\\nwe found 5 negative samples to work slightly better than 10.\\n\\n1539\\n\\n\\n\\nTable 4: F1 score on NER task with 50d vectors.\\nDiscrete is the baseline without word vectors. We\\nuse publicly-available vectors for HPCA, HSMN,\\nand CW. See text for details.\\n\\nModel Dev Test ACE MUC7\\nDiscrete 91.0 85.4 77.4 73.4\\n\\nSVD 90.8 85.7 77.3 73.7\\nSVD-S 91.0 85.5 77.6 74.3\\nSVD-L 90.5 84.8 73.6 71.5\\nHPCA 92.6 88.7 81.7 80.7\\nHSMN 90.5 85.7 78.7 74.7\\n\\nCW 92.2 87.4 81.7 80.2\\nCBOW 93.1 88.2 82.2 81.1\\nGloVe 93.2 88.3 82.9 82.2\\n\\nshown for neural vectors in (Turian et al., 2010).\\n\\n4.4 Model Analysis: Vector Length and\\nContext Size\\n\\nIn Fig. 2, we show the results of experiments that\\nvary vector length and context window. A context\\nwindow that extends to the left and right of a tar-\\nget word will be called symmetric, and one which\\nextends only to the left will be called asymmet-\\nric. In (a), we observe diminishing returns for vec-\\ntors larger than about 200 dimensions. In (b) and\\n(c), we examine the effect of varying the window\\nsize for symmetric and asymmetric context win-\\ndows. Performance is better on the syntactic sub-\\ntask for small and asymmetric context windows,\\nwhich aligns with the intuition that syntactic infor-\\nmation is mostly drawn from the immediate con-\\ntext and can depend strongly on word order. Se-\\nmantic information, on the other hand, is more fre-\\nquently non-local, and more of it is captured with\\nlarger window sizes.\\n\\n4.5 Model Analysis: Corpus Size\\nIn Fig. 3, we show performance on the word anal-\\nogy task for 300-dimensional vectors trained on\\ndifferent corpora. On the syntactic subtask, there\\nis a monotonic increase in performance as the cor-\\npus size increases. This is to be expected since\\nlarger corpora typically produce better statistics.\\nInterestingly, the same trend is not true for the se-\\nmantic subtask, where the models trained on the\\nsmaller Wikipedia corpora do better than those\\ntrained on the larger Gigaword corpus. This is\\nlikely due to the large number of city- and country-\\nbased analogies in the analogy dataset and the fact\\nthat Wikipedia has fairly comprehensive articles\\nfor most such locations. Moreover, Wikipedia\\u2019s\\n\\n50\\n\\n55\\n\\n60\\n\\n65\\n\\n70\\n\\n75\\n\\n80\\n\\n85\\n\\nOverallSyntacticSemantic\\n\\nWiki2010\\n1B tokens\\n\\nA\\ncc\\n\\nu\\nra\\n\\ncy\\n [%\\n\\n]\\n\\nWiki2014\\n1.6B tokens\\n\\nGigaword5\\n4.3B tokens\\n\\nGigaword5 + \\nWiki2014\\n6B tokens\\n\\nCommon Crawl \\n42B tokens\\n\\nFigure 3: Accuracy on the analogy task for 300-\\ndimensional vectors trained on different corpora.\\n\\nentries are updated to assimilate new knowledge,\\nwhereas Gigaword is a fixed news repository with\\noutdated and possibly incorrect information.\\n\\n4.6 Model Analysis: Run-time\\n\\nThe total run-time is split between populating X\\nand training the model. The former depends on\\nmany factors, including window size, vocabulary\\nsize, and corpus size. Though we did not do so,\\nthis step could easily be parallelized across mul-\\ntiple machines (see, e.g., Lebret and Collobert\\n(2014) for some benchmarks). Using a single\\nthread of a dual 2.1GHz Intel Xeon E5-2658 ma-\\nchine, populating X with a 10 word symmetric\\ncontext window, a 400,000 word vocabulary, and\\na 6 billion token corpus takes about 85 minutes.\\nGiven X , the time it takes to train the model de-\\npends on the vector size and the number of itera-\\ntions. For 300-dimensional vectors with the above\\nsettings (and using all 32 cores of the above ma-\\nchine), a single iteration takes 14 minutes. See\\nFig. 4 for a plot of the learning curve.\\n\\n4.7 Model Analysis: Comparison with\\nword2vec\\n\\nA rigorous quantitative comparison of GloVe with\\nword2vec is complicated by the existence of\\nmany parameters that have a strong effect on per-\\nformance. We control for the main sources of vari-\\nation that we identified in Sections 4.4 and 4.5 by\\nsetting the vector length, context window size, cor-\\npus, and vocabulary size to the configuration men-\\ntioned in the previous subsection.\\n\\nThe most important remaining variable to con-\\ntrol for is training time. For GloVe, the rele-\\nvant parameter is the number of training iterations.\\nFor word2vec, the obvious choice would be the\\nnumber of training epochs. Unfortunately, the\\ncode is currently designed for only a single epoch:\\n\\n1540\\n\\n\\n\\n1 2 3 4 5 6\\n\\n60\\n\\n62\\n\\n64\\n\\n66\\n\\n68\\n\\n70\\n\\n72\\n\\n5 10 15 20 25\\n\\n1357 10 15 20 25 30 40 50\\n\\nA\\ncc\\n\\nu\\nra\\n\\ncy\\n [%\\n\\n]\\n\\nIterations (GloVe)\\n\\nNegative Samples (CBOW)\\n\\nTraining Time (hrs)\\n\\n \\n\\nGloVe\\nCBOW\\n\\n(a) GloVe vs CBOW\\n\\n3 6 9 12 15 18 21 24\\n\\n60\\n\\n62\\n\\n64\\n\\n66\\n\\n68\\n\\n70\\n\\n72\\n\\n20 40 60 80 100\\n\\n1 2 3 4 5 6 7 10 12 15 20\\n\\nGloVe\\nSkip-Gram\\n\\nA\\ncc\\n\\nu\\nra\\n\\ncy\\n [%\\n\\n]\\n\\nIterations (GloVe)\\n\\nNegative Samples (Skip-Gram)\\n\\nTraining Time (hrs)\\n\\n(b) GloVe vs Skip-Gram\\n\\nFigure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by\\nthe number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram\\n(b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 +\\nGigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.\\n\\nit specifies a learning schedule specific to a single\\npass through the data, making a modification for\\nmultiple passes a non-trivial task. Another choice\\nis to vary the number of negative samples. Adding\\nnegative samples effectively increases the number\\nof training words seen by the model, so in some\\nways it is analogous to extra epochs.\\n\\nWe set any unspecified parameters to their de-\\nfault values, assuming that they are close to opti-\\nmal, though we acknowledge that this simplifica-\\ntion should be relaxed in a more thorough analysis.\\n\\nIn Fig. 4, we plot the overall performance on\\nthe analogy task as a function of training time.\\nThe two x-axes at the bottom indicate the corre-\\nsponding number of training iterations for GloVe\\nand negative samples for word2vec. We note\\nthat word2vec\\u2019s performance actually decreases\\nif the number of negative samples increases be-\\nyond about 10. Presumably this is because the\\nnegative sampling method does not approximate\\nthe target probability distribution well.9\\n\\nFor the same corpus, vocabulary, window size,\\nand training time, GloVe consistently outperforms\\nword2vec. It achieves better results faster, and\\nalso obtains the best results irrespective of speed.\\n\\n5 Conclusion\\n\\nRecently, considerable attention has been focused\\non the question of whether distributional word\\nrepresentations are best learned from count-based\\n\\n9In contrast, noise-contrastive estimation is an approxi-\\nmation which improves with more negative samples. In Ta-\\nble 1 of (Mnih et al., 2013), accuracy on the analogy task is a\\nnon-decreasing function of the number of negative samples.\\n\\nmethods or from prediction-based methods. Cur-\\nrently, prediction-based models garner substantial\\nsupport; for example, Baroni et al. (2014) argue\\nthat these models perform better across a range of\\ntasks. In this work we argue that the two classes\\nof methods are not dramatically different at a fun-\\ndamental level since they both probe the under-\\nlying co-occurrence statistics of the corpus, but\\nthe efficiency with which the count-based meth-\\nods capture global statistics can be advantageous.\\nWe construct a model that utilizes this main ben-\\nefit of count data while simultaneously capturing\\nthe meaningful linear substructures prevalent in\\nrecent log-bilinear prediction-based methods like\\nword2vec. The result, GloVe, is a new global\\nlog-bilinear regression model for the unsupervised\\nlearning of word representations that outperforms\\nother models on word analogy, word similarity,\\nand named entity recognition tasks.\\n\\nAcknowledgments\\n\\nWe thank the anonymous reviewers for their valu-\\nable comments. Stanford University gratefully\\nacknowledges the support of the Defense Threat\\nReduction Agency (DTRA) under Air Force Re-\\nsearch Laboratory (AFRL) contract no. FA8650-\\n10-C-7020 and the Defense Advanced Research\\nProjects Agency (DARPA) Deep Exploration and\\nFiltering of Text (DEFT) Program under AFRL\\ncontract no. FA8750-13-2-0040. Any opinions,\\nfindings, and conclusion or recommendations ex-\\npressed in this material are those of the authors and\\ndo not necessarily reflect the view of the DTRA,\\nAFRL, DEFT, or the US government.\\n\\n1541\\n\\n\\n\\nReferences\\n\\nTom M. Apostol. 1976. Introduction to Analytic\\nNumber Theory. Introduction to Analytic Num-\\nber Theory.\\n\\nMarco Baroni, Georgiana Dinu, and Germa\\u0301n\\nKruszewski. 2014. Don\\u2019t count, predict! A\\nsystematic comparison of context-counting vs.\\ncontext-predicting semantic vectors. In ACL.\\n\\nYoshua Bengio. 2009. Learning deep architectures\\nfor AI. Foundations and Trends in Machine\\nLearning.\\n\\nYoshua Bengio, Re\\u0301jean Ducharme, Pascal Vin-\\ncent, and Christian Janvin. 2003. A neural prob-\\nabilistic language model. JMLR, 3:1137\\u20131155.\\n\\nJohn A. Bullinaria and Joseph P. Levy. 2007. Ex-\\ntracting semantic representations from word co-\\noccurrence statistics: A computational study.\\nBehavior Research Methods, 39(3):510\\u2013526.\\n\\nDan C. Ciresan, Alessandro Giusti, Luca M. Gam-\\nbardella, and Ju\\u0308rgen Schmidhuber. 2012. Deep\\nneural networks segment neuronal membranes\\nin electron microscopy images. In NIPS, pages\\n2852\\u20132860.\\n\\nRonan Collobert and Jason Weston. 2008. A uni-\\nfied architecture for natural language process-\\ning: deep neural networks with multitask learn-\\ning. In Proceedings of ICML, pages 160\\u2013167.\\n\\nRonan Collobert, Jason Weston, Le\\u0301on Bottou,\\nMichael Karlen, Koray Kavukcuoglu, and Pavel\\nKuksa. 2011. Natural Language Processing (Al-\\nmost) from Scratch. JMLR, 12:2493\\u20132537.\\n\\nScott Deerwester, Susan T. Dumais, George W.\\nFurnas, Thomas K. Landauer, and Richard\\nHarshman. 1990. Indexing by latent semantic\\nanalysis. Journal of the American Society for\\nInformation Science, 41.\\n\\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\\nAdaptive subgradient methods for online learn-\\ning and stochastic optimization. JMLR, 12.\\n\\nLev Finkelstein, Evgenly Gabrilovich, Yossi Ma-\\ntias, Ehud Rivlin, Zach Solan, Gadi Wolfman,\\nand Eytan Ruppin. 2001. Placing search in con-\\ntext: The concept revisited. In Proceedings\\nof the 10th international conference on World\\nWide Web, pages 406\\u2013414. ACM.\\n\\nEric H. Huang, Richard Socher, Christopher D.\\nManning, and Andrew Y. Ng. 2012. Improving\\n\\nWord Representations via Global Context and\\nMultiple Word Prototypes. In ACL.\\n\\nRe\\u0301mi Lebret and Ronan Collobert. 2014. Word\\nembeddings through Hellinger PCA. In EACL.\\n\\nOmer Levy, Yoav Goldberg, and Israel Ramat-\\nGan. 2014. Linguistic regularities in sparse and\\nexplicit word representations. CoNLL-2014.\\n\\nKevin Lund and Curt Burgess. 1996. Producing\\nhigh-dimensional semantic spaces from lexical\\nco-occurrence. Behavior Research Methods, In-\\nstrumentation, and Computers, 28:203\\u2013208.\\n\\nMinh-Thang Luong, Richard Socher, and Christo-\\npher D Manning. 2013. Better word represen-\\ntations with recursive neural networks for mor-\\nphology. CoNLL-2013.\\n\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\\nfrey Dean. 2013a. Efficient Estimation of Word\\nRepresentations in Vector Space. In ICLR Work-\\nshop Papers.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg\\nCorrado, and Jeffrey Dean. 2013b. Distributed\\nrepresentations of words and phrases and their\\ncompositionality. In NIPS, pages 3111\\u20133119.\\n\\nTomas Mikolov, Wen tau Yih, and Geoffrey\\nZweig. 2013c. Linguistic regularities in con-\\ntinuous space word representations. In HLT-\\nNAACL.\\n\\nGeorge A. Miller and Walter G. Charles. 1991.\\nContextual correlates of semantic similarity.\\nLanguage and cognitive processes, 6(1):1\\u201328.\\n\\nAndriy Mnih and Koray Kavukcuoglu. 2013.\\nLearning word embeddings efficiently with\\nnoise-contrastive estimation. In NIPS.\\n\\nDouglas L. T. Rohde, Laura M. Gonnerman,\\nand David C. Plaut. 2006. An improved\\nmodel of semantic similarity based on lexical\\nco-occurence. Communications of the ACM,\\n8:627\\u2013633.\\n\\nHerbert Rubenstein and John B. Goodenough.\\n1965. Contextual correlates of synonymy. Com-\\nmunications of the ACM, 8(10):627\\u2013633.\\n\\nFabrizio Sebastiani. 2002. Machine learning in au-\\ntomated text categorization. ACM Computing\\nSurveys, 34:1\\u201347.\\n\\nRichard Socher, John Bauer, Christopher D. Man-\\nning, and Andrew Y. Ng. 2013. Parsing With\\nCompositional Vector Grammars. In ACL.\\n\\n1542\\n\\n\\n\\nStefanie Tellex, Boris Katz, Jimmy Lin, Aaron\\nFernandes, and Gregory Marton. 2003. Quanti-\\ntative evaluation of passage retrieval algorithms\\nfor question answering. In Proceedings of the\\nSIGIR Conference on Research and Develop-\\nment in Informaion Retrieval.\\n\\nErik F. Tjong Kim Sang and Fien De Meul-\\nder. 2003. Introduction to the CoNLL-2003\\nshared task: Language-independent named en-\\ntity recognition. In CoNLL-2003.\\n\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\\n2010. Word representations: a simple and gen-\\neral method for semi-supervised learning. In\\nProceedings of ACL, pages 384\\u2013394.\\n\\nMengqiu Wang and Christopher D. Manning.\\n2013. Effect of non-linear deep architecture in\\nsequence labeling. In Proceedings of the 6th\\nInternational Joint Conference on Natural Lan-\\nguage Processing (IJCNLP).\\n\\n1543\\n\\n\\n\""