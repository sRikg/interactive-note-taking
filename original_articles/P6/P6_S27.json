Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms\u00a0[15]. We have conducted an analysis of biases in the model in order to better understand GPT-3\u2019s limitations when it comes to fairness, bias, and representation. \u2078\u20788Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work. See, for example, [39, 82, 109]. Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model\u2019s biases even within the studied categories. Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension.