"Setting CoQA DROP QuAC SQuADv2 RACE-h RACE-m - Fine-tuned SOTA 90.7^(a) 89.1^(b) 74.4^(c) 93.0^(d) 90.0^(e) 93.1^(e) GPT-3 Zero-Shot 81.5 23.6 41.5 59.5 45.5 58.4 GPT-3 One-Shot 84.0 34.3 43.3 65.4 45.9 57.4 GPT-3 Few-Shot 85.0 36.5 44.3 69.8 46.8 58.1 Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy. ^(a)[46] ^(b)[43] ^(c)[2] ^(d)[93] ^(e)[114] [Figure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models.] Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3\u2019s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset. GPT-3 performs best (within 3 points of the human baseline) on CoQA [95] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC [13] a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP [21], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems [99]. On SQuAD 2.0 [97], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE [70], a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still 45% behind SOTA. SuperGLUE BoolQ CB CB COPA RTE - Average Accuracy Accuracy F1 Accuracy Accuracy Fine-tuned SOTA 89.0 91.0 96.9 93.9 94.8 92.5 Fine-tuned BERT-Large 69.0 77.4 83.6 75.7 70.6 71.7 GPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0 WiC WSC MultiRC MultiRC ReCoRD ReCoRD - Accuracy Accuracy Accuracy F1a Accuracy F1 Fine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3 Fine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0 GPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1 Table 3.8: Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates. [Figure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of K\u2004=\u200432 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context.]"
