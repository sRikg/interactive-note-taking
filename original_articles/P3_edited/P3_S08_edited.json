"The Transformer uses multi-head attention in three different ways: -   \u2022     In \"encoder-decoder attention\" layers, the queries come from the     previous decoder layer, and the memory keys and values come from the     output of the encoder. This allows every position in the decoder to     attend over all positions in the input sequence. This mimics the     typical encoder-decoder attention mechanisms in sequence-to-sequence     models such as (wu2016google, ; bahdanau2014neural, ;     JonasFaceNet2017, ).     The encoder contains self-attention layers. In a self-attention     layer all of the keys, values and queries come from the same place,     in this case, the output of the previous layer in the encoder. Each     position in the encoder can attend to all positions in the previous     layer of the encoder.     Similarly, self-attention layers in the decoder allow each position     in the decoder to attend to all positions in the decoder up to and     including that position. We need to prevent leftward information     flow in the decoder to preserve the auto-regressive property. We     implement this inside of scaled dot-product attention by masking out     (setting to \u2005\u2212\u2005\u221e) all values in the input of the softmax which     correspond to illegal connections. See Figure\u00a02."
