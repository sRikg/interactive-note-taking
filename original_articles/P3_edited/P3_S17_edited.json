"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) ------------------------------------------------- -------------------------- ----------- Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) KVparse15 WSJ only, discriminative 88.3 Petrov et al. (2006) petrov-EtAl:2006:ACL WSJ only, discriminative 90.4 Zhu et al. (2013) zhu-EtAl:2013:ACL WSJ only, discriminative 90.4 Dyer et al. (2016) dyer-rnng:16 WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) zhu-EtAl:2013:ACL semi-supervised 91.3 Huang & Harper (2009) huang-harper:2009:EMNLP semi-supervised 91.3 McClosky et al. (2006) mcclosky-etAl:2006:NAACL semi-supervised 92.1 Vinyals & Kaiser el al. (2014) KVparse15 semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Dyer et al. (2016) dyer-rnng:16 generative 93.3 To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes KVparse15 . We trained a 4-layer transformer with d_(model)\u2004=\u20041024 on the Wall Street Journal (WSJ) portion of the Penn Treebank (marcus1993building, ), about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences (KVparse15, ). We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section\u00a05.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1\u2004=\u20040.3 for both WSJ only and the semi-supervised setting. Our results in Table\u00a04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar dyer-rnng:16 . In contrast to RNN sequence-to-sequence models (KVparse15, ), the Transformer outperforms the BerkeleyParser petrov-EtAl:2006:ACL even when training only on the WSJ training set of 40K sentences."
