{"[1708.02182] Regularizing and Optimizing LSTM Language Models": null, "Regularizing and Optimizing LSTM Language Models": null, "Stephen Merity \u2003\u2003 Nitish Shirish Keskar \u2003\u2003 Richard Socher": null, "Abstract": null, "Recurrent neural networks (RNNs), such as long short-term memory": null, "networks (LSTMs), serve as a fundamental building block for many": null, "sequence learning tasks, including machine translation, language": null, "modeling, and question answering. In this paper, we consider the": null, "specific problem of word-level language modeling and investigate": null, "strategies for regularizing and optimizing LSTM-based models. We propose": null, "the weight-dropped LSTM which uses DropConnect on hidden-to-hidden": null, "weights as a form of recurrent regularization. Further, we introduce": null, "NT-ASGD, a variant of the averaged stochastic gradient method, wherein": null, "the averaging trigger is determined using a non-monotonic condition as": null, "opposed to being tuned by the user. Using these and other regularization": null, "strategies, we achieve state-of-the-art word level perplexities on two": null, "data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring": null, "the effectiveness of a neural cache in conjunction with our proposed": null, "model, we achieve an even lower state-of-the-art perplexity of 52.8 on": null, "Penn Treebank and 52.0 on WikiText-2.": null, "machine learning, deep learning, LSTM, regularization, optimization": null, "1 Introduction": null, "Effective regularization techniques for deep learning have been the": null, "subject of much research in recent years. Given the": null, "over-parameterization of neural networks, generalization performance": null, "crucially relies on the ability to regularize the models sufficiently.": null, "Strategies such as dropout (Srivastava et\u00a0al., 2014) and batch": null, "normalization (Ioffe & Szegedy, 2015) have found great success and are": null, "now ubiquitous in feed-forward and convolutional neural networks.": null, "Na\u00efvely applying these approaches to the case of recurrent neural": null, "networks (RNNs) has not been highly successful however. Many recent": null, "works have hence been focused on the extension of these regularization": null, "strategies to RNNs; we briefly discuss some of them below.": null, "A na\u00efve application of dropout (Srivastava et\u00a0al., 2014) to an RNN\u2019s": null, "hidden state is ineffective as it disrupts the RNN\u2019s ability to retain": null, "long term dependencies (Zaremba et\u00a0al., 2014). Gal & Ghahramani (2016)": null, "propose overcoming this problem by retaining the same dropout mask": null, "across multiple time steps as opposed to sampling a new binary mask at": null, "each timestep. Another approach is to regularize the network through": null, "limiting updates to the RNN\u2019s hidden state. One such approach is taken": null, "by Semeniuta et\u00a0al. (2016) wherein the authors drop updates to network": null, "units, specifically the input gates of the LSTM, in lieu of the units": null, "themselves. This is reminiscent of zoneout (Krueger et\u00a0al., 2016) where": null, "updates to the hidden state may fail to occur for randomly selected": null, "neurons.": null, "Instead of operating on the RNN\u2019s hidden states, one can regularize the": null, "network through restrictions on the recurrent matrices as well. This can": null, "be done either through restricting the capacity of the matrix (Arjovsky": null, "et\u00a0al., 2016; Wisdom et\u00a0al., 2016; Jing et\u00a0al., 2016) or through": null, "element-wise interactions (Balduzzi & Ghifary, 2016; Bradbury et\u00a0al.,": null, "2016; Seo et\u00a0al., 2016).": null, "Other forms of regularization explicitly act upon activations such as": null, "batch normalization\u00a0(Ioffe & Szegedy, 2015), recurrent batch": null, "normalization\u00a0(Cooijmans et\u00a0al., 2016), and layer normalization\u00a0(Ba": null, "et\u00a0al., 2016). These all introduce additional training parameters and": null, "can complicate the training process while increasing the sensitivity of": null, "the model.": null, "In this work, we investigate a set of regularization strategies that are": null, "not only highly effective but which can also be used with no": null, "modification to existing LSTM implementations. The weight-dropped LSTM": null, "applies recurrent regularization through a DropConnect mask on the": null, "hidden-to-hidden recurrent weights. Other strategies include the use of": null, "randomized-length backpropagation through time (BPTT), embedding": null, "dropout, activation regularization (AR), and temporal activation": null, "regularization (TAR).": null, "As no modifications are required of the LSTM implementation these": null, "regularization strategies are compatible with black box libraries, such": null, "as NVIDIA cuDNN, which can be many times faster than na\u00efve LSTM": null, "implementations.": null, "Effective methods for training deep recurrent networks have also been a": null, "topic of renewed interest. Once a model has been defined, the training": null, "algorithm used is required to not only find a good minimizer of the loss": null, "function but also converge to such a minimizer rapidly. The choice of": null, "the optimizer is even more important in the context of regularized": null, "models since such strategies, especially the use of dropout, can impede": null, "the training process. Stochastic gradient descent (SGD), and its": null, "variants such as Adam (Kingma & Ba, 2014) and RMSprop (Tieleman &": null, "Hinton, 2012) are amongst the most popular training methods. These": null, "methods iteratively reduce the training loss through scaled (stochastic)": null, "gradient steps. In particular, Adam has been found to be widely": null, "applicable despite requiring less tuning of its hyperparameters. In the": null, "context of word-level language modeling, past work has empirically found": null, "that SGD outperforms other methods in not only the final loss but also": null, "in the rate of convergence. This is in agreement with recent evidence": null, "pointing to the insufficiency of adaptive gradient methods (Wilson": null, "et\u00a0al., 2017).": null, "Given the success of SGD, especially within the language modeling": null, "domain, we investigate the use of averaged SGD (ASGD) (Polyak &": null, "Juditsky, 1992) which is known to have superior theoretical guarantees.": null, "ASGD carries out iterations similar to SGD, but instead of returning the": null, "last iterate as the solution, returns an average of the iterates past a": null, "certain, tuned, threshold T. This threshold T is typically tuned and has": null, "a direct impact on the performance of the method. We propose a variant": null, "of ASGD where T is determined on the fly through a non-monotonic": null, "criterion and show that it achieves better training outcomes compared to": null, "SGD.": null, "2 Weight-dropped LSTM": null, "We refer to the mathematical formulation of the LSTM,": null, "  -- --------------------------- ------------------------------------------------------------------------------------- --": null, "     i_(t)                       \u2004=\u2004\u03c3(W^(i)x_(t)+U^(i)h_(t\u2005\u2212\u20051))                                                       ": null, "     f_(t)                       \u2004=\u2004\u03c3(W^(f)x_(t)+U^(f)h_(t\u2005\u2212\u20051))                                                       ": null, "     o_(t)                       \u2004=\u2004\u03c3(W^(o)x_(t)+U^(o)h_(t\u2005\u2212\u20051))                                                       ": null, "     ${\\overset{\\sim}{c}}_{t}$   \u2004=\u2004tanh(W^(c)x_(t)+U^(c)h_(t\u2005\u2212\u20051))                                                    ": null, "     c_(t)                       $= i_{t} \\odot {\\overset{\\sim}{c}}_{t} + f_{t} \\odot + {\\overset{\\sim}{c}}_{t - 1}$   ": null, "     h_(t)                       \u2004=\u2004o_(t)\u2005\u2299\u2005tanh(c_(t))                                                                ": null, "where [W^(i),\u2006W^(f),\u2006W^(o),\u2006U^(i),\u2006U^(f),\u2006U^(o)] are weight matrices,": null, "x_(t) is the vector input to the timestep t, h_(t) is the current": null, "exposed hidden state, c_(t) is the memory cell state, and \u2299 is": null, "element-wise multiplication.": null, "Preventing overfitting within the recurrent connections of an RNN has": null, "been an area of extensive research in language modeling. The majority of": null, "previous recurrent regularization techniques have acted on the hidden": null, "state vector h_(t\u2005\u2212\u20051), most frequently introducing a dropout operation": null, "between timesteps, or performing dropout on the update to the memory": null, "state c_(t). These modifications to a standard LSTM prevent the use of": null, "black box RNN implementations that may be many times faster due to": null, "low-level hardware-specific optimizations.": null, "We propose the use of DropConnect (Wan et\u00a0al., 2013) on the recurrent": null, "hidden to hidden weight matrices which does not require any": null, "modifications to an RNN\u2019s formulation. As the dropout operation is": null, "applied once to the weight matrices, before the forward and backward": null, "pass, the impact on training speed is minimal and any standard RNN": null, "implementation can be used, including inflexible but highly optimized": null, "black box LSTM implementations such as NVIDIA\u2019s cuDNN LSTM.": null, "By performing DropConnect on the hidden-to-hidden weight matrices": null, "[U^(i),\u2006U^(f),\u2006U^(o),\u2006U^(c)] within the LSTM, we can prevent overfitting": null, "from occurring on the recurrent connections of the LSTM. This": null, "regularization technique would also be applicable to preventing": null, "overfitting on the recurrent weight matrices of other RNN cells.": null, "As the same weights are reused over multiple timesteps, the same": null, "individual dropped weights remain dropped for the entirety of the": null, "forward and backward pass. The result is similar to variational dropout,": null, "which applies the same dropout mask to recurrent connections within the": null, "LSTM by performing dropout on h_(t\u2005\u2212\u20051), except that the dropout is": null, "applied to the recurrent weights. DropConnect could also be used on the": null, "non-recurrent weights of the LSTM [W^(i),\u2006W^(f),\u2006W^(o)] though our focus": null, "was on preventing overfitting on the recurrent connection.": null, "3 Optimization": null, "SGD is among the most popular methods for training deep learning models": null, "across various modalities including computer vision, natural language": null, "processing, and reinforcement learning. The training of deep networks": null, "can be posed as a non-convex optimization problem": null, "  -- ----------------------------------------------------------------------------- --": null, "     ${\\min\\limits_{w}\\quad{\\frac{1}{N}{\\sum\\limits_{i = 1}^{N}{f_{i}{(w)}}}}},$   ": null, "where f_(i) is the loss function for the i^(th) data point, w are the": null, "weights of the network, and the expectation is taken over the data.": null, "Given a sequence of learning rates, \u03b3_(k), SGD iteratively takes steps": null, "of the form": null, "  -- --------------------------------------------------------------- -- -----": null, "     ${w_{k + 1} = {w_{k} - {\\gamma_{k}\\hat{\\nabla}f{(w_{k})}}}},$      (1)": null, "where the subscript denotes the iteration number and the $\\hat{\\nabla}$": null, "denotes a stochastic gradient that may be computed on a minibatch of": null, "data points. SGD demonstrably performs well in practice and also": null, "possesses several attractive theoretical properties such as linear": null, "convergence (Bottou et\u00a0al., 2016), saddle point avoidance (Panageas &": null, "Piliouras, 2016) and better generalization performance (Hardt et\u00a0al.,": null, "2015). For the specific task of neural language modeling, traditionally": null, "SGD without momentum has been found to outperform other algorithms such": null, "as momentum SGD (Sutskever et\u00a0al., 2013), Adam (Kingma & Ba, 2014),": null, "Adagrad (Duchi et\u00a0al., 2011) and RMSProp (Tieleman & Hinton, 2012) by a": null, "statistically significant margin.": null, "Motivated by this observation, we investigate averaged SGD (ASGD) to": null, "further improve the training process. ASGD has been analyzed in depth": null, "theoretically and many surprising results have been shown including its": null, "asymptotic second-order convergence (Polyak & Juditsky, 1992; Mandt": null, "et\u00a0al., 2017). ASGD takes steps identical to equation (1) but instead of": null, "returning the last iterate as the solution, returns": null, "$\\frac{1}{({{K - T} + 1})}{\\sum_{i = T}^{K}w_{i}}$, where K is the total": null, "number of iterations and T\u2004<\u2004K is a user-specified averaging trigger.": null, "Algorithm 1 Non-monotonically Triggered ASGD (NT-ASGD)": null, "Inputs: Initial point w\u2080, learning rate \u03b3, logging interval L,": null, "non-monotone interval n.": null, "1:\u00a0\u00a0Initialize k\u21900, t\u21900, T\u21900, logs \u2190 []": null, "2:\u00a0\u00a0while\u00a0stopping criterion not met\u00a0do": null, "3:\u00a0\u00a0\u00a0\u00a0\u00a0Compute stochastic gradient $\\hat{\\nabla}f{(w_{k})}$ and take SGD": null, "step (1).": null, "4:\u00a0\u00a0\u00a0\u00a0\u00a0if\u00a0 mod(k,L)\u2004=\u20040 and T\u2004=\u20040\u00a0then": null, "5:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Compute validation perplexity v.": null, "6:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if\u00a0t\u2004>\u2004n and $v > \\min\\limits_{l \\in {\\{{t - n},\\cdots,t\\}}}$": null, "logs[l]\u00a0then": null, "7:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set T\u2190k": null, "8:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0end\u00a0if": null, "9:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Append v to logs": null, "10:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0t\u2190t\u2005+\u20051": null, "11:\u00a0\u00a0\u00a0\u00a0\u00a0end\u00a0if": null, "12:\u00a0\u00a0end\u00a0while": null, "return $\\frac{\\sum_{i = T}^{k}w_{i}}{({{k - T} + 1})}$": null, "Despite its theoretical appeal, ASGD has found limited practical use in": null, "training of deep networks. This may be in part due to unclear tuning": null, "guidelines for the learning-rate schedule \u03b3_(k) and averaging trigger T.": null, "If the averaging is triggered too soon, the efficacy of the method is": null, "impacted, and if it is triggered too late, many additional iterations": null, "may be needed to converge to the solution. In this section, we describe": null, "a non-monotonically triggered variant of ASGD (NT-ASGD), which obviates": null, "the need for tuning T. Further, the algorithm uses a constant learning": null, "rate throughout the experiment and hence no further tuning is necessary": null, "for the decay scheduling.": null, "Ideally, averaging needs to be triggered when the SGD iterates converge": null, "to a steady-state distribution (Mandt et\u00a0al., 2017). This is roughly": null, "equivalent to the convergence of SGD to a neighborhood around a": null, "solution. In the case of SGD, certain learning-rate reduction strategies": null, "such as the step-wise strategy analogously reduce the learning rate by a": null, "fixed quantity at such a point. A common strategy employed in language": null, "modeling is to reduce the learning rates by a fixed proportion when the": null, "performance of the model\u2019s primary metric (such as perplexity) worsens": null, "or stagnates. Along the same lines, one could make a triggering decision": null, "based on the performance of the model on the validation set. However,": null, "instead of averaging immediately after the validation metric worsens, we": null, "propose a non-monotonic criterion that conservatively triggers the": null, "averaging when the validation metric fails to improve for multiple": null, "cycles; see Algorithm 1. Given that the choice of triggering is": null, "irreversible, this conservatism ensures that the randomness of training": null, "does not play a major role in the decision. Analogous strategies have": null, "also been proposed for learning-rate reduction in SGD (Keskar & Saon,": null, "2015).": null, "While the algorithm introduces two additional hyperparameters, the": null, "logging interval L and non-monotone interval n, we found that setting L": null, "to be the number of iterations in an epoch and n\u2004=\u20045 worked well across": null, "various models and data sets. As such, we use this setting in all of our": null, "NT-ASGD experiments in the following section and demonstrate that it": null, "achieves better training outcomes as compared to SGD.": null, "4 Extended regularization techniques": null, "In addition to the regularization and optimization techniques above, we": null, "explored additional regularization techniques that aimed to improve data": null, "efficiency during training and to prevent overfitting of the RNN model.": null, "4.1 Variable length backpropagation sequences": null, "Given a fixed sequence length that is used to break a data set into": null, "fixed length batches, the data set is not efficiently used. To": null, "illustrate this, imagine being given 100 elements to perform": null, "backpropagation through with a fixed backpropagation through time (BPTT)": null, "window of 10. Any element divisible by 10 will never have any elements": null, "to backprop into, no matter how many times you may traverse the data": null, "set. Indeed, the backpropagation window that each element receives is": null, "equal to imod\u200610 where i is the element\u2019s index. This is data": null, "inefficient, preventing $\\frac{1}{10}$ of the data set from ever being": null, "able to improve itself in a recurrent fashion, and resulting in": null, "$\\frac{8}{10}$ of the remaining elements receiving only a partial": null, "backpropagation window compared to the full possible backpropagation": null, "window of length 10.": null, "To prevent such inefficient data usage, we randomly select the sequence": null, "length for the forward and backward pass in two steps. First, we select": null, "the base sequence length to be seq with probability p and": null, "$\\frac{\\text{seq}}{2}$ with probability 1\u2005\u2212\u2005p, where p is a high value": null, "approaching 1. This spreads the starting point for the BPTT window": null, "beyond the base sequence length. We then select the sequence length": null, "according to \ud835\udca9(seq,s), where seq is the base sequence length and s is": null, "the standard deviation. This jitters the starting point such that it": null, "doesn\u2019t always fall on a specific word divisible by seq or": null, "$\\frac{\\text{seq}}{2}$. From these, the sequence length more efficiently": null, "uses the data set, ensuring that when given enough epochs all the": null, "elements in the data set experience a full BPTT window, while ensuring": null, "the average sequence length remains around the base sequence length for": null, "computational efficiency.": null, "During training, we rescale the learning rate depending on the length of": null, "the resulting sequence compared to the original specified sequence": null, "length. The rescaling step is necessary as sampling arbitrary sequence": null, "lengths with a fixed learning rate favors short sequences over longer": null, "ones. This linear scaling rule has been noted as important for training": null, "large scale minibatch SGD without loss of accuracy (Goyal et\u00a0al., 2017)": null, "and is a component of unbiased truncated backpropagation through time": null, "(Tallec & Ollivier, 2017).": null, "4.2 Variational dropout": null, "In standard dropout, a new binary dropout mask is sampled each and every": null, "time the dropout function is called. New dropout masks are sampled even": null, "if the given connection is repeated, such as the input x\u2080 to an LSTM at": null, "timestep t\u2004=\u20040 receiving a different dropout mask than the input x\u2081 fed": null, "to the same LSTM at t\u2004=\u20041. A variant of this, variational dropout (Gal &": null, "Ghahramani, 2016), samples a binary dropout mask only once upon the": null, "first call and then to repeatedly use that locked dropout mask for all": null, "repeated connections within the forward and backward pass.": null, "While we propose using DropConnect rather than variational dropout to": null, "regularize the hidden-to-hidden transition within an RNN, we use": null, "variational dropout for all other dropout operations, specifically using": null, "the same dropout mask for all inputs and outputs of the LSTM within a": null, "given forward and backward pass. Each example within the minibatch uses": null, "a unique dropout mask, rather than a single dropout mask being used over": null, "all examples, ensuring diversity in the elements dropped out.": null, "4.3 Embedding dropout": null, "Following Gal & Ghahramani (2016), we employ embedding dropout. This is": null, "equivalent to performing dropout on the embedding matrix at a word": null, "level, where the dropout is broadcast across all the word vector\u2019s": null, "embedding. The remaining non-dropped-out word embeddings are scaled by": null, "$\\frac{1}{1 - p_{e}}$ where p_(e) is the probability of embedding": null, "dropout. As the dropout occurs on the embedding matrix that is used for": null, "a full forward and backward pass, this means that all occurrences of a": null, "specific word will disappear within that pass, equivalent to performing": null, "variational dropout on the connection between the one-hot embedding and": null, "the embedding lookup.": null, "4.4 Weight tying": null, "Weight tying (Inan et\u00a0al., 2016; Press & Wolf, 2016) shares the weights": null, "between the embedding and softmax layer, substantially reducing the": null, "total parameter count in the model. The technique has theoretical": null, "motivation (Inan et\u00a0al., 2016) and prevents the model from having to": null, "learn a one-to-one correspondence between the input and output,": null, "resulting in substantial improvements to the standard LSTM language": null, "model.": null, "4.5 Independent embedding size and hidden size": null, "In most natural language processing tasks, both pre-trained and trained": null, "word vectors are of relatively low dimensionality\u2014frequently between 100": null, "and 400 dimensions in size. Most previous LSTM language models tie the": null, "dimensionality of the word vectors to the dimensionality of the LSTM\u2019s": null, "hidden state. Even if reducing the word embedding size was not": null, "beneficial in preventing overfitting, the easiest reduction in total": null, "parameters for a language model is reducing the word vector size. To": null, "achieve this, the first and last LSTM layers are modified such that": null, "their input and output dimensionality respectively are equal to the": null, "reduced embedding size.": null, "4.6 Activation Regularization (AR) and Temporal Activation Regularization (TAR)": null, "L\u2082-regularization is often used on the weights of the network to control": null, "the norm of the resulting model and reduce overfitting. In addition, L\u2082": null, "decay can be used on the individual unit activations and on the": null, "difference in outputs of an RNN at different time steps; these": null, "strategies labeled as activation regularization (AR) and temporal": null, "activation regularization (TAR) respectively (Merity et\u00a0al., 2017). AR": null, "penalizes activations that are significantly larger than 0 as a means of": null, "regularizing the network. Concretely, AR is defined as": null, "  -- -------------- --": null, "     \u03b1L\u2082(m\u2299h_(t))   ": null, "where m is the dropout mask, L\u2082(\u22c5)\u2004=\u2004\u2004\u2225\u2004\u2005\u22c5\u2005\u2225\u2082, h_(t) is the output of": null, "the RNN at timestep t, and \u03b1 is a scaling coefficient. TAR falls under": null, "the broad category of slowness regularizers (Hinton, 1989; F\u00f6ldi\u00e1k,": null, "1991; Luciw & Schmidhuber, 2012; Jonschkowski & Brock, 2015) which": null, "penalize the model from producing large changes in the hidden state.": null, "Using the notation from AR, TAR is defined as": null, "  -- ---------------------- --": null, "     \u03b2L\u2082(h_(t)\u2212h_(t\u2005+\u20051))   ": null, "where \u03b2 is a scaling coefficient. As in Merity et\u00a0al. (2017), the AR and": null, "TAR loss are only applied to the output of the final RNN layer as": null, "opposed to being applied to all layers.": null, "5 Experiment Details": null, "For evaluating the impact of these approaches, we perform language": null, "modeling over a preprocessed version of the Penn Treebank (PTB) (Mikolov": null, "et\u00a0al., 2010) and the WikiText-2 (WT2) data set (Merity et\u00a0al., 2016).": null, "PTB: The Penn Treebank data set has long been a central data set for": null, "experimenting with language modeling. The data set is heavily": null, "preprocessed and does not contain capital letters, numbers, or": null, "punctuation. The vocabulary is also capped at 10,000 unique words, quite": null, "small in comparison to most modern datasets, which results in a large": null, "number of out of vocabulary (OoV) tokens.": null, "WT2: WikiText-2 is sourced from curated Wikipedia articles and is": null, "approximately twice the size of the PTB data set. The text is tokenized": null, "and processed using the Moses tokenizer (Koehn et\u00a0al., 2007), frequently": null, "used for machine translation, and features a vocabulary of over 30,000": null, "words. Capitalization, punctuation, and numbers are retained in this": null, "data set.": null, "All experiments use a three-layer LSTM model with 1150 units in the": null, "hidden layer and an embedding of size 400. The loss was averaged over": null, "all examples and timesteps. All embedding weights were uniformly": null, "initialized in the interval [\u2005\u2212\u20050.1,\u20060.1] and all other weights were": null, "initialized between": null, "$\\lbrack{- \\frac{1}{\\sqrt{H}}},\\frac{1}{\\sqrt{H}}\\rbrack$, where H is": null, "the hidden size.": null, "For training the models, we use the NT-ASGD algorithm discussed in the": null, "previous section for 750 epochs with L equivalent to one epoch and": null, "n\u2004=\u20045. We use a batch size of 80 for WT2 and 40 for PTB. Empirically, we": null, "found relatively large batch sizes (e.g., 40-80) performed better than": null, "smaller sizes (e.g., 10-20) for NT-ASGD. After completion, we run ASGD": null, "with T\u2004=\u20040 and hot-started w\u2080 as a fine-tuning step to further improve": null, "the solution. For this fine-tuning step, we terminate the run using the": null, "same non-monotonic criterion detailed in Algorithm 1.": null, "We carry out gradient clipping with maximum norm 0.25 and use an initial": null, "learning rate of 30 for all experiments. We use a random BPTT length": null, "which is \ud835\udca9(70,5) with probability 0.95 and \ud835\udca9(35,5) with probability": null, "0.05. The values used for dropout on the word vectors, the output": null, "between LSTM layers, the output of the final LSTM layer, and embedding": null, "dropout where (0.4,0.3,0.4,0.1) respectively. For the weight-dropped": null, "LSTM, a dropout of 0.5 was applied to the recurrent weight matrices. For": null, "WT2, we increase the input dropout to 0.65 to account for the increased": null, "vocabulary size. For all experiments, we use AR and TAR values of 2 and": null, "1 respectively, and tie the embedding and softmax weights. These": null, "hyperparameters were chosen through trial and error and we expect": null, "further improvements may be possible if a fine-grained hyperparameter": null, "search were to be conducted. In the results, we abbreviate our approach": null, "as AWD-LSTM for ASGD Weight-Dropped LSTM.": null, "  Model                                                           Parameters   Validation   Test": null, "  --------------------------------------------------------------- ------------ ------------ ------------": null, "  Mikolov & Zweig (2012) - KN-5                                   2M^(\u2021)       \u2212            141.2": null, "  Mikolov & Zweig (2012) - KN5 + cache                            2M^(\u2021)       \u2212            125.7": null, "  Mikolov & Zweig (2012) - RNN                                    6M^(\u2021)       \u2212            124.7": null, "  Mikolov & Zweig (2012) - RNN-LDA                                7M^(\u2021)       \u2212            113.7": null, "  Mikolov & Zweig (2012) - RNN-LDA + KN-5 + cache                 9M^(\u2021)       \u2212            92.0": null, "  Zaremba et\u00a0al. (2014) - LSTM (medium)                           20M          86.2         82.7": null, "  Zaremba et\u00a0al. (2014) - LSTM (large)                            66M          82.2         78.4": null, "  Gal & Ghahramani (2016) - Variational LSTM (medium)             20M          81.9\u2005\u00b1\u20050.2   79.7\u2005\u00b1\u20050.1": null, "  Gal & Ghahramani (2016) - Variational LSTM (medium, MC)         20M          \u2212            78.6\u2005\u00b1\u20050.1": null, "  Gal & Ghahramani (2016) - Variational LSTM (large)              66M          77.9\u2005\u00b1\u20050.3   75.2\u2005\u00b1\u20050.2": null, "  Gal & Ghahramani (2016) - Variational LSTM (large, MC)          66M          \u2212            73.4\u2005\u00b1\u20050.0": null, "  Kim et\u00a0al. (2016) - CharCNN                                     19M          \u2212            78.9": null, "  Merity et\u00a0al. (2016) - Pointer Sentinel-LSTM                    21M          72.4         70.9": null, "  Grave et\u00a0al. (2016) - LSTM                                      \u2212            \u2212            82.3": null, "  Grave et\u00a0al. (2016) - LSTM + continuous cache pointer           \u2212            \u2212            72.1": null, "  Inan et\u00a0al. (2016) - Variational LSTM (tied) + augmented loss   24M          75.7         73.2": null, "  Inan et\u00a0al. (2016) - Variational LSTM (tied) + augmented loss   51M          71.1         68.5": null, "  Zilly et\u00a0al. (2016) - Variational RHN (tied)                    23M          67.9         65.4": null, "  Zoph & Le (2016) - NAS Cell (tied)                              25M          \u2212            64.0": null, "  Zoph & Le (2016) - NAS Cell (tied)                              54M          \u2212            62.4": null, "  Melis et\u00a0al. (2017) - 4-layer skip connection LSTM (tied)       24M          60.9         58.3": null, "  AWD-LSTM - 3-layer LSTM (tied)                                  24M          60.0         57.3": null, "  AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer       24M          53.9         52.8": null, "Table 1: Single model perplexity on validation and test sets for the": null, "Penn Treebank language modeling task. Parameter numbers with \u2021 are": null, "estimates based upon our understanding of the model and with reference": null, "to Merity et\u00a0al. (2016). Models noting tied use weight tying on the": null, "embedding and softmax weights. Our model, AWD-LSTM, stands for ASGD": null, "Weight-Dropped LSTM.": null, "  Model                                                                     Parameters   Validation   Test": null, "  ------------------------------------------------------------------------- ------------ ------------ ------": null, "  Inan et\u00a0al. (2016) - Variational LSTM (tied) (h\u2004=\u2004650)                    28M          92.3         87.7": null, "  Inan et\u00a0al. (2016) - Variational LSTM (tied) (h\u2004=\u2004650) + augmented loss   28M          91.5         87.0": null, "  Grave et\u00a0al. (2016) - LSTM                                                \u2212            \u2212            99.3": null, "  Grave et\u00a0al. (2016) - LSTM + continuous cache pointer                     \u2212            \u2212            68.9": null, "  Melis et\u00a0al. (2017) - 1-layer LSTM (tied)                                 24M          69.3         65.9": null, "  Melis et\u00a0al. (2017) - 2-layer skip connection LSTM (tied)                 24M          69.1         65.9": null, "  AWD-LSTM - 3-layer LSTM (tied)                                            33M          68.6         65.8": null, "  AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer                 33M          53.8         52.0": null, "Table 2: Single model perplexity over WikiText-2. Models noting tied use": null, "weight tying on the embedding and softmax weights. Our model, AWD-LSTM,": null, "stands for ASGD Weight-Dropped LSTM.": null, "6 Experimental Analysis": null, "We present the single-model perplexity results for both our models": null, "(AWD-LSTM) and other competitive models in Table 1 and 2 for PTB and WT2": null, "respectively. On both data sets we improve the state-of-the-art, with": null, "our vanilla LSTM model beating the state of the art by approximately 1": null, "unit on PTB and 0.1 units on WT2.": null, "In comparison to other recent state-of-the-art models, our model uses a": null, "vanilla LSTM. Zilly et\u00a0al. (2016) propose the recurrent highway network,": null, "which extends the LSTM to allow multiple hidden state updates per": null, "timestep. Zoph & Le (2016) use a reinforcement learning agent to": null, "generate an RNN cell tailored to the specific task of language modeling,": null, "with the cell far more complex than the LSTM.": null, "Independently of our work, Melis et\u00a0al. (2017) apply extensive": null, "hyperparameter search to an LSTM based language modeling implementation,": null, "analyzing the sensitivity of RNN based language models to": null, "hyperparameters. Unlike our work, they use a modified LSTM, which caps": null, "the input gate i_(t) to be min\u2006(1\u2005\u2212\u2005f_(t),i_(t)), use Adam with \u03b2\u2081\u2004=\u20040": null, "rather than SGD or ASGD, use skip connections between LSTM layers, and": null, "use a black box hyperparameter tuner for exploring models and settings.": null, "Of particular interest is that their hyperparameters were tuned": null, "individually for each data set compared to our work which shared almost": null, "all hyperparameters between PTB and WT2, including the embedding and": null, "hidden size for both data sets. Due to this, they used less model": null, "parameters than our model and found shallow LSTMs of one or two layers": null, "worked best for WT2.": null, "Like our work, Melis et\u00a0al. (2017) find that the underlying LSTM": null, "architecture can be highly effective compared to complex custom": null, "architectures when well tuned hyperparameters are used. The approaches": null, "used in our work and Melis et\u00a0al. (2017) may be complementary and would": null, "be worth exploration.": null, "7 Pointer models": null, "In past work, pointer based attention models have been shown to be": null, "highly effective in improving language modeling (Merity et\u00a0al., 2016;": null, "Grave et\u00a0al., 2016). Given such substantial improvements to the": null, "underlying neural language model, it remained an open question as to how": null, "effective pointer augmentation may be, especially when improvements such": null, "as weight tying may act in mutually exclusive ways.": null, "The neural cache model (Grave et\u00a0al., 2016) can be added on top of a": null, "pre-trained language model at negligible cost. The neural cache stores": null, "the previous hidden states in memory cells and then uses a simple convex": null, "combination of the probability distributions suggested by the cache and": null, "the language model for prediction. The cache model has three": null, "hyperparameters: the memory size (window) for the cache, the coefficient": null, "of the combination (which determines how the two distributions are": null, "mixed), and the flatness of the cache distribution. All of these are": null, "tuned on the validation set once a trained language model has been": null, "obtained and require no training by themselves, making it quite": null, "inexpensive to use. The tuned values for these hyperparameters were": null, "(2000,0.1,1.0) for PTB and (3785,0.1279,0.662) for WT2 respectively.": null, "In Tables 1 and 2, we show that the model further improves the": null, "perplexity of the language model by as much as 6 perplexity points for": null, "PTB and 11 points for WT2. While this is smaller than the gains reported": null, "in Grave et\u00a0al. (2016), which used an LSTM without weight tying, this is": null, "still a substantial drop. Given the simplicity of the neural cache": null, "model, and the lack of any trained components, these results suggest": null, "that existing neural language models remain fundamentally lacking,": null, "failing to capture long term dependencies or remember recently seen": null, "words effectively.": null, "To understand the impact the pointer had on the model, specifically the": null, "validation set perplexity, we detail the contribution that each word has": null, "on the cache model\u2019s overall perplexity in Table 3. We compute the sum": null, "of the total difference in the loss function value (i.e., log": null, "perplexity) between the LSTM-only and LSTM-with-cache models for the": null, "target words in the validation portion of the WikiText-2 data set. We": null, "present results for the sum of the difference as opposed to the mean": null, "since the latter undesirably overemphasizes infrequently occurring words": null, "for which the cache helps significantly and ignores frequently occurring": null, "words for which the cache provides modest improvements that cumulatively": null, "make a strong contribution.": null, "The largest cumulative gain is in improving the handling of <unk>": null, "tokens, though this is over 11540 instances. The second best": null, "improvement, approximately one fifth the gain given by the <unk> tokens,": null, "is for Meridian, yet this word only occurs 161 times. This indicates the": null, "cache still helps significantly even for relatively rare words, further": null, "demonstrated by Churchill, Blythe, or Sonic. The cache is not beneficial": null, "when handling frequent word categories, such as punctuation or stop": null, "words, for which the language model is likely well suited. These": null, "observations motivate the design of a cache framework that is more aware": null, "of the relative strengths of the two models.": null, "  Word    Count   \u0394loss     Word          Count   \u0394loss": null, "  ------- ------- --------- ------------- ------- ---------": null, "  .       7632    -696.45   <unk>         11540   5047.34": null, "  ,       9857    -687.49   Meridian      161     1057.78": null, "  of      5816    -365.21   Churchill     137     849.43": null, "  =       2884    -342.01   -             67      682.15": null, "  to      4048    -283.10   Blythe        97      554.95": null, "  in      4178    -222.94   Sonic         75      543.85": null, "  <eos>   3690    -216.42   Richmond      101     429.18": null, "  and     5251    -215.38   Starr         74      416.52": null, "  the     12481   -209.97   Australian    234     366.36": null, "  a       3381    -149.78   Pagan         54      365.19": null, "  \"       2540    -127.99   Asahi         39      316.24": null, "  that    1365    -118.09   Japanese      181     295.97": null, "  by      1252    -113.05   Hu            43      285.58": null, "  was     2279    -107.95   Hedgehog      29      266.48": null, "  )       1101    -94.74    Burma         35      263.65": null, "  with    1176    -93.01    29            92      260.88": null, "  for     1215    -87.68    Mississippi   72      241.59": null, "  on      1485    -81.55    German        108     241.23": null, "  as      1338    -77.05    mill          67      237.76": null, "  at      879     -59.86    Cooke         33      231.11": null, "Table 3: The sum total difference in loss (log perplexity) that a given": null, "word results in over all instances in the validation data set of": null, "WikiText-2 when the continuous cache pointer is introduced. The right": null, "column contains the words with the twenty best improvements (i.e., where": null, "the cache was advantageous), and the left column the twenty most": null, "deteriorated (i.e., where the cache was disadvantageous).": null, "8 Model Ablation Analysis": null, "  ----------------------------- ------------ ------ ------------ ------": null, "                                PTB                 WT2          ": null, "  Model                         Validation   Test   Validation   Test": null, "  AWD-LSTM (tied)               60.0         57.3   68.6         65.8": null, "  \u2013 fine-tuning                 60.7         58.8   69.1         66.0": null, "  \u2013 NT-ASGD                     66.3         63.7   73.3         69.7": null, "  \u2013 variable sequence lengths   61.3         58.9   69.3         66.2": null, "  \u2013 embedding dropout           65.1         62.7   71.1         68.1": null, "  \u2013 weight decay                63.7         61.0   71.9         68.7": null, "  \u2013 AR/TAR                      62.7         60.3   73.2         70.1": null, "  \u2013 full sized embedding        68.0         65.6   73.7         70.7": null, "  \u2013 weight-dropping             71.1         68.9   78.4         74.9": null, "Table 4: Model ablations for our best LSTM models reporting results over": null, "the validation and test set on Penn Treebank and WikiText-2. Ablations": null, "are split into optimization and regularization variants, sorted": null, "according to the achieved validation perplexity on WikiText-2.": null, "In Table 4, we present the values of validation and testing perplexity": null, "for different variants of our best-performing LSTM model. Each variant": null, "removes a form of optimization or regularization.": null, "The first two variants deal with the optimization of the language models": null, "while the rest deal with the regularization. For the model using SGD": null, "with learning rate reduced by 2 using the same nonmonotonic fashion,": null, "there is a significant degradation in performance. This stands as": null, "empirical evidence regarding the benefit of averaging of the iterates.": null, "Using a monotonic criterion instead also hampered performance.": null, "Similarly, the removal of the fine-tuning step expectedly also degrades": null, "the performance. This step helps improve the estimate of the minimizer": null, "by resetting the memory of the previous experiment. While this process": null, "of fine-tuning can be repeated multiple times, we found little benefit": null, "in repeating it more than once.": null, "The removal of regularization strategies paints a similar picture; the": null, "inclusion of all of the proposed strategies was pivotal in ensuring": null, "state-of-the-art performance. The most extreme perplexity jump was in": null, "removing the hidden-to-hidden LSTM regularization provided by the": null, "weight-dropped LSTM. Without such hidden-to-hidden regularization,": null, "perplexity rises substantially, up to 11 points. This is in line with": null, "previous work showing the necessity of recurrent regularization in": null, "state-of-the-art models (Gal & Ghahramani, 2016; Inan et\u00a0al., 2016).": null, "We also experiment with static sequence lengths which we had": null, "hypothesized would lead to inefficient data usage. This also worsens the": null, "performance by approximately one perplexity unit. Next, we experiment": null, "with reverting to matching the sizes of the embedding vectors and the": null, "hidden states. This significantly increases the number of parameters in": null, "the network (to 43M in the case of PTB and 70M for WT2) and leads to": null, "degradation by almost 8 perplexity points, which we attribute to": null, "overfitting in the word embeddings. While this could potentially be": null, "improved with more aggressive regularization, the computational overhead": null, "involved with substantially larger embeddings likely outweighs any": null, "advantages. Finally, we experiment with the removal of embedding": null, "dropout, AR/TAR and weight decay. In all of the cases, the model suffers": null, "a perplexity increase of 2\u20136 points which we hypothesize is due to": null, "insufficient regularization in the network.": null, "9 Conclusion": null, "In this work, we discuss regularization and optimization strategies for": null, "neural language models. We propose the weight-dropped LSTM, a strategy": null, "that uses a DropConnect mask on the hidden-to-hidden weight matrices, as": null, "a means to prevent overfitting across the recurrent connections.": null, "Further, we investigate the use of averaged SGD with a non-monontonic": null, "trigger for training language models and show that it outperforms SGD by": null, "a significant margin. We investigate other regularization strategies": null, "including the use of variable BPTT length and achieve a new": null, "state-of-the-art perplexity on the PTB and WikiText-2 data sets. Our": null, "models outperform custom-built RNN cells and complex regularization": null, "strategies that preclude the possibility of using optimized libraries": null, "such as the NVIDIA cuDNN LSTM. Finally, we explore the use of a neural": null, "cache in conjunction with our proposed model and show that this further": null, "improves the performance, thus attaining an even lower state-of-the-art": null, "perplexity. While the regularization and optimization strategies": null, "proposed are demonstrated on the task of language modeling, we": null, "anticipate that they would be generally applicable across other sequence": null, "learning tasks.": null, "References": null, "-   Arjovsky et\u00a0al. (2016) Arjovsky, M., Shah, A., and Bengio, Y.": null, "    Unitary evolution recurrent neural networks. In International": null, "    Conference on Machine Learning, pp.\u00a01120\u20131128, 2016.": null, "-   Ba et\u00a0al. (2016) Ba, J., Kiros, J., and Hinton, G.\u00a0E. Layer": null, "    normalization. CoRR, abs/1607.06450, 2016.": null, "-   Balduzzi & Ghifary (2016) Balduzzi, D. and Ghifary, M.": null, "    Strongly-typed recurrent neural networks. arXiv preprint": null, "    arXiv:1602.02218, 2016.": null, "-   Bottou et\u00a0al. (2016) Bottou, L., Curtis, F.\u00a0E., and Nocedal, J.": null, "    Optimization methods for large-scale machine learning. arXiv": null, "    preprint arXiv:1606.04838, 2016.": null, "-   Bradbury et\u00a0al. (2016) Bradbury, J., Merity, S., Xiong, C., and": null, "    Socher, R. Quasi-Recurrent Neural Networks. arXiv preprint": null, "    arXiv:1611.01576, 2016.": null, "-   Cooijmans et\u00a0al. (2016) Cooijmans, T., Ballas, N., Laurent, C., and": null, "    Courville, A.\u00a0C. Recurrent batch normalization. CoRR,": null, "    abs/1603.09025, 2016.": null, "-   Duchi et\u00a0al. (2011) Duchi, J., Hazan, E., and Singer, Y. Adaptive": null, "    subgradient methods for online learning and stochastic optimization.": null, "    Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.": null, "-   F\u00f6ldi\u00e1k (1991) F\u00f6ldi\u00e1k, P. Learning invariance from transformation": null, "    sequences. Neural Computation, 3(2):194\u2013200, 1991.": null, "-   Gal & Ghahramani (2016) Gal, Y. and Ghahramani, Z. A theoretically": null, "    grounded application of dropout in recurrent neural networks. In": null, "    NIPS, 2016.": null, "-   Goyal et\u00a0al. (2017) Goyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis,": null, "    P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K.": null, "    Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv": null, "    preprint arXiv:1706.02677, 2017.": null, "-   Grave et\u00a0al. (2016) Grave, E., Joulin, A., and Usunier, N. Improving": null, "    neural language models with a continuous cache. arXiv preprint": null, "    arXiv:1612.04426, 2016.": null, "-   Hardt et\u00a0al. (2015) Hardt, M., Recht, B., and Singer, Y. Train": null, "    faster, generalize better: Stability of stochastic gradient descent.": null, "    arXiv preprint arXiv:1509.01240, 2015.": null, "-   Hinton (1989) Hinton, G.\u00a0E. Connectionist learning procedures.": null, "    Artificial intelligence, 40(1-3):185\u2013234, 1989.": null, "-   Inan et\u00a0al. (2016) Inan, H., Khosravi, K., and Socher, R. Tying Word": null, "    Vectors and Word Classifiers: A Loss Framework for Language": null, "    Modeling. arXiv preprint arXiv:1611.01462, 2016.": null, "-   Ioffe & Szegedy (2015) Ioffe, S. and Szegedy, C. Batch": null, "    normalization: Accelerating deep network training by reducing": null, "    internal covariate shift. In ICML, 2015.": null, "-   Jing et\u00a0al. (2016) Jing, L., Shen, Y., Dub\u010dek, T., Peurifoy, J.,": null, "    Skirlo, S., Tegmark, M., and Solja\u010di\u0107, M. Tunable Efficient Unitary": null, "    Neural Networks (EUNN) and their application to RNN. arXiv preprint": null, "    arXiv:1612.05231, 2016.": null, "-   Jonschkowski & Brock (2015) Jonschkowski, R. and Brock, O. Learning": null, "    state representations with robotic priors. Auton. Robots,": null, "    39:407\u2013428, 2015.": null, "-   Keskar & Saon (2015) Keskar, N. and Saon, G. A nonmonotone learning": null, "    rate strategy for sgd training of deep neural networks. In": null, "    Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE": null, "    International Conference on, pp.\u00a0 4974\u20134978. IEEE, 2015.": null, "-   Kim et\u00a0al. (2016) Kim, Y., Jernite, Y., Sontag, D., and Rush, A.\u00a0M.": null, "    Character-aware neural language models. In Thirtieth AAAI Conference": null, "    on Artificial Intelligence, 2016.": null, "-   Kingma & Ba (2014) Kingma, D. and Ba, J. Adam: A method for": null, "    stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.": null, "-   Koehn et\u00a0al. (2007) Koehn, P., Hoang, H., Birch, A., Callison-Burch,": null, "    C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C.,": null, "    Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. Moses:": null, "    Open source toolkit for statistical machine translation. In ACL,": null, "    2007.": null, "-   Krueger et\u00a0al. (2016) Krueger, D., Maharaj, T., Kram\u00e1r, J.,": null, "    Pezeshki, M., Ballas, N., Ke, N., Goyal, A., Bengio, Y., Larochelle,": null, "    H., Courville, A., et\u00a0al. Zoneout: Regularizing RNNss by randomly": null, "    preserving hidden activations. arXiv preprint arXiv:1606.01305,": null, "    2016.": null, "-   Luciw & Schmidhuber (2012) Luciw, M. and Schmidhuber, J. Low": null, "    complexity proto-value function learning from sensory observations": null, "    with incremental slow feature analysis. Artificial Neural Networks": null, "    and Machine Learning\u2013ICANN 2012, pp.\u00a0 279\u2013287, 2012.": null, "-   Mandt et\u00a0al. (2017) Mandt, S., Hoffman, M.\u00a0D., and Blei, D.\u00a0M.": null, "    Stochastic gradient descent as approximate bayesian inference. arXiv": null, "    preprint arXiv:1704.04289, 2017.": null, "-   Melis et\u00a0al. (2017) Melis, G., Dyer, C., and Blunsom, P. On the": null, "    State of the Art of Evaluation in Neural Language Models. arXiv": null, "    preprint arXiv:1707.05589, 2017.": null, "-   Merity et\u00a0al. (2016) Merity, S., Xiong, C., Bradbury, J., and": null, "    Socher, R. Pointer Sentinel Mixture Models. arXiv preprint": null, "    arXiv:1609.07843, 2016.": null, "-   Merity et\u00a0al. (2017) Merity, S., McCann, B., and Socher, R.": null, "    Revisiting activation regularization for language rnns. arXiv": null, "    preprint arXiv:1708.01009, 2017.": null, "-   Mikolov & Zweig (2012) Mikolov, T. and Zweig, G. Context dependent": null, "    recurrent neural network language model. SLT, 12:234\u2013239, 2012.": null, "-   Mikolov et\u00a0al. (2010) Mikolov, T., Karafi\u00e1t, M., Burget, L.,": null, "    Cernock\u00fd, J., and Khudanpur, S. Recurrent neural network based": null, "    language model. In INTERSPEECH, 2010.": null, "-   Panageas & Piliouras (2016) Panageas, I. and Piliouras, G. Gradient": null, "    descent converges to minimizers: The case of non-isolated critical": null, "    points. CoRR, abs/1605.00405, 2016.": null, "-   Polyak & Juditsky (1992) Polyak, B. and Juditsky, A. Acceleration of": null, "    stochastic approximation by averaging. SIAM Journal on Control and": null, "    Optimization, 30(4):838\u2013855, 1992.": null, "-   Press & Wolf (2016) Press, O. and Wolf, L. Using the output": null, "    embedding to improve language models. arXiv preprint": null, "    arXiv:1608.05859, 2016.": null, "-   Semeniuta et\u00a0al. (2016) Semeniuta, S., Severyn, A., and Barth, E.": null, "    Recurrent dropout without memory loss. In COLING, 2016.": null, "-   Seo et\u00a0al. (2016) Seo, M., Min, S., Farhadi, A., and Hajishirzi, H.": null, "    Query-Reduction Networks for Question Answering. arXiv preprint": null, "    arXiv:1606.04582, 2016.": null, "-   Srivastava et\u00a0al. (2014) Srivastava, N., Hinton, G., Krizhevsky, A.,": null, "    Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to": null, "    prevent neural networks from overfitting. Journal of Machine": null, "    Learning Research, 15:1929\u20131958, 2014.": null, "-   Sutskever et\u00a0al. (2013) Sutskever, I., Martens, J., Dahl, G., and": null, "    Hinton, G. On the importance of initialization and momentum in deep": null, "    learning. In International conference on machine learning,": null, "    pp.\u00a01139\u20131147, 2013.": null, "-   Tallec & Ollivier (2017) Tallec, C. and Ollivier, Y. Unbiasing": null, "    truncated backpropagation through time. arXiv preprint": null, "    arXiv:1705.08209, 2017.": null, "-   Tieleman & Hinton (2012) Tieleman, T. and Hinton, G. Lecture": null, "    6.5-rmsprop: Divide the gradient by a running average of its recent": null, "    magnitude. COURSERA: Neural networks for machine learning,": null, "    4(2):26\u201331, 2012.": null, "-   Wan et\u00a0al. (2013) Wan, L., Zeiler, M., Zhang, S., LeCun, Y, and": null, "    Fergus, R. Regularization of neural networks using dropconnect. In": null, "    Proceedings of the 30th international conference on machine learning": null, "    (ICML-13), pp.\u00a0 1058\u20131066, 2013.": null, "-   Wilson et\u00a0al. (2017) Wilson, A.\u00a0C, Roelofs, R., Stern, M., Srebro,": null, "    N., and Recht, B. The marginal value of adaptive gradient methods in": null, "    machine learning. arXiv preprint arXiv:1705.08292, 2017.": null, "-   Wisdom et\u00a0al. (2016) Wisdom, S., Powers, T., Hershey, J., Le\u00a0Roux,": null, "    J., and Atlas, L. Full-capacity unitary recurrent neural networks.": null, "    In Advances in Neural Information Processing Systems, pp.\u00a04880\u20134888,": null, "-   Zaremba et\u00a0al. (2014) Zaremba, W., Sutskever, I., and Vinyals, O.": null, "    Recurrent neural network regularization. arXiv preprint": null, "    arXiv:1409.2329, 2014.": null, "-   Zilly et\u00a0al. (2016) Zilly, J.\u00a0G., Srivastava, R.\u00a0K., Koutn\u00edk, J.,": null, "    and Schmidhuber, J. Recurrent highway networks. arXiv preprint": null, "    arXiv:1607.03474, 2016.": null, "-   Zoph & Le (2016) Zoph, B. and Le, Q.\u00a0V. Neural architecture search": null, "    with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.": null, "\u25c4 [ar5iv homepage] Feeling": null, "lucky? Conversion": null, "report Report": null, "an issue View\u00a0original": null, "on\u00a0arXiv\u25ba": null, "Copyright Privacy Policy": null, "Generated on Fri Dec 31 15:01:12 2021 by LaTeXML [[LOGO]]": null}