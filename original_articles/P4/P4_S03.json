LSTM We refer to the mathematical formulation of the LSTM,  - - - -  i_(t) \u2004=\u2004\u03c3(W^(i)x_(t)+U^(i)h_(t\u2005\u2212\u20051))   f_(t) \u2004=\u2004\u03c3(W^(f)x_(t)+U^(f)h_(t\u2005\u2212\u20051))   o_(t) \u2004=\u2004\u03c3(W^(o)x_(t)+U^(o)h_(t\u2005\u2212\u20051))   ${\\overset{\\sim}{c}}_{t}$ \u2004=\u2004tanh(W^(c)x_(t)+U^(c)h_(t\u2005\u2212\u20051))   c_(t) $= i_{t} \\odot {\\overset{\\sim}{c}}_{t} + f_{t} \\odot + {\\overset{\\sim}{c}}_{t - 1}$   h_(t) \u2004=\u2004o_(t)\u2005\u2299\u2005tanh(c_(t))  where [W^(i),\u2006W^(f),\u2006W^(o),\u2006U^(i),\u2006U^(f),\u2006U^(o)] are weight matrices, x_(t) is the vector input to the timestep t, h_(t) is the current exposed hidden state, c_(t) is the memory cell state, and \u2299 is element-wise multiplication. Preventing overfitting within the recurrent connections of an RNN has been an area of extensive research in language modeling. The majority of previous recurrent regularization techniques have acted on the hidden state vector h_(t\u2005\u2212\u20051), most frequently introducing a dropout operation between timesteps, or performing dropout on the update to the memory state c_(t). These modifications to a standard LSTM prevent the use of black box RNN implementations that may be many times faster due to low-level hardware-specific optimizations. We propose the use of DropConnect (Wan et\u00a0al., 2013) on the recurrent hidden to hidden weight matrices which does not require any modifications to an RNN\u2019s formulation. As the dropout operation is applied once to the weight matrices, before the forward and backward pass, the impact on training speed is minimal and any standard RNN implementation can be used, including inflexible but highly optimized black box LSTM implementations such as NVIDIA\u2019s cuDNN LSTM. By performing DropConnect on the hidden-to-hidden weight matrices [U^(i),\u2006U^(f),\u2006U^(o),\u2006U^(c)] within the LSTM, we can prevent overfitting from occurring on the recurrent connections of the LSTM. This regularization technique would also be applicable to preventing overfitting on the recurrent weight matrices of other RNN cells. As the same weights are reused over multiple timesteps, the same individual dropped weights remain dropped for the entirety of the forward and backward pass. The result is similar to variational dropout, which applies the same dropout mask to recurrent connections within the LSTM by performing dropout on h_(t\u2005\u2212\u20051), except that the dropout is applied to the recurrent weights. DropConnect could also be used on the non-recurrent weights of the LSTM [W^(i),\u2006W^(f),\u2006W^(o)] though our focus was on preventing overfitting on the recurrent connection.