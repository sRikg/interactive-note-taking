The adapter size controls the parameter efficiency, smaller adapters introduce fewer parameters, at a possible cost to performance. To explore this trade-off, we consider different adapter sizes, and compare to two baselines: (i) Fine-tuning of only the top k layers of BERT_(BASE). (ii) Tuning only the layer normalization parameters. The learning rate is tuned using the same range as in Section\u00a03.2. Figure\u00a03 shows the parameter/performance trade-off aggregated over all tasks in each suite. On GLUE, one sees a dramatic decrease in performance as fewer layers are tuned. Some of the additional tasks benefit from training fewer layers, so performance of fine-tuning decays much less. In both cases, adapters yield good performance across a range of sizes two orders of magnitude fewer than fine-tuning. Figure\u00a04 shows more details for two GLUE tasks: MNLI_(m) and CoLA. Top layer tuning has more task-specific parameters for all k\u2004>\u20042. When fine-tuning using a comparable number of task-specific parameters, the performance decreases substantially compared to our strategy. For instance, fine-tuning just the top layer yields approximately 9M trainable parameters and 77.8%\u2005\u00b1\u20050.1% validation accuracy on MNLI_(m). In contrast, adapter tuning with size 64 yields approximately 2M trainable parameters and 83.7%\u2005\u00b1\u20050.1% validation accuracy. For comparison full fine-tuning attains 84.4%\u2005\u00b1\u20050.02% on MNLI_(m). We observe a similar trend on CoLA. As a further comparison, we tune the parameters of layer normalization alone. These layers only contain point-wise additions and multiplications, so introduce very few trainable parameters: 40k for BERT_(BASE). However this strategy performs poorly: performance decreases by approximately 3.5% on CoLA and 4% on MNLI. To summarize, adapter tuning is highly parameter-efficient, and is able to produce a compact model with a strong performance, comparable to full fine-tuning. Training adapters with sizes 0.5\u2005\u2212\u20055% of the original model, performance is within 1% of the competitive published results on BERT_(LARGE).