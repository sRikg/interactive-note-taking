Multi-task learning (MTL) involves training on tasks simultaneously. Early work shows that sharing network parameters across tasks exploits task regularities, yielding improved performance\u00a0(Caruana, 1997). The authors share weights in lower layers of a network, and use specialized higher layers. Many NLP systems have exploited MTL. Some examples include: text processing systems (part of speech, chunking, named entity recognition, etc.)\u00a0(Collobert & Weston, 2008), multilingual models\u00a0(Huang et\u00a0al., 2013), semantic parsing\u00a0(Peng et\u00a0al., 2017), machine translation\u00a0(Johnson et\u00a0al., 2017), and question answering\u00a0(Choi et\u00a0al., 2017). MTL yields a single model to solve all problems. However, unlike our adapters, MTL requires simultaneous access to all of the tasks during training.