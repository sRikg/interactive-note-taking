"We show on many datasets that adapters achieve parameter efficient transfer for text classification. On the GLUE benchmark\u00a0(Wang et\u00a0al., 2018), adapter tuning is within 0.4% of full fine-tuning of BERT, but it adds only 3% of the number of parameters trained by fine-tuning. We confirm this result on a further 17 public classification tasks. We finish with an analysis that shows that adapter-based tuning automatically focuses on the higher layers and is robust to choices such as initialization scheme and number of neurons. We use the public, pre-trained BERT Transformer network as our base model. To perform classification with BERT, we take the approach presented in\u00a0Devlin et\u00a0al. (2018). The first token in each sequence is a special \u201cclassification token\u201d. We attach a linear layer to the embedding of this token to predict the class label. Our training procedure follows\u00a0Devlin et\u00a0al. (2018). We optimize using Adam\u00a0(Kingma & Ba, 2014), whose learning rate is increased linearly over the first 10% of the steps, and then decayed linearly to zero. All training runs are performed on 4 Google Cloud TPUs with a batch size of 32. For each dataset and algorithm, we run a hyperparameter sweep and select the best model according to accuracy on the validation set. On the GLUE tasks, we report the test metrics provided by the submission website\u00b9\u00b91https://gluebenchmark.com/. We report test-set accuracy on the other datasets. We compare to fine-tuning, the current standard for transfer of large pre-trained models, and the strategy successfully used by BERT. For N tasks, full fine-tuning requires N\u00d7 the number of parameters of the pre-trained model. Our goal is to attain performance equal to fine-tuning, but with the fewest total parameters, ideally near to 1\u00d7."
