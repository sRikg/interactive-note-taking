We provide additional analyses and discussion of the adapters. First, we perform an ablation to determine which adapters are most influential. For each layer in turn, we remove its adapters and evaluate the model on the validation set. The experiment is performed on BERT_(BASE) with adapter size 64 on MNLI and CoLA. Figure\u00a05 shows the results. The performance decrease is, perhaps surprisingly, always small; the largest drop is 2%. In contrast, when all of the adapters are removed from the network, the performance drops to 69% on CoLA and to 37% on MNLI \u2013 scores attained by predicting the majority class on these datasets. This indicates that although each adapter has a small influence on the overall network, the overall effect is large. Figure\u00a05 also suggests that the adapters on the lower layers have a smaller effect than those on higher layers. This indicates that the strong performance of adapters might come from the way it prioritizes automatically the top layers. Indeed, the strategy to focus on the upper layers is popular in fine-tuning\u00a0(Howard & Ruder, 2018). One possible intuition is that lower layers of the network extract lower-level features that are shared among tasks, while the higher layers build features that are unique to different tasks. This is related to our observation that for some tasks, fine-tuning only the top layers outperforms full fine-tuning, see Table\u00a02. Next, we investigate the robustness of the adapter modules to number of neurons and initialization scale. In our main experiments the weights of the fully connected layers in the adapter module were drawn from a zero-centered Gaussian with standard deviation of 10^(\u2005\u2212\u20052), truncated to two standard deviations. To analyze the impact of the initialization scale on the performance, we pick standard deviations in the interval [10^(\u2005\u2212\u20057),\u20061]. Figure\u00a06 summarizes the results. We observe that on both datasets, the performance of adapters is robust for standard deviations below 10^(\u2005\u2212\u20052). However, when the initialization is too large, performance degrades, more substantially on CoLA. To investigate robustness of adapters to the number of neurons, we re-examine the experimental data from Section\u00a03.2. We find that the quality of the model across adapter sizes is stable, and a fixed adapter size across all the tasks could be used with small detriment to performance. For each adapter size we calculate the mean validation accuracy across the eight classification tasks by selecting the optimal learning rate and number of epochs\u2075\u20755 We treat here MNLI_(m) and MNLI_(mm) as separate tasks. For consistency, for all datasets we use accuracy metric and exclude the regression STS-B task.. For adapter sizes 8, 64, and 256, the mean validation accuracies are 86.2%, 85.8% and 85.7%, respectively. This message is further corroborated by Figure\u00a04, which shows a stable performance across few orders of magnitude. Finally, we experimented with a number of extensions to the adapter\u2019s architecture that did not yield a significant boost in performance. We document them here for completeness. We experimented with (i) adding a batch/layer normalization to the adapter, (ii) increasing the number of layers per adapter, (iii) different activation functions, such as tanh, (iv) inserting adapters only inside the attention layer, (v) adding adapters in parallel to the main layers, and possibly with a multiplicative interaction. In all cases we observed the resulting performance to be similar to the bottleneck proposed in Section\u00a02.1. Therefore, due to its simplicity and strong performance, we recommend the original adapter architecture. [Figure 5: Ablation of trained adapters from each layer of BERT. x-axis: The index of the layer whose adapters are removed. y-axis: Relative performance of the model before and after ablation. Values smaller than zero indicate a decrease in performance after ablation. The line indicates the mean across three models trained with different random seeds, and the error bars indicate \u2005\u00b1\u20051 s.e.m.] [Figure 6: Performance of the BERT_(BASE) model with adapters under varying weight magnitude at initialization. The weights are sampled from a zero-centered, truncated normal distribution of standard deviation \u03c3. Adapters are generally robust with respect to initialization parameters. However, performance rapidly decays when using standard deviations above 10^(\u2005\u2212\u20052).]