3.4 Parameter/Performance trade-off 

The adapter size controls the parameter efficiency, smaller adapters introduce fewer parameters, at a possible cost to performance. To explore this trade-off, we consider different adapter sizes, and compare to two baselines: (i) Fine-tuning of only the top k layers of BERT_(BASE). (ii) Tuning only the layer normalization parameters. The learning rate is tuned using the same range as in Section\u00a03.2. Figure\u00a03 shows the parameter/performance trade-off aggregated over all tasks in each suite. On GLUE, one sees a dramatic decrease in performance as fewer layers are tuned. Some of the additional tasks benefit from training fewer layers, so performance of fine-tuning decays much less. In both cases, adapters yield good performance across a range of sizes two orders of magnitude fewer than fine-tuning. Figure\u00a04 shows more details for two GLUE tasks: MNLI_(m) and CoLA. Top layer tuning has more task-specific parameters for all k\u2004>\u20042. When fine-tuning using a comparable number of task-specific parameters, the performance decreases substantially compared to our strategy. For instance, fine-tuning just the top layer yields approximately 9M trainable parameters and 77.8%\u2005\u00b1\u20050.1% validation accuracy on MNLI_(m). In contrast, adapter tuning with size 64 yields approximately 2M trainable parameters and 83.7%\u2005\u00b1\u20050.1% validation accuracy. For comparison full fine-tuning attains 84.4%\u2005\u00b1\u20050.02% on MNLI_(m). We observe a similar trend on CoLA. As a further comparison, we tune the parameters of layer normalization alone. These layers only contain point-wise additions and multiplications, so introduce very few trainable parameters: 40k for BERT_(BASE). However this strategy performs poorly: performance decreases by approximately 3.5% on CoLA and 4% on MNLI. To summarize, adapter tuning is highly parameter-efficient, and is able to produce a compact model with a strong performance, comparable to full fine-tuning. Training adapters with sizes 0.5\u2005\u2212\u20055% of the original model, performance is within 1% of the competitive published results on BERT_(LARGE). 


3.5 Analysis and Discussion 

We provide additional analyses and discussion of the adapters. First, we perform an ablation to determine which adapters are most influential. For each layer in turn, we remove its adapters and evaluate the model on the validation set. The experiment is performed on BERT_(BASE) with adapter size 64 on MNLI and CoLA. Figure\u00a05 shows the results. The performance decrease is, perhaps surprisingly, always small; the largest drop is 2%. In contrast, when all of the adapters are removed from the network, the performance drops to 69% on CoLA and to 37% on MNLI \u2013 scores attained by predicting the majority class on these datasets. This indicates that although each adapter has a small influence on the overall network, the overall effect is large. Figure\u00a05 also suggests that the adapters on the lower layers have a smaller effect than those on higher layers. This indicates that the strong performance of adapters might come from the way it prioritizes automatically the top layers. Indeed, the strategy to focus on the upper layers is popular in fine-tuning\u00a0(Howard & Ruder, 2018). One possible intuition is that lower layers of the network extract lower-level features that are shared among tasks, while the higher layers build features that are unique to different tasks. This is related to our observation that for some tasks, fine-tuning only the top layers outperforms full fine-tuning, see Table\u00a02. Next, we investigate the robustness of the adapter modules to number of neurons and initialization scale. In our main experiments the weights of the fully connected layers in the adapter module were drawn from a zero-centered Gaussian with standard deviation of 10^(\u2005\u2212\u20052), truncated to two standard deviations. To analyze the impact of the initialization scale on the performance, we pick standard deviations in the interval [10^(\u2005\u2212\u20057),\u20061]. Figure\u00a06 summarizes the results. We observe that on both datasets, the performance of adapters is robust for standard deviations below 10^(\u2005\u2212\u20052). However, when the initialization is too large, performance degrades, more substantially on CoLA. To investigate robustness of adapters to the number of neurons, we re-examine the experimental data from Section\u00a03.2. We find that the quality of the model across adapter sizes is stable, and a fixed adapter size across all the tasks could be used with small detriment to performance. For each adapter size we calculate the mean validation accuracy across the eight classification tasks by selecting the optimal learning rate and number of epochs\u2075\u20755 We treat here MNLI_(m) and MNLI_(mm) as separate tasks. For consistency, for all datasets we use accuracy metric and exclude the regression STS-B task.. For adapter sizes 8, 64, and 256, the mean validation accuracies are 86.2%, 85.8% and 85.7%, respectively. This message is further corroborated by Figure\u00a04, which shows a stable performance across few orders of magnitude. Finally, we experimented with a number of extensions to the adapter\u2019s architecture that did not yield a significant boost in performance. We document them here for completeness. We experimented with (i) adding a batch/layer normalization to the adapter, (ii) increasing the number of layers per adapter, (iii) different activation functions, such as tanh, (iv) inserting adapters only inside the attention layer, (v) adding adapters in parallel to the main layers, and possibly with a multiplicative interaction. In all cases we observed the resulting performance to be similar to the bottleneck proposed in Section\u00a02.1. Therefore, due to its simplicity and strong performance, we recommend the original adapter architecture. [Figure 5: Ablation of trained adapters from each layer of BERT. x-axis: The index of the layer whose adapters are removed. y-axis: Relative performance of the model before and after ablation. Values smaller than zero indicate a decrease in performance after ablation. The line indicates the mean across three models trained with different random seeds, and the error bars indicate \u2005\u00b1\u20051 s.e.m.] [Figure 6: Performance of the BERT_(BASE) model with adapters under varying weight magnitude at initialization. The weights are sampled from a zero-centered, truncated normal distribution of standard deviation \u03c3. Adapters are generally robust with respect to initialization parameters. However, performance rapidly decays when using standard deviations above 10^(\u2005\u2212\u20052).] 

4 Related Work 

Pre-trained text representations 

Pre-trained textual representations are widely used to improve performance on NLP tasks. These representations are trained on large corpora (usually, but not always, unsupervised), and fed as features to downstream models. In deep networks, these features may also be fine-tuned on the downstream task. Brown clusters, trained on distributional information, are a classic example of pre-trained representations\u00a0(Brown et\u00a0al., 1992). Turian et\u00a0al. (2010) show that pre-trained embeddings of words outperform those trained from scratch. Since the deep-learning era, word embeddings have been widely used, and training strategies these have arisen\u00a0(Mikolov et\u00a0al., 2013; Pennington et\u00a0al., 2014; Bojanowski et\u00a0al., 2017). Embeddings of longer texts, sentences and paragraphs, have also been developed\u00a0(Le & Mikolov, 2014; Kiros et\u00a0al., 2015; Conneau et\u00a0al., 2017; Cer et\u00a0al., 2019). To encode context in these representations, features are extracted from internal representations of sequence models, such as MT systems\u00a0(McCann et\u00a0al., 2017), and BiLSTM language models, as used in ELMo\u00a0(Peters et\u00a0al., 2018). As with adapters, ELMo exploits the layers other than the top layer of a pre-trained network. However, this strategy only reads from the inner layers. In contrast, adapters write to the inner layers, re-configuring the processing of features through the entire network. 

Fine-tuning 

Fine-tuning an entire pre-trained model has become a popular alternative to features\u00a0(Dai & Le, 2015; Howard & Ruder, 2018; Radford et\u00a0al., 2018) In NLP, the upstream model is usually a neural language model\u00a0(Bengio et\u00a0al., 2003). Recent state-of-the-art results on question answering\u00a0(Rajpurkar et\u00a0al., 2016) and text classification\u00a0(Wang et\u00a0al., 2018) have been attained by fine-tuning a Transformer network\u00a0(Vaswani et\u00a0al., 2017) with a Masked Language Model loss\u00a0(Devlin et\u00a0al., 2018). Performance aside, an advantage of fine-tuning is that it does not require task-specific model design, unlike representation-based transfer. However, vanilla fine-tuning does require a new set of network weights for every new task. 

Multi-task Learning 

Multi-task learning (MTL) involves training on tasks simultaneously. Early work shows that sharing network parameters across tasks exploits task regularities, yielding improved performance\u00a0(Caruana, 1997). The authors share weights in lower layers of a network, and use specialized higher layers. Many NLP systems have exploited MTL. Some examples include: text processing systems (part of speech, chunking, named entity recognition, etc.)\u00a0(Collobert & Weston, 2008), multilingual models\u00a0(Huang et\u00a0al., 2013), semantic parsing\u00a0(Peng et\u00a0al., 2017), machine translation\u00a0(Johnson et\u00a0al., 2017), and question answering\u00a0(Choi et\u00a0al., 2017). MTL yields a single model to solve all problems. However, unlike our adapters, MTL requires simultaneous access to all of the tasks during training. 

Continual Learning 

As an alternative to simultaneous training, continual, or lifelong, learning algorithms aim to learn from a sequence of tasks\u00a0(Thrun, 1998). However, when re-trained, deep networks tend to forget how to perform previous tasks; a challenge termed catastrophic forgetting\u00a0(McCloskey & Cohen, 1989; French, 1999). Techniques have been proposed to mitigate forgetting\u00a0(Kirkpatrick et\u00a0al., 2017; Zenke et\u00a0al., 2017), however, unlike for adapters, the memory is still imperfect. Progressive Networks avoid forgetting by instantiating a new network \u201ccolumn\u201d for each task\u00a0(Rusu et\u00a0al., 2016). However, the number of parameters grows linearly with the number of tasks, since adapters are very small, our models scale much more favorably. 

Transfer Learning in Visual Perception 

Fine-tuning models pre-trained on ImageNet\u00a0(Deng et\u00a0al., 2009) is ubiquitous when building image recognition models\u00a0(Yosinski et\u00a0al., 2014; Huh et\u00a0al., 2016). This technique attains state-of-the-art performance on many vision tasks, including classification\u00a0(Kornblith et\u00a0al., 2018), fine-grained classifcation\u00a0(Hermans et\u00a0al., 2017), segmentation\u00a0(Long et\u00a0al., 2015), and detection\u00a0(Girshick et\u00a0al., 2014). In vision, convolutional adapter modules have been studied\u00a0(Rebuffi et\u00a0al., 2017, 2018; Rosenfeld & Tsotsos, 2018). These works perform incremental learning in multiple domains by adding small convolutional layers to a ResNet\u00a0(He et\u00a0al., 2016) or VGG net\u00a0(Simonyan & Zisserman, 2014). Adapter size is limited using 1\u2005\u00d7\u20051 convolutions, whilst the original networks typically use 3\u2005\u00d7\u20053. This yields 11% increase in overall model size per task. Since the kernel size cannot be further reduced other weight compression techniques must be used to attain further savings. Our bottleneck adapters can be much smaller, and still perform well. 

5 Conclusion 

We propose a transfer strategy for large text models using bottleneck adapter modules. Adapter tuning yields models that are competitive with state-of-the-art fine-tuning, whilst using only small number of task-specific parameters. This strategy is designed for an online setting, where tasks arrive in series, and one wants to solve them with a single model. For future work, adapters could be applied in multi-task training, where tasks are available simultaneously. Adapter modules could learn task-specific features while the main network learns generalizable representations. Unlike traditional approaches that use a task specific network \u201chead\u201d, adapters allow task-dependent modulation at any layer.