\\pbox3cmTotal num params \\pbox3cmTrained params / task CoLA SST MRPC STS-B QQP MNLI_(m) MNLI_(mm) QNLI RTE Total BERT_(LARGE) 9.0\u00d7 100% 60.5 94.9 89.3 87.6 72.1 86.7 85.9 91.1 70.1 80.4 Adapters (8-256) 1.3\u00d7 3.6% 59.5 94.0 89.5 86.9 71.8 84.9 85.1 90.7 71.5 80.0 Adapters (64) 1.2\u00d7 2.1% 56.9 94.2 89.6 87.3 71.8 85.3 84.6 91.4 68.8 79.6 Table 1: Results on GLUE test sets scored using the GLUE evaluation server. MRPC and QQP are evaluated using F1 score. STS-B is evaluated using Spearman\u2019s correlation coefficient. The other tasks are evaluated using accuracy. Adapter tuning achieves comparable overall score (80.0) to full fine-tuning (80.4) using 1.3\u00d7 parameters in total, compared to 9\u00d7. Fixing the adapter size to 64 leads to a slightly decreased overall score of 79.6 and slightly smaller model. We first evaluate on GLUE.\u00b2\u00b22 We omit WNLI as in\u00a0Devlin et\u00a0al. (2018) because the no current algorithm beats the baseline of predicting the majority class. For these datasets, we transfer from the pre-trained BERT_(LARGE) model, which contains 24 layers, and a total of 330M parameters, see\u00a0Devlin et\u00a0al. (2018) for details. We perform a small hyperparameter sweep for adapter tuning: We sweep learning rates in {3\u2005\u22c5\u200510^(\u2005\u2212\u20055),\u20063\u2005\u22c5\u200510^(\u2005\u2212\u20054),\u20063\u2005\u22c5\u200510^(\u2005\u2212\u20053)}, and number of epochs in {3,\u200620}. We test both using a fixed adapter size (number of units in the bottleneck), and selecting the best size per task from {8,\u200664,\u2006256}. The adapter size is the only adapter-specific hyperparameter that we tune. Finally, due to training instability, we rerun 5 times with different random seeds and select the best model on the validation set, similar to\u00a0Devlin et\u00a0al. (2018). Table\u00a01 summarizes the results. Adapters achieve a mean GLUE score of 80.0, compared to 80.4 achieved by full fine-tuning. The optimal adapter size varies per dataset. For example, 256 is chosen for MNLI, whereas for the smallest dataset, RTE, 8 is chosen. Restricting always to size 64, leads to a small decrease in average accuracy to 79.6. To solve all of the datasets in Table\u00a01, fine-tuning requires 9\u00d7 the total number of BERT parameters.\u00b3\u00b33 We treat MNLI_(m) and MNLI_(mm) as separate tasks with individually tuned hyperparameters. However, they could be combined into one model, leaving 8\u00d7 overall. In contrast, adapters only require 1.3\u00d7.