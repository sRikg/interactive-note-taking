We instantiate adapter-based tuning for text Transformers. These models attain state-of-the-art performance in many NLP tasks, including translation, extractive QA, and text classification problems\u00a0(Vaswani et\u00a0al., 2017; Radford et\u00a0al., 2018; Devlin et\u00a0al., 2018). We consider the standard Transformer architecture, as proposed in\u00a0Vaswani et\u00a0al. (2017). Adapter modules present many architectural choices. We provide a simple design that attains good performance. We experimented with a number of more complex designs, see Section\u00a03.5, but we found the following strategy performed as well as any other that we tested, across many datasets. Figure\u00a02 shows our adapter architecture, and its application it to the Transformer. Each layer of the Transformer contains two primary sub-layers: an attention layer and a feedforward layer. Both layers are followed immediately by a projection that maps the features size back to the size of layer\u2019s input. A skip-connection is applied across each of the sub-layers. The output of each sub-layer is fed into layer normalization. We insert two serial adapters after each of these sub-layers. The adapter is always applied directly to the output of the sub-layer, after the projection back to the input size, but before adding the skip connection back. The output of the adapter is then passed directly into the following layer normalization. To limit the number of parameters, we propose a bottleneck architecture. The adapters first project the original d-dimensional features into a smaller dimension, m, apply a nonlinearity, then project back to d dimensions. The total number of parameters added per layer, including biases, is 2md\u2005+\u2005d\u2005+\u2005m. By setting m\u2004\u226a\u2004d, we limit the number of parameters added per task; in practice, we use around 0.5\u2005\u2212\u20058% of the parameters of the original model. The bottleneck dimension, m, provides a simple means to trade-off performance with parameter efficiency. The adapter module itself has a skip-connection internally. With the skip-connection, if the parameters of the projection layers are initialized to near-zero, the module is initialized to an approximate identity function. Alongside the layers in the adapter module, we also train new layer normalization parameters per task. This technique, similar to conditional batch normalization\u00a0(De\u00a0Vries et\u00a0al., 2017), FiLM\u00a0(Perez et\u00a0al., 2018), and self-modulation\u00a0(Chen et\u00a0al., 2019), also yields parameter-efficient adaptation of a network; with only 2d parameters per layer. However, training the layer normalization parameters alone is insufficient for good performance, see Section\u00a03.4.