"\"\\nGloVe: Global Vectors for Word Representation\\n\\nJeffrey Pennington, Richard Socher, Christopher D. Manning\\nComputer Science Department, Stanford University, Stanford, CA 94305\\njpennin@stanford.edu, richard@socher.org, manning@stanford.edu\\n\\n\\n\\n\\n\\nAbstract\\nRecent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. The model pro- duces a vector space with meaningful sub- structure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on simi- larity tasks and named entity recognition.\\nIntroduction\\nSemantic vector space models of language repre- sent each word with a real-valued vector. These vectors can be used as features in a variety of ap- plications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013).\\nMost word vector methods rely on the distance or angle between pairs of word vectors as the pri- mary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evalua- tion scheme based on word analogies that probes\\nthe finer structure of the word vector space by ex- amining not the scalar distance between word vec- tors, but rather their various dimensions of dif- ference.\\tFor example, the analogy \\u201cking is to queen as man is to woman\\u201d should be encoded  in the vector space by the vector equation king queen = man\\twoman. This evaluation scheme favors models that produce dimensions of mean- ing, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009).\\nThe two main model families for learning word vectors are: 1) global matrix factorization meth- ods, such as latent semantic analysis (LSA) (Deer- wester et al., 1990) and 2) local context window methods, such as the skip-gram model of Mikolov et al. (2013c). Currently, both families suffer sig- nificant drawbacks. While methods like LSA ef- ficiently leverage statistical information, they do relatively poorly on the word analogy task, indi- cating a sub-optimal vector space structure. Meth- ods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the cor- pus since they train on separate local context win- dows instead of on global co-occurrence counts.\\nIn this work, we analyze the model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression mod- els are appropriate for doing so. We propose a spe- cific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model pro- duces a word vector space with meaningful sub- structure, as evidenced by its state-of-the-art per- formance of 75% accuracy on the word analogy dataset. We also demonstrate that our methods outperform other current methods on several word similarity tasks, and also on a common named en- tity recognition (NER) benchmark.\\nWe provide the source code for the model as well as trained word vectors at http://nlp. stanford.edu/projects/glove/.\\n\\n\\n1532\\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\\u20131543, October 25-29, 2014, Doha, Qatar. \\u24cdc 2014 Association for Computational Linguistics\\n\\nRelated Work\\nMatrix Factorization Methods. Matrix factor- ization methods for generating low-dimensional word representations have roots stretching as far back as LSA. These methods utilize low-rank ap- proximations to decompose large matrices that capture statistical information about a corpus. The particular type of information captured by such matrices varies by application. In LSA, the ma- trices are of \\u201cterm-document\\u201d type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus. In contrast, the Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), for example, utilizes matrices of \\u201cterm-term\\u201d type, i.e., the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word.\\nA main problem with HAL and related meth- ods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness. A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method (Rohde et al., 2006), in which the co-occurrence matrix is first transformed by an entropy- or correlation-based normalization. An advantage of this type of trans- formation is that the raw co-occurrence counts, which for a reasonably sized corpus might span  8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller inter- val. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mu- tual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effec- tive way of learning word representations.\\nShallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context win- dows. For example, Bengio et al. (2003) intro- duced a model that learns word vector representa- tions as part of a simple neural network architec- ture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved\\nthe way for Collobert et al. (2011) to use the full context of a word for learning the word represen- tations, rather than just the preceding context as is the case with language models.\\nRecently, the importance of the full neural net- work structure for learning useful word repre- sentations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a sim- ple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embed- dings based on a PPMI metric.\\nIn the skip-gram and ivLBL models, the objec- tive is to predict a word\\u2019s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its con- text. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors.\\nUnlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the en- tire corpus, which fails to take advantage of the vast amount of repetition in the data.\\n\\nThe GloVe Model\\nThe statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word represen- tations, and although many such methods now ex- ist, the question still remains as to how meaning is generated from these statistics, and how the re- sulting word vectors might represent that meaning. In this section, we shed some light on this ques- tion. We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statis- tics are captured directly by the model.\\nFirst we establish some notation. Let the matrix of word-word co-occurrence counts be denoted by X , whose entries Xi j tabulate the number of times word j  occurs in the context of word i.  Let Xi = k Xik be the number of times any word appears  in the context of word i. Finally, let Pi j = P( j i) = Xi j /Xi be the probability that word j appear in the\\n\\nTable 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6 billion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam.\\n\\n\\n\\n\\nP(k |steam)\\nP(k |ice)/P(k |steam)\\ncontext of word i.\\n2.2 \\u00d7 10\\u22125\\t7.8 \\u00d7 10\\u22124\\t2.2 \\u00d7 10\\u22123\\t1.8 \\u00d7 10\\u22125\\n8.9\\t8.5 \\u00d7 10\\u22122\\t1.36\\t0.96\\nthe information present the ratio Pik /Pjk in the\\n\\nWe begin with a simple example that showcases how certain aspects of meaning can be extracted directly from co-occurrence probabilities. Con- sider two words i and j that exhibit a particular as- pect of interest; for concreteness, suppose we are interested in the concept of thermodynamic phase, for which we might take i = ice and j = steam. The relationship of these words can be examined by studying the ratio of their co-occurrence prob- abilities with various probe words, k.  For words k related to ice but not steam, say k = solid, we expect the ratio Pik /Pjk will be large. Similarly, for words k related to steam but not ice, say k = gas, the ratio should be small. For words k like water or fashion, that are either related to both ice and steam, or to neither, the ratio should be close to one. Table 1 shows these probabilities and their ratios for a large corpus, and the numbers confirm these expectations. Compared to the raw probabil- ities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discrimi- nate between the two relevant words.\\nThe above argument suggests that the appropri-\\nate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves. Noting that the ratio Pik /Pjk depends on three words i, j, and k, the most general model takes the form,\\nword vector space. Since vector spaces are inher- ently linear structures, the most natural way to do this is with vector differences. With this aim, we can restrict our consideration to those functions F that depend only on the difference of the two target words, modifying Eqn. (1) to,\\nF (wi\\tw j , w\\u02dc k ) =  Pik  .\\t(2)\\njk\\nNext, we note that the arguments of F in Eqn. (2) are vectors while the right-hand side is a scalar. While F could be taken to be a complicated func- tion parameterized by, e.g., a neural network, do- ing so would obfuscate the linear structure we are trying to capture. To avoid this issue, we can first take the dot product of the arguments,\\nF .(wi \\u2212 w j )T w\\u02dc k \\u03a3 =  Pik  ,\\t(3)\\nwhich prevents F from mixing the vector dimen- sions in undesirable ways. Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles. To do so consistently, we must not only exchange w       w\\u02dc but also X XT . Our final model should be in- variant under this relabeling, but Eqn. (3) is not. However, the symmetry can be restored in two steps. First, we require that F be a homomorphism\\n\\nF (wi , w j , w\\u02dc k ) =\\nPik\\n\\n,\\t(1)\\nbetween the groups (R,+) and (R>0, \\u00d7 ), i.e.,\\n\\nPjk\\nd\\nd\\tF .(wi\\n\\u2212 w j )T w\\u02dc\\nk \\u03a3 =\\nF (wT w\\u02dc k )\\n\\n\\nF (wT w\\u02dc   )\\n,\\t(4)\\n\\nwhere  w   \\u2208   R\\tare  word  vectors  and  w\\u02dc   \\u2208   R\\tj\\tk\\n\\nare separate context word vectors whose role will be discussed in Section 4.2. In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified pa- rameters. The number of possibilities for F is vast, but by enforcing a few desiderata we can select a unique choice. First, we would like F to encode\\nwhich, by Eqn. (3), is solved by,\\nF (wT w\\u02dc k ) = Pik  =  Xik  .\\t(5)\\ni\\tXi\\nThe solution to Eqn. (4) is F = exp, or,\\nwT w\\u02dc k  = log(Pik ) = log(Xik ) \\u2212 log(Xi ) .\\t(6)\\n\\nNext, we note that Eqn. (6) would exhibit the ex- change symmetry if not for the log(Xi ) on the right-hand side. However, this term is indepen- dent of k so it can be absorbed into a bias bi for wi .  Finally, adding an additional bias b\\u02dck   for w\\u02dc k restores the symmetry,\\nwT w\\u02dc k + bi + b\\u02dck  = log(Xik ) .\\t(7)\\nEqn. (7) is a drastic simplification over Eqn. (1), but it is actually ill-defined since the logarithm di- verges whenever its argument is zero. One reso- lution to this issue is to include an additive shift in the logarithm, log(Xik ) log(1 + Xik ), which maintains the sparsity of X while avoiding the di- vergences. The idea of factorizing the log of the co-occurrence matrix is closely related to LSA and we will use the resulting model as a baseline in our experiments. A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never. Such rare co- occurrences are noisy and carry less information than the more frequent ones \\u2014 yet even just the zero entries account for 75\\u201395% of the data in X , depending on the vocabulary size and corpus.\\nWe propose a new weighted least squares re- gression model that addresses these problems. Casting Eqn. (7) as a least squares problem and introducing a weighting function f (Xi j ) into the cost function gives us the model\\nV\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n\\n\\nFigure 1: Weighting function f with \\u03b1 = 3/4.\\nThe performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments. We found that \\u03b1 = 3/4 gives a mod- est improvement over a linear version with \\u03b1 = 1. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a sim- ilar fractional power scaling was found to give the best performance in (Mikolov et al., 2013a).\\n\\nRelationship to Other Models\\nBecause all unsupervised methods for learning word vectors are ultimately based on the occur- rence statistics of a corpus, there should be com- monalities between the models. Nevertheless, cer- tain models remain somewhat opaque in this re- gard, particularly the recent window-based meth- ods like skip-gram and ivLBL. Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn. (8).\\nThe starting point for the skip-gram or ivLBL methods is a model Qi j for the probability that\\n\\nJ =\\ni, j=1\\nf .Xi j\\n\\u03a3 .wT w\\u02dc j\\n+ bi\\n+ b\\u02dc j\\n\\u2212 log Xi j\\n2 ,\\n(8)\\nword j appears in the context of word i. For con- creteness, let us assume that Qi j is a softmax,\\nexp(wT w\\u02dc j )\\n\\nwhere V is the size of the vocabulary. The weight-\\nQ\\t= \\ti\\t.\\t(10)\\n\\ning function should obey the following properties:\\ni j\\tV\\nk =1\\nexp(wT w\\u02dc k )\\n\\nf (0) = 0. If f is viewed as a continuous function, it should vanish as x \\u2192 0 fast\\nenough that the limx\\u21920 f (x) log2 x is finite.\\nf (x) should be non-decreasing so that rare\\nco-occurrences are not overweighted.\\nf (x) should be relatively small for large val-\\nMost of the details of these models are irrelevant for our purposes, aside from the the fact that they attempt to maximize the log probability as a con- text window scans over the corpus. Training pro- ceeds in an on-line, stochastic fashion, but the im- plied global objective function can be written as,\\n\\nues of x, so that frequent co-occurrences are not overweighted.\\nJ = \\u2212\\ni \\u2208corpus j \\u2208context(i)\\nlog Qi j .\\t(11)\\n\\nOf course a large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as,\\nEvaluating the normalization factor of the soft- max for each term in this sum is costly. To al-  low for efficient training, the skip-gram and ivLBL\\n\\nf (x) =\\n(x/xmax)\\u03b1\\tif x < xmax\\n1\\totherwise .\\n(9)\\nmodels introduce approximations to Qi j . How- ever, the sum in Eqn. (11) can be evaluated much\\n\\nmore efficiently if we first group together those terms that have the same values for i and j,\\nJ = \\u2212 . . Xi j log Qi j ,\\t(12)\\nsquared error of the logarithms of P\\u02c6 and Q\\u02c6 instead,\\nJ\\u02c6   =   . X  . log P\\u02c6    \\u2212 log Q\\u02c6   \\u03a32\\n=   . Xi .wT w\\u02dc j \\u2212 log Xi j \\u03a32  .\\t(15)\\n\\ni=1  j=1\\n\\nwhere we have used the fact that the number of like terms is given by the co-occurrence matrix X . Recalling  our  notation  for Xi  =\\tk Xik and\\nPi j = Xi j /Xi , we can rewrite J as,\\nJ = \\u2212 . Xi . Pi j log Qi j = . Xi H (Pi , Qi ) ,\\ni, j\\n\\nFinally, we observe that while the weighting factor Xi is preordained by the on-line training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal. In fact, Mikolov et al. (2013a) observe that performance can be increased by filtering the data so as to re- duce the effective value of the weighting factor for\\n\\ni=1\\nj=1\\ni=1\\n(13)\\nfrequent words. With this in mind, we introduce\\n\\nwhere H (P , Q ) is the cross entropy of the dis-\\na more general weighting function, which we are\\n\\ni\\ti\\tfree to take to depend on the context word as well.\\n\\ntributions Pi and Qi , which we define in analogy to Xi . As a weighted sum of cross-entropy error, this objective bears some formal resemblance to the weighted least squares objective of Eqn. (8). In fact, it is possible to optimize Eqn. (13) directly as opposed to the on-line training methods used in the skip-gram and ivLBL models. One could inter- pret this objective as a \\u201cglobal skip-gram\\u201d model, and it might be interesting to investigate further. On the other hand, Eqn. (13) exhibits a number of undesirable properties that ought to be addressed before adopting it as a model for learning word vectors.\\nTo begin, cross entropy error is just one among many possible distance measures between prob- ability distributions, and it has the unfortunate property that distributions with long tails are of- ten modeled poorly with too much weight given to the unlikely events. Furthermore, for the mea- sure to be bounded it requires that the model dis- tribution Q be properly normalized. This presents a computational bottleneck owing to the sum over the whole vocabulary in Eqn. (10), and it would be desirable to consider a different distance measure that did not require this property of Q. A natural choice would be a least squares objective in which normalization factors in Q and P are discarded,\\nJ\\u02c6 =\\tXi  P\\u02c6i j  \\u2212 Q\\u02c6i j   2\\t(14)\\ni, j\\nThe result is,\\nJ\\u02c6 =\\tf (Xi j )  wT w\\u02dc j \\u2212 log Xi j  2 ,\\t(16)\\ni, j\\n\\nwhich is equivalent1 to the cost  function  of  Eqn. (8), which we derived previously.\\nComplexity of the model\\nAs can be seen from Eqn. (8) and the explicit form of the weighting function f (X ), the computational complexity of the model depends on the number of nonzero elements in the matrix X . As this num- ber is always less than the total number of en- tries of the matrix, the model scales no worse than ( V 2). At first glance this might seem like a sub- stantial improvement over the shallow window- based approaches, which scale with the corpus size, C . However, typical vocabularies have hun- dreds of thousands of words, so that V 2 can be in the hundreds of billions, which is actually much larger than most corpora. For this reason it is im- portant to determine whether a tighter bound can be placed on the number of nonzero elements of\\nX .\\nIn order to make any concrete statements about the number of nonzero elements in X , it is neces- sary to make some assumptions about the distribu- tion of word co-occurrences. In particular, we will assume that the number of co-occurrences of word i with word j, Xi j , can be modeled as a power-law\\n\\nwhere  P\\u02c6i j   =  Xi j  and Q\\u02c6i j   =  exp(wT w\\u02dc j )  are the unnormalized distributions. At this stage another\\nfunction of the frequency rank of that word pair,\\nri j :\\nk\\n\\nproblem emerges, namely that Xi j often takes very\\nlarge values, which can complicate the optimiza-\\t \\t\\nXi j =\\ni j\\n)\\u03b1 .\\t(17)\\n\\ntion.\\tAn  effective  remedy  is  to minimize the\\t1We could also include bias terms in Eqn. (16).\\n\\nThe total number of words in the corpus is pro- portional to the sum over all elements of the co- occurrence matrix X ,\\n| X |\\ni j\\t\\u03b1\\t| X |,\\u03b1\\ni j\\tr =1\\n\\nwhere we have rewritten the last sum in terms of\\nTable 2: Results on the word analogy task, given as percent accuracy. Underlined scores are best within groups of similarly-sized models; bold scores are best overall. HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al., 2013);   skip-gram  (SG)  and  CBOW  results are\\nfrom (Mikolov  et al.,  2013a,b);  we trained SG\\u2020\\n\\nthe generalized harmonic number H\\n\\nn, m\\n. The up-\\nand CBOW\\u2020 using the word2vec tool3. See text\\nfor details and a description of the SVD models.\\n\\nper limit of the sum, X , is the maximum fre- quency rank, which coincides with the number of nonzero elements in the matrix X . This number is also equal to the maximum value of r in Eqn. (17) such that Xi j  1, i.e.,  X   = k1/\\u03b1. Therefore we can write Eqn. (18) as,\\n|C | \\u223c | X |\\u03b1 H| X |,\\u03b1 .\\t(19) We are interested in how X is related to C when\\nboth numbers are large; therefore we are free to expand the right hand side of the equation for large X . For this purpose we use the expansion of gen- eralized harmonic numbers (Apostol, 1976),\\n\\n\\nHx, s\\nx1\\u2212s\\n= 1 \\u2212 s\\n+ \\u03b6 (s) + O (x\\u2212s\\n\\n)   if  s > 0, s \\u00c7 1 ,\\n(20)\\n\\ngiving,\\nC\\t | X |\\n1 \\u2212 \\u03b1\\n\\n+ \\u03b6 (\\u03b1) | X |\\u03b1 + O (1) ,\\t(21)\\n\\nwhere \\u03b6 (s) is the Riemann zeta function. In the limit that X is large, only one of the two terms on the right hand side of Eqn. (21) will be relevant, and which term that is depends on whether \\u03b1 > 1,\\ndataset for NER (Tjong Kim Sang and De Meul- der, 2003).\\nWord analogies.  The word analogy task con-\\nsists of questions like, \\u201ca is to b as c is to  ?\\u201d  The dataset contains 19,544 such questions, di-\\n\\nO (|C |)\\tif \\u03b1 < 1,\\nO (|C |1/\\u03b1 )\\tif \\u03b1 > 1.\\n(22)\\nvided into a semantic subset and a syntactic sub- set. The semantic questions are typically analogies\\n\\nFor the corpora studied in this article, we observe that  Xi j  is well-modeled by Eqn. (17) with  \\u03b1 =\\n1.25.  In this case we have that   X   =     ( C 0.8).\\nTherefore we conclude that the complexity of the model is much better than the worst case (V 2), and in fact it does somewhat better than the on-line\\nwindow-based methods which scale like O (|C |).\\nExperiments\\nEvaluation methods\\nWe conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark\\nabout people or places, like \\u201cAthens is to Greece as Berlin is to ?\\u201d. The syntactic questions are typically analogies about verb tenses or forms of adjectives, for example \\u201cdance is to dancing as fly is to ?\\u201d. To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match. We answer the question \\u201ca is to b as c is to ?\\u201d by finding the word d whose repre-\\nsentation wd is closest to wb wa + wc according\\nto the cosine similarity.4\\n\\n\\n2http://lebret.ch/words/\\n3http://code.google.com/p/word2vec/\\n4Levy et al. (2014) introduce a multiplicative analogy evaluation, 3COSMUL, and report an accuracy of 68.24% on\\n\\n80\\t70\\t70\\n\\n70\\t65\\t65\\n\\n60\\t60\\t60\\n\\n50\\t55\\t55\\n\\n40\\t50\\t50\\n\\n30\\t45\\t45\\n\\n\\n20\\n0\\t100\\t200\\t300\\t400\\t500\\t600\\nVector Dimension\\n\\nSymmetric context\\n40\\n2\\t4\\t6\\t8\\t10\\nWindow Size\\n\\nSymmetric context\\n40\\n2\\t4\\t6\\t8\\t10\\nWindow Size\\n\\nAsymmetric context\\n\\n\\nFigure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100.\\n\\nWord similarity. While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013).\\nNamed entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collec- tion of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train mod- els on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7\\nFormal Run test set. We adopt the BIO2 annota- tion standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNLL- 2003 training dataset. In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features. With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRFjoin model of (Wang and Manning, 2013).\\nCorpora and training details\\nWe trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion to- kens; a 2014 Wikipedia dump with 1.6 billion to- kens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which\\n\\nthe analogy task. This number is evaluated on a subset of the dataset so it is not included in Table 2. 3COSMUL performed worse than cosine similarity in almost all of our experiments.\\nhas 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl5. We tokenize and lowercase each corpus with the Stanford to- kenizer, build a vocabulary of the 400,000 most frequent words6, and then construct a matrix of co- occurrence counts X . In constructing X , we must choose how large the context window should be and whether to distinguish left context from right context. We explore the effect of these choices be- low. In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count. This is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words\\u2019 relationship to one another.\\nFor all our experiments, we set xmax = 100,\\n\\u03b1 = 3/4, and train the model using AdaGrad (Duchi et al., 2011), stochastically sampling non- zero elements from X , with initial learning rate of\\n0.05. We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4.6 for more details about the convergence rate). Unless otherwise noted, we use a context of ten words to the left and ten words to the right.\\nThe model generates two sets of word vectors, W  and W\\u02dc .  When  X  is symmetric, W  and W\\u02dc  are equivalent and differ only as a result of their ran-\\ndom initializations; the two sets of vectors should perform equivalently. On the other hand, there is evidence that for certain types of neural networks, training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results (Ciresan et al., 2012).  With this in mind, we choose to use\\n\\n\\n5To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 bil- lion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.\\n6For the model trained on Common Crawl data, we use a larger vocabulary of about 2 million words.\\n\\nthe sum W + W\\u02dc  as our word vectors. Doing so typ- ically gives a small boost in performance, with the biggest increase in the semantic analogy task.\\nWe compare with the published results of a va- riety of state-of-the-art models, as well as with our own results produced using the word2vec tool and with several baselines using SVDs. With\\nword2vec, we train the skip-gram (SG\\u2020) and continuous bag-of-words (CBOW\\u2020) models on the 6 billion token corpus (Wikipedia 2014 + Giga-\\nword 5) with a vocabulary of the top 400,000 most frequent words and a context window size of 10. We used 10 negative samples, which we show in Section 4.6 to be a good choice for this corpus.\\nFor the SVD baselines, we generate a truncated matrix Xtrunc which retains the information of how frequently each word occurs with only the top 10,000 most frequent words. This step is typi-  cal of many matrix-factorization-based methods as the extra columns can contribute a disproportion- ate number of zero entries and the methods are otherwise computationally expensive.\\nThe singular vectors of this matrix constitute the baseline \\u201cSVD\\u201d. We also evaluate two related baselines: \\u201cSVD-S\\u201d in which we take the SVD of Xtrunc, and \\u201cSVD-L\\u201d in which we take the SVD  of log(1+ Xtrunc). Both methods help compress the\\notherwise large range of values in X .7\\nResults\\nWe present results on the word analogy task in Ta- ble 2. The GloVe model performs significantly better than the other baselines, often with smaller vector sizes and smaller corpora. Our results us- ing the word2vec tool are somewhat better than most of the previously published results. This is due to a number of factors, including our choice to use negative sampling (which typically works bet- ter than the hierarchical softmax), the number of negative samples, and the choice of the corpus.\\nWe demonstrate that the model can easily be trained on a large 42 billion token corpus, with a substantial corresponding performance boost. We note that increasing the corpus size does not guar- antee improved results for other models, as can be seen by the decreased  performance  of the SVD-\\n\\n\\n7We also investigated several other weighting schemes for transforming X ; what we report here performed best. Many weighting schemes like PPMI destroy the sparsity of X and therefore cannot feasibly be used with large vocabularies.\\n\\nTable 3: Spearman rank correlation on word simi- larity tasks. All vectors are 300-dimensional. The\\nCBOW\\u2217 vectors are from the word2vec website\\nand differ in that they contain phrase vectors.\\n\\nModel\\nSize\\nWS353\\nMC\\nRG\\nSCWS\\nRW\\nSVD\\n6B\\n35.3\\n35.1\\n42.5\\n38.3\\n25.6\\nSVD-S\\n6B\\n56.5\\n71.5\\n71.0\\n53.6\\n34.7\\nSVD-L\\n6B\\n65.7\\n72.7\\n75.1\\n56.5\\n37.0\\nCBOW\\u2020\\n6B\\n57.2\\n65.6\\n68.2\\n57.0\\n32.5\\nSG\\u2020\\n6B\\n62.8\\n65.2\\n69.7\\n58.1\\n37.2\\nGloVe\\n6B\\n65.8\\n72.7\\n77.8\\n53.9\\n38.1\\nSVD-L\\n42B\\n74.0\\n76.4\\n74.1\\n58.3\\n39.9\\nGloVe\\n42B\\n75.9\\n83.6\\n82.9\\n59.6\\n47.8\\nCBOW\\u2217\\n100B\\n68.4\\n79.6\\n75.4\\n59.4\\n45.5\\n\\n\\nL model on this larger corpus. The fact that this basic SVD model does not scale well to large cor- pora lends further evidence to the necessity of the type of weighting scheme proposed in our model. Table 3 shows results on five different word similarity datasets. A similarity score is obtained from the word vectors by first normalizing each feature across the vocabulary and then calculat- ing the cosine similarity. We compute Spearman\\u2019s\\nthe human judgments. CBOW\\u2217 denotes the vec- rank correlation coefficient between this score and tors available on the word2vec website that are\\ntrained with word and phrase vectors on 100B words of news data. GloVe outperforms it while using a corpus less than half the size.\\nTable 4 shows results on the NER task with the CRF-based model. The L-BFGS training termi- nates when no improvement has been achieved on the dev set for 25 iterations. Otherwise all config- urations are identical to those used by Wang and Manning (2013). The model labeled Discrete is the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vec- tor features. In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vec tool8. The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first\\n\\nWith smaller vocabularies, these information-theoretic trans-\\t \\t\\nformations do indeed work well on word similarity measures,\\t\\t8We use the same parameters as above, except in this case but they perform very poorly on the word analogy task.\\twe found 5 negative samples to work slightly better than 10.\\n\\nTable 4: F1 score on NER task with 50d vectors. Discrete is the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for details.\\n\\n\\n85\\n\\n80\\n\\n75\\n\\n70\\n\\n65\\n\\n60\\n\\n55\\n\\n50\\n\\nWiki2010 1B tokens\\nSemantic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWiki2014 1.6B tokens\\nSyntactic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGigaword5 4.3B tokens\\nOverall\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGigaword5 + Wiki2014 6B tokens\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCommon Crawl 42B tokens\\n\\n\\n\\n\\n\\n\\nshown for neural vectors in (Turian et al., 2010).\\nFigure 3: Accuracy on the analogy task for 300- dimensional vectors trained on different corpora.\\nentries are updated to assimilate new knowledge, whereas Gigaword is a fixed news repository with outdated and possibly incorrect information.\\n\\nModel Analysis: Vector Length and Context Size\\nIn Fig. 2, we show the results of experiments that vary vector length and context window. A context window that extends to the left and right of a tar- get word will be called symmetric, and one which extends only to the left will be called asymmet- ric. In (a), we observe diminishing returns for vec- tors larger than about 200 dimensions. In (b) and (c), we examine the effect of varying the window size for symmetric and asymmetric context win- dows. Performance is better on the syntactic sub- task for small and asymmetric context windows, which aligns with the intuition that syntactic infor- mation is mostly drawn from the immediate con- text and can depend strongly on word order. Se- mantic information, on the other hand, is more fre- quently non-local, and more of it is captured with larger window sizes.\\nModel Analysis: Corpus Size\\nIn Fig. 3, we show performance on the word anal- ogy task for 300-dimensional vectors trained on different corpora. On the syntactic subtask, there is a monotonic increase in performance as the cor- pus size increases. This is to be expected since larger corpora typically produce better statistics. Interestingly, the same trend is not true for the se- mantic subtask, where the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus. This is likely due to the large number of city- and country- based analogies in the analogy dataset and the fact that Wikipedia has fairly comprehensive articles for most such locations. Moreover, Wikipedia\\u2019s\\nModel Analysis: Run-time\\nThe total run-time is split between populating X and training the model. The former depends on many factors, including window size, vocabulary size, and corpus size. Though we did not do so, this step could easily be parallelized across mul- tiple machines (see, e.g., Lebret and Collobert (2014) for some benchmarks). Using a single thread of a dual 2.1GHz Intel Xeon E5-2658 ma- chine, populating X with a 10 word symmetric context window, a 400,000 word vocabulary, and a 6 billion token corpus takes about 85 minutes. Given X , the time it takes to train the model de- pends on the vector size and the number of itera- tions. For 300-dimensional vectors with the above settings (and using all 32 cores of the above ma- chine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve.\\nModel Analysis: Comparison with\\nword2vec\\nA rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on per- formance. We control for the main sources of vari- ation that we identified in Sections 4.4 and 4.5 by setting the vector length, context window size, cor- pus, and vocabulary size to the configuration men- tioned in the previous subsection.\\nThe most important remaining variable to con- trol for is training time. For GloVe, the rele-   vant parameter is the number of training iterations. For word2vec, the obvious choice would be the number of training epochs. Unfortunately, the code is currently designed for only a single epoch:\\n\\nTraining Time (hrs)\\n1\\t2\\t3\\t4\\t5\\t6\\n72\\nTraining Time (hrs)\\n3\\t6\\t9\\t12\\t15\\t18\\t21\\t24\\n72\\n\\n\\n70\\t70\\n\\n68\\t68\\n\\n66\\t66\\n\\n64\\t64\\n\\n62\\t62\\n\\n\\n60\\tIterations (GloVe)\\n\\n1357 10   15   20    25  30\\t40\\t50\\nNegative Samples (CBOW)\\nGloVe vs CBOW\\n60\\tIterations (GloVe)\\n\\n1  2  3  4  5   6    7     10  12\\t15\\t20\\nNegative Samples (Skip-Gram)\\nGloVe vs Skip-Gram\\n\\nFigure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by the number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram (b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 + Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.\\n\\nit specifies a learning schedule specific to a single pass through the data, making a modification for multiple passes a non-trivial task. Another choice is to vary the number of negative samples. Adding negative samples effectively increases the number of training words seen by the model, so in some ways it is analogous to extra epochs.\\nWe set any unspecified parameters to their de- fault values, assuming that they are close to opti- mal, though we acknowledge that this simplifica- tion should be relaxed in a more thorough analysis. In Fig. 4, we plot the overall performance on the analogy task as a function of training time. The two x-axes at the bottom indicate the corre- sponding number of training iterations for GloVe and negative samples for word2vec. We note that word2vec\\u2019s performance actually decreases if the number of negative samples increases be- yond about 10. Presumably this is because the negative sampling method does not approximate\\nthe target probability distribution well.9\\nFor the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec. It achieves better results faster, and also obtains the best results irrespective of speed.\\nConclusion\\nRecently, considerable attention has been focused on the question of whether distributional word representations are best learned from count-based\\n\\n\\n9In contrast, noise-contrastive estimation is an approxi- mation which improves with more negative samples. In Ta- ble 1 of (Mnih et al., 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples.\\nmethods or from prediction-based methods. Cur- rently, prediction-based models garner substantial support; for example, Baroni et al. (2014) argue that these models perform better across a range of tasks. In this work we argue that the two classes of methods are not dramatically different at a fun- damental level since they both probe the under- lying co-occurrence statistics of the corpus, but the efficiency with which the count-based meth- ods capture global statistics can be advantageous. We construct a model that utilizes this main ben- efit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec. The result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.\\nAcknowledgments\\nWe thank the anonymous reviewers for their valu- able comments. Stanford University gratefully acknowledges the support of the Defense Threat Reduction Agency (DTRA) under Air Force Re- search Laboratory (AFRL) contract no. FA8650- 10-C-7020 and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under AFRL contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations ex- pressed in this material are those of the authors and do not necessarily reflect the view of the DTRA, AFRL, DEFT, or the US government.\\n\\nReferences\\nTom M. Apostol. 1976. Introduction to Analytic Number Theory. Introduction to Analytic Num- ber Theory.\\nMarco   Baroni,   Georgiana   Dinu,   and   Germa\\u00b4n Kruszewski. 2014. Don\\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL.\\nYoshua Bengio. 2009. Learning deep architectures for AI. Foundations and Trends in Machine Learning.\\nYoshua  Bengio,  Re\\u00b4jean  Ducharme,  Pascal  Vin- cent, and Christian Janvin. 2003. A neural prob- abilistic language model. JMLR, 3:1137\\u20131155.\\nJohn A. Bullinaria and Joseph P. Levy. 2007. Ex- tracting semantic representations from word co- occurrence statistics: A computational study. Behavior Research Methods, 39(3):510\\u2013526.\\nDan C. Ciresan, Alessandro Giusti, Luca M. Gam- bardella, and Ju\\u00a8rgen Schmidhuber. 2012.  Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, pages 2852\\u20132860.\\nRonan Collobert and Jason Weston. 2008. A uni- fied architecture for natural language process- ing: deep neural networks with multitask learn- ing. In Proceedings of ICML, pages 160\\u2013167.\\nRonan  Collobert,  Jason  Weston,  Le\\u00b4on  Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Al- most) from Scratch. JMLR, 12:2493\\u20132537.\\nScott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41.\\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learn- ing and stochastic optimization. JMLR, 12.\\nLev Finkelstein, Evgenly Gabrilovich, Yossi Ma- tias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in con- text:  The concept revisited.  In Proceedings   of the 10th international conference on World Wide Web, pages 406\\u2013414. ACM.\\nEric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving\\nWord Representations via Global Context and Multiple Word Prototypes. In ACL.\\nRe\\u00b4mi  Lebret  and  Ronan  Collobert.  2014.   Word embeddings through Hellinger PCA. In EACL.\\nOmer Levy, Yoav Goldberg, and Israel Ramat- Gan. 2014. Linguistic regularities in sparse and explicit word representations. CoNLL-2014.\\nKevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, In- strumentation, and Computers, 28:203\\u2013208.\\nMinh-Thang Luong, Richard Socher, and Christo- pher D Manning. 2013. Better word represen- tations with recursive neural networks for mor- phology. CoNLL-2013.\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In ICLR Work- shop Papers.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\\u20133119.\\nTomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in con- tinuous space word representations. In HLT- NAACL.\\nGeorge A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and cognitive processes, 6(1):1\\u201328.\\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In NIPS.\\nDouglas L. T. Rohde, Laura  M.  Gonnerman, and David C. Plaut. 2006. An  improved  model of semantic similarity based on lexical co-occurence. Communications of the ACM, 8:627\\u2013633.\\nHerbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Com- munications of the ACM, 8(10):627\\u2013633.\\nFabrizio Sebastiani. 2002. Machine learning in au- tomated text categorization. ACM Computing Surveys, 34:1\\u201347.\\nRichard Socher, John Bauer, Christopher D. Man- ning, and Andrew Y. Ng. 2013. Parsing With Compositional Vector Grammars. In ACL.\\n\\nStefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. 2003. Quanti- tative evaluation of passage retrieval algorithms for question answering. In Proceedings of the SIGIR Conference on Research and Develop- ment in Informaion Retrieval.\\nErik F. Tjong Kim Sang and  Fien  De  Meul-  der. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named en- tity recognition. In CoNLL-2003.\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and gen- eral method for semi-supervised learning. In Proceedings of ACL, pages 384\\u2013394.\\nMengqiu Wang and Christopher D. Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In Proceedings of the 6th International Joint Conference on Natural Lan- guage Processing (IJCNLP).\\n\""