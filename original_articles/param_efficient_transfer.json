"Parameter-Efficient Transfer Learning for NLPParameter-Efficient Transfer Learning for NLPNeil Houlsby 1 Andrei Giurgiu 1 * Stanis\u0142aw Jastrze\u0327bski 2 * Bruna Morrone 1 Quentin de Laroussilhe 1Andrea Gesmundo 1 Mona Attariyan 1 Sylvain Gelly 1AbstractFine-tuning large pre-trained models is an effec-tive transfer mechanism in NLP. However, in thepresence of many downstream tasks, fine-tuningis parameter inefficient: an entire new model isrequired for every task. As an alternative, wepropose transfer with adapter modules. Adaptermodules yield a compact and extensible model;they add only a few trainable parameters per task,and new tasks can be added without revisitingprevious ones. The parameters of the originalnetwork remain fixed, yielding a high degree ofparameter sharing. To demonstrate adapter\u2019s ef-fectiveness, we transfer the recently proposedBERT Transformer model to 26 diverse text clas-sification tasks, including the GLUE benchmark.Adapters attain near state-of-the-art performance,whilst adding only a few parameters per task. OnGLUE, we attain within 0.4% of the performanceof full fine-tuning, adding only 3.6% parametersper task. By contrast, fine-tuning trains 100% ofthe parameters per task.11. IntroductionTransfer from pre-trained models yields strong performanceon many NLP tasks (Dai & Le, 2015; Howard & Ruder,2018; Radford et al., 2018). BERT, a Transformer networktrained on large text corpora with an unsupervised loss,attained state-of-the-art performance on text classificationand extractive question answering (Devlin et al., 2018).In this paper we address the online setting, where tasksarrive in a stream. The goal is to build a system that per-forms well on all of them, but without training an entire newmodel for every new task. A high degree of sharing between*Equal contribution 1Google Research 2Jagiellonian University.Correspondence to: Neil Houlsby <neilhoulsby@google.com>.Proceedings of the 36 th International Conference on MachineLearning, Long Beach, California, PMLR 97, 2019. Copyright2019 by the author(s).1Code at https://github.com/google-research/adapter-bert105 106 107 108 109Num trainable parameters / task\u221225\u221220\u221215\u221210\u2212505Accuracy delta (%)Adapters (ours)Fine-tune top layersFigure 1. Trade-off between accuracy and number of trained task-specific parameters, for adapter tuning and fine-tuning. The y-axisis normalized by the performance of full fine-tuning, details inSection 3. The curves show the 20th, 50th, and 80th performancepercentiles across nine tasks from the GLUE benchmark. Adapter-based tuning attains a similar performance to full fine-tuning withtwo orders of magnitude fewer trained parameters.tasks is particularly useful for applications such as cloudservices, where models need to be trained to solve manytasks that arrive from customers in sequence. For this, wepropose a transfer learning strategy that yields compact andextensible downstream models. Compact models are thosethat solve many tasks using a small number of additionalparameters per task. Extensible models can be trained in-crementally to solve new tasks, without forgetting previousones. Our method yields a such models without sacrificingperformance.The two most common transfer learning techniques in NLPare feature-based transfer and fine-tuning. Instead, wepresent an alternative transfer method based on adaptermodules (Rebuffi et al., 2017). Features-based transfer in-volves pre-training real-valued embeddings vectors. Theseembeddings may be at the word (Mikolov et al., 2013), sen-tence (Cer et al., 2019), or paragraph level (Le & Mikolov,2014). The embeddings are then fed to custom downstreammodels. Fine-tuning involves copying the weights from apre-trained network and tuning them on the downstreamtask. Recent work shows that fine-tuning often enjoys betterarXiv:1902.00751v2  [cs.LG]  13 Jun 2019https://github.com/google-research/adapter-berthttps://github.com/google-research/adapter-bertParameter-Efficient Transfer Learning for NLPperformance than feature-based transfer (Howard & Ruder,2018).Both feature-based transfer and fine-tuning require a newset of weights for each task. Fine-tuning is more parameterefficient if the lower layers of a network are shared betweentasks. However, our proposed adapter tuning method is evenmore parameter efficient. Figure 1 demonstrates this trade-off. The x-axis shows the number of parameters trained pertask; this corresponds to the marginal increase in the modelsize required to solve each additional task. Adapter-basedtuning requires training two orders of magnitude fewer pa-rameters to fine-tuning, while attaining similar performance.Adapters are new modules added between layers of apre-trained network. Adapter-based tuning differs fromfeature-based transfer and fine-tuning in the following way.Consider a function (neural network) with parameters w:\u03c6w(x). Feature-based transfer composes \u03c6w with a newfunction, \u03c7v, to yield \u03c7v(\u03c6w(x)). Only the new, task-specific, parameters, v, are then trained. Fine-tuning in-volves adjusting the original parameters, w, for each newtask, limiting compactness. For adapter tuning, a newfunction, \u03c8w,v(x), is defined, where parameters w arecopied over from pre-training. The initial parameters v0are set such that the new function resembles the original:\u03c8w,v0(x) \u2248 \u03c6w(x). During training, only v are tuned.For deep networks, defining \u03c8w,v typically involves addingnew layers to the original network, \u03c6w. If one chooses|v| \ufffd |w|, the resulting model requires \u223c |w| parametersfor many tasks. Since w is fixed, the model can be extendedto new tasks without affecting previous ones.Adapter-based tuning relates to multi-task and continuallearning. Multi-task learning also results in compact models.However, multi-task learning requires simultaneous accessto all tasks, which adapter-based tuning does not require.Continual learning systems aim to learn from an endlessstream of tasks. This paradigm is challenging because net-works forget previous tasks after re-training (McCloskey& Cohen, 1989; French, 1999). Adapters differ in that thetasks do not interact and the shared parameters are frozen.This means that the model has perfect memory of previoustasks using a small number of task-specific parameters.We demonstrate on a large and diverse set of text classifica-tion tasks that adapters yield parameter-efficient tuning forNLP. The key innovation is to design an effective adaptermodule and its integration with the base model. We proposea simple yet effective, bottleneck architecture. On the GLUEbenchmark, our strategy almost matches the performance ofthe fully fine-tuned BERT, but uses only 3% task-specificparameters, while fine-tuning uses 100% task-specific pa-rameters. We observe similar results on a further 17 publictext datasets, and SQuAD extractive question answering. Insummary, adapter-based tuning yields a single, extensible,model that attains near state-of-the-art performance in textclassification.2. Adapter tuning for NLPWe present a strategy for tuning a large text model on severaldownstream tasks. Our strategy has three key properties:(i) it attains good performance, (ii) it permits training ontasks sequentially, that is, it does not require simultaneousaccess to all datasets, and (iii) it adds only a small numberof additional parameters per task. These properties areespecially useful in the context of cloud services, wheremany models need to be trained on a series of downstreamtasks, so a high degree of sharing is desirable.To achieve these properties, we propose a new bottleneckadapter module. Tuning with adapter modules involvesadding a small number of new parameters to a model, whichare trained on the downstream task (Rebuffi et al., 2017).When performing vanilla fine-tuning of deep networks, amodification is made to the top layer of the network. This isrequired because the label spaces and losses for the upstreamand downstream tasks differ. Adapter modules performmore general architectural modifications to re-purpose a pre-trained network for a downstream task. In particular, theadapter tuning strategy involves injecting new layers intothe original network. The weights of the original networkare untouched, whilst the new adapter layers are initializedat random. In standard fine-tuning, the new top-layer andthe original weights are co-trained. In contrast, in adapter-tuning, the parameters of the original network are frozenand therefore may be shared by many tasks.Adapter modules have two main features: a small numberof parameters, and a near-identity initialization. The adaptermodules need to be small compared to the layers of the orig-inal network. This means that the total model size growsrelatively slowly when more tasks are added. A near-identityinitialization is required for stable training of the adaptedmodel; we investigate this empirically in Section 3.6. Byinitializing the adapters to a near-identity function, originalnetwork is unaffected when training starts. During training,the adapters may then be activated to change the distributionof activations throughout the network. The adapter mod-ules may also be ignored if not required; in Section 3.6 weobserve that some adapters have more influence on the net-work than others. We also observe that if the initializationdeviates too far from the identity function, the model mayfail to train.2.1. Instantiation for Transformer NetworksWe instantiate adapter-based tuning for text Transformers.These models attain state-of-the-art performance in manyNLP tasks, including translation, extractive QA, and textParameter-Efficient Transfer Learning for NLPMulti-headed attentionLayer Norm+Adapter2x Feed-forward layerLayer Norm+AdapterFeed-forward layerTransformerLayerNonlinearityFeedforward up-projectFeedforward down-projectAdapter Layer +Figure 2. Architecture of the adapter module and its integrationwith the Transformer. Left: We add the adapter module twiceto each Transformer layer: after the projection following multi-headed attention and after the two feed-forward layers. Right: Theadapter consists of a bottleneck which contains few parameters rel-ative to the attention and feedforward layers in the original model.The adapter also contains a skip-connection. During adapter tun-ing, the green layers are trained on the downstream data, thisincludes the adapter, the layer normalization parameters, and thefinal classification layer (not shown in the figure).classification problems (Vaswani et al., 2017; Radford et al.,2018; Devlin et al., 2018). We consider the standard Trans-former architecture, as proposed in Vaswani et al. (2017).Adapter modules present many architectural choices. Weprovide a simple design that attains good performance. Weexperimented with a number of more complex designs, seeSection 3.6, but we found the following strategy performedas well as any other that we tested, across many datasets.Figure 2 shows our adapter architecture, and its applicationit to the Transformer. Each layer of the Transformer containstwo primary sub-layers: an attention layer and a feedforwardlayer. Both layers are followed immediately by a projectionthat maps the features size back to the size of layer\u2019s input.A skip-connection is applied across each of the sub-layers.The output of each sub-layer is fed into layer normalization.We insert two serial adapters after each of these sub-layers.The adapter is always applied directly to the output of thesub-layer, after the projection back to the input size, butbefore adding the skip connection back. The output ofthe adapter is then passed directly into the following layernormalization.To limit the number of parameters, we propose a bottle-neck architecture. The adapters first project the originald-dimensional features into a smaller dimension, m, applya nonlinearity, then project back to d dimensions. The totalnumber of parameters added per layer, including biases, is2md + d + m. By setting m \ufffd d, we limit the numberof parameters added per task; in practice, we use around0.5 \u2212 8% of the parameters of the original model. Thebottleneck dimension, m, provides a simple means to trade-off performance with parameter efficiency. The adaptermodule itself has a skip-connection internally. With theskip-connection, if the parameters of the projection layersare initialized to near-zero, the module is initialized to anapproximate identity function.Alongside the layers in the adapter module, we also trainnew layer normalization parameters per task. This tech-nique, similar to conditional batch normalization (De Vrieset al., 2017), FiLM (Perez et al., 2018), and self-modulation (Chen et al., 2019), also yields parameter-efficient adaptation of a network; with only 2d parametersper layer. However, training the layer normalization pa-rameters alone is insufficient for good performance, seeSection 3.4.3. ExperimentsWe show that adapters achieve parameter efficient transferfor text tasks. On the GLUE benchmark (Wang et al., 2018),adapter tuning is within 0.4% of full fine-tuning of BERT,but it adds only 3% of the number of parameters trained byfine-tuning. We confirm this result on a further 17 publicclassification tasks and SQuAD question answering. Analy-sis shows that adapter-based tuning automatically focuseson the higher layers of the network.3.1. Experimental SettingsWe use the public, pre-trained BERT Transformer networkas our base model. To perform classification with BERT,we follow the approach in Devlin et al. (2018). The firsttoken in each sequence is a special \u201cclassification token\u201d.We attach a linear layer to the embedding of this token topredict the class label.Our training procedure also follows Devlin et al. (2018).We optimize using Adam (Kingma & Ba, 2014), whoselearning rate is increased linearly over the first 10% of thesteps, and then decayed linearly to zero. All runs are trainedon 4 Google Cloud TPUs with a batch size of 32. For eachdataset and algorithm, we run a hyperparameter sweep andselect the best model according to accuracy on the validationset. For the GLUE tasks, we report the test metrics providedby the submission website2. For the other classificationtasks we report test-set accuracy.2https://gluebenchmark.com/https://gluebenchmark.com/Parameter-Efficient Transfer Learning for NLPWe compare to fine-tuning, the current standard for transferof large pre-trained models, and the strategy successfullyused by BERT. For N tasks, full fine-tuning requires N\u00d7the number of parameters of the pre-trained model. Ourgoal is to attain performance equal to fine-tuning, but withfewer total parameters, ideally near to 1\u00d7.3.2. GLUE benchmarkWe first evaluate on GLUE.3 For these datasets, we trans-fer from the pre-trained BERTLARGE model, which con-tains 24 layers, and a total of 330M parameters, see Devlinet al. (2018) for details. We perform a small hyperparam-eter sweep for adapter tuning: We sweep learning ratesin {3 \u00b7 10\u22125, 3 \u00b7 10\u22124, 3 \u00b7 10\u22123}, and number of epochsin {3, 20}. We test both using a fixed adapter size (num-ber of units in the bottleneck), and selecting the best sizeper task from {8, 64, 256}. The adapter size is the onlyadapter-specific hyperparameter that we tune. Finally, dueto training instability, we re-run 5 times with different ran-dom seeds and select the best model on the validation set.Table 1 summarizes the results. Adapters achieve a meanGLUE score of 80.0, compared to 80.4 achieved by fullfine-tuning. The optimal adapter size varies per dataset. Forexample, 256 is chosen for MNLI, whereas for the smallestdataset, RTE, 8 is chosen. Restricting always to size 64,leads to a small decrease in average accuracy to 79.6. Tosolve all of the datasets in Table 1, fine-tuning requires 9\u00d7the total number of BERT parameters.4 In contrast, adaptersrequire only 1.3\u00d7 parameters.3.3. Additional Classification TasksTo further validate that adapters yields compact, performant,models, we test on additional, publicly available, text clas-sification tasks. This suite contains a diverse set of tasks:The number of training examples ranges from 900 to 330k,the number of classes ranges from 2 to 157, and the averagetext length ranging from 57 to 1.9k characters. Statisticsand references for all of the datasets are in the appendix.For these datasets, we use a batch size of 32. The datasetsare diverse, so we sweep a wide range of learning rates:{1 \u00b7 10\u22125, 3 \u00b7 10\u22125, 1 \u00b7 10\u22124, 3 \u00b7 10\u22123}. Due to the largenumber of datasets, we select the number of training epochsfrom the set {20, 50, 100} manually, from inspection of thevalidation set learning curves. We select the optimal valuesfor both fine-tuning and adapters; the exact values are in theappendix.3 We omit WNLI as in Devlin et al. (2018) because the nocurrent algorithm beats the baseline of predicting the majorityclass.4 We treat MNLIm and MNLImm as separate tasks with individ-ually tuned hyperparameters. However, they could be combinedinto one model, leaving 8\u00d7 overall.We test adapters sizes in {2, 4, 8, 16, 32, 64}. Since someof the datasets are small, fine-tuning the entire networkmay be sub-optimal. Therefore, we run an additional base-line: variable fine-tuning. For this, we fine-tune onlythe top n layers, and freeze the remainder. We sweepn \u2208 {1, 2, 3, 5, 7, 9, 11, 12}. In these experiments, we usethe BERTBASE model with 12 layers, therefore, variablefine-tuning subsumes full fine-tuning when n = 12.Unlike the GLUE tasks, there is no comprehensive set ofstate-of-the-art numbers for this suite of tasks. Therefore, toconfirm that our BERT-based models are competitive, wecollect our own benchmark performances. For this, we runa large-scale hyperparameter search over standard networktopologies. Specifically, we run the single-task Neural Au-toML algorithm, similar to Zoph & Le (2017); Wong et al.(2018). This algorithm searches over a space of feedfor-ward and convolutional networks, stacked on pre-trainedtext embeddings modules publicly available via TensorFlowHub5. The embeddings coming from the TensorFlow Hubmodules may be frozen or fine-tuned. The full search spaceis described in the appendix. For each task, we run AutoMLfor one week on CPUs, using 30 machines. In this timethe algorithm explores over 10k models on average per task.We select the best final model for each task according tovalidation set accuracy.The results for the AutoML benchmark (\u201cno BERT base-line\u201d), fine-tuning, variable fine-tuning, and adapter-tuningare reported in Table 2. The AutoML baseline demon-strates that the BERT models are competitive. This baselineexplores thousands of models, yet the BERT models per-form better on average. We see similar pattern of results toGLUE. The performance of adapter-tuning is close to fullfine-tuning (0.4% behind). Fine-tuning requires 17\u00d7 thenumber of parameters to BERTBASE to solve all tasks. Vari-able fine-tuning performs slightly better than fine-tuning,whilst training fewer layers. The optimal setting of variablefine-tuning results in training 52% of the network on averageper task, reducing the total to 9.9\u00d7 parameters. Adapters,however, offer a much more compact model. They intro-duce 1.14% new parameters per task, resulting in 1.19\u00d7parameters for all 17 tasks.3.4. Parameter/Performance trade-offThe adapter size controls the parameter efficiency, smalleradapters introduce fewer parameters, at a possible cost toperformance. To explore this trade-off, we consider differentadapter sizes, and compare to two baselines: (i) Fine-tuningof only the top k layers of BERTBASE. (ii) Tuning only thelayer normalization parameters. The learning rate is tunedusing the range presented in Section 3.2.5https://www.tensorflow.org/hubhttps://www.tensorflow.org/hubParameter-Efficient Transfer Learning for NLPTotal numparamsTrainedparams / taskCoLA SST MRPC STS-B QQP MNLIm MNLImm QNLI RTE TotalBERTLARGE 9.0\u00d7 100% 60.5 94.9 89.3 87.6 72.1 86.7 85.9 91.1 70.1 80.4Adapters (8-256) 1.3\u00d7 3.6% 59.5 94.0 89.5 86.9 71.8 84.9 85.1 90.7 71.5 80.0Adapters (64) 1.2\u00d7 2.1% 56.9 94.2 89.6 87.3 71.8 85.3 84.6 91.4 68.8 79.6Table 1. Results on GLUE test sets scored using the GLUE evaluation server. MRPC and QQP are evaluated using F1 score. STS-B isevaluated using Spearman\u2019s correlation coefficient. CoLA is evaluated using Matthew\u2019s Correlation. The other tasks are evaluated usingaccuracy. Adapter tuning achieves comparable overall score (80.0) to full fine-tuning (80.4) using 1.3\u00d7 parameters in total, compared to9\u00d7. Fixing the adapter size to 64 leads to a slightly decreased overall score of 79.6 and slightly smaller model.Dataset No BERTbaselineBERTBASEFine-tuneBERTBASEVariable FTBERTBASEAdapters20 newsgroups 91.1 92.8\u00b1 0.1 92.8\u00b1 0.1 91.7\u00b1 0.2Crowdflower airline 84.5 83.6\u00b1 0.3 84.0\u00b1 0.1 84.5\u00b1 0.2Crowdflower corporate messaging 91.9 92.5\u00b1 0.5 92.4\u00b1 0.6 92.9\u00b1 0.3Crowdflower disasters 84.9 85.3\u00b1 0.4 85.3\u00b1 0.4 84.1\u00b1 0.2Crowdflower economic news relevance 81.1 82.1\u00b1 0.0 78.9\u00b1 2.8 82.5\u00b1 0.3Crowdflower emotion 36.3 38.4\u00b1 0.1 37.6\u00b1 0.2 38.7\u00b1 0.1Crowdflower global warming 82.7 84.2\u00b1 0.4 81.9\u00b1 0.2 82.7\u00b1 0.3Crowdflower political audience 81.0 80.9\u00b1 0.3 80.7\u00b1 0.8 79.0\u00b1 0.5Crowdflower political bias 76.8 75.2\u00b1 0.9 76.5\u00b1 0.4 75.9\u00b1 0.3Crowdflower political message 43.8 38.9\u00b1 0.6 44.9\u00b1 0.6 44.1\u00b1 0.2Crowdflower primary emotions 33.5 36.9\u00b1 1.6 38.2\u00b1 1.0 33.9\u00b1 1.4Crowdflower progressive opinion 70.6 71.6\u00b1 0.5 75.9\u00b1 1.3 71.7\u00b1 1.1Crowdflower progressive stance 54.3 63.8\u00b1 1.0 61.5\u00b1 1.3 60.6\u00b1 1.4Crowdflower US economic performance 75.6 75.3\u00b1 0.1 76.5\u00b1 0.4 77.3\u00b1 0.1Customer complaint database 54.5 55.9\u00b1 0.1 56.4\u00b1 0.1 55.4\u00b1 0.1News aggregator dataset 95.2 96.3\u00b1 0.0 96.5\u00b1 0.0 96.2\u00b1 0.0SMS spam collection 98.5 99.3\u00b1 0.2 99.3\u00b1 0.2 95.1\u00b1 2.2Average 72.7 73.7 74.0 73.3Total number of params \u2014 17\u00d7 9.9\u00d7 1.19\u00d7Trained params/task \u2014 100% 52.9% 1.14%Table 2. Test accuracy for additional classification tasks. In these experiments we transfer from the BERTBASE model. For each taskand algorithm, the model with the best validation set accuracy is chosen. We report the mean test accuracy and s.e.m. across runs withdifferent random seeds.Figure 3 shows the parameter/performance trade-off ag-gregated over all classification tasks in each suite (GLUEand \u201cadditional\u201d). On GLUE, performance decreases dra-matically when fewer layers are fine-tuned. Some of theadditional tasks benefit from training fewer layers, so per-formance of fine-tuning decays much less. In both cases,adapters yield good performance across a range of sizes twoorders of magnitude fewer than fine-tuning.Figure 4 shows more details for two GLUE tasks: MNLImand CoLA. Tuning the top layers trains more task-specificparameters for all k > 2. When fine-tuning using a compa-rable number of task-specific parameters, the performancedecreases substantially compared to adapters. For instance,fine-tuning just the top layer yields approximately 9M train-able parameters and 77.8%\u00b1 0.1% validation accuracy onMNLIm. In contrast, adapter tuning with size 64 yields ap-proximately 2M trainable parameters and 83.7% \u00b1 0.1%validation accuracy. For comparison, full fine-tuning attains84.4%\u00b1 0.02% on MNLIm. We observe a similar trend onCoLA.As a further comparison, we tune the parameters of layernormalization alone. These layers only contain point-wiseadditions and multiplications, so introduce very few train-able parameters: 40k for BERTBASE. However this strategyperforms poorly: performance decreases by approximately3.5% on CoLA and 4% on MNLI.To summarize, adapter tuning is highly parameter-efficient,and produces a compact model with a strong performance,comparable to full fine-tuning. Training adapters with sizes0.5\u2212 5% of the original model, performance is within 1%of the competitive published results on BERTLARGE.Parameter-Efficient Transfer Learning for NLPGLUE (BERTLARGE) Additional Tasks (BERTBASE)105 106 107 108 109Num trainable parameters / task\u221225\u221220\u221215\u221210\u2212505Accuracy delta (%)AdaptersFine-tune top layers105 106 107 108Num trainable parameters / task\u22124\u22123\u22122\u221210123Accuracy delta (%)AdaptersFine-tune top layersFigure 3. Accuracy versus the number of trained parameters, aggregated across tasks. We compare adapters of different sizes (orange)with fine-tuning the top n layers, for varying n (blue). The lines and shaded areas indicate the 20th, 50th, and 80th percentiles acrosstasks. For each task and algorithm, the best model is selected for each point along the curve. For GLUE, the validation set accuracy isreported. For the additional tasks, we report the test-set accuracies. To remove the intra-task variance in scores, we normalize the scoresfor each model and task by subtracting the performance of full fine-tuning on the corresponding task.MNLIm(BERTBASE) CoLA (BERTBASE)104 105 106 107 108Num trainable parameters / task767880828486Validation accuracy (%)Layer Norm.AdaptersFine-tune top layers104 105 106 107 108Num trainable parameters / task74767880828486Validation accuracy (%)Layer Norm.AdaptersFine-tune top layersFigure 4. Validation set accuracy versus number of trained parameters for three methods: (i) Adapter tuning with an adapter sizes 2nfor n = 0 . . . 9 (orange). (ii) Fine-tuning the top k layers for k = 1 . . . 12 (blue). (iii) Tuning the layer normalization parameters only(green). Error bars indicate \u00b11 s.e.m. across three random seeds.3.5. SQuAD Extractive Question AnsweringFinally, we confirm that adapters work on tasks other thanclassification by running on SQuAD v1.1 (Rajpurkar et al.,2018). Given a question and Wikipedia paragraph, this taskrequires selecting the answer span to the question from theparagraph. Figure 5 displays the parameter/performancetrade-off of fine-tuning and adapters on the SQuAD valida-tion set. For fine-tuning, we sweep the number of trained lay-ers, learning rate in {3\u00b710\u22125, 5\u00b710\u22125, 1\u00b710\u22124}, and numberof epochs in {2, 3, 5}. For adapters, we sweep the adaptersize, learning rate in {3 \u00b7 10\u22125, 1 \u00b7 10\u22124, 3 \u00b7 10\u22124, 1 \u00b7 10\u22123},and number of epochs in {3, 10, 20}. As for classification,adapters attain performance comparable to full fine-tuning,while training many fewer parameters. Adapters of size 64(2% parameters) attain a best F1 of 90.4%, while fine-tuningattains 90.7. SQuAD performs well even with very smalladapters, those of size 2 (0.1% parameters) attain an F1 of89.9.3.6. Analysis and DiscussionWe perform an ablation to determine which adapters areinfluential. For this, we remove some trained adapters andre-evaluate the model (without re-training) on the valida-tion set. Figure 6 shows the change in performance whenremoving adapters from all continuous layer spans. Theexperiment is performed on BERTBASE with adapter size 64on MNLI and CoLA.First, we observe that removing any single layer\u2019s adaptershas only a small impact on performance. The elements onParameter-Efficient Transfer Learning for NLP105 106 107 108 109Num trainable parameters707580859095f1 scoreAdaptersFine-tune top layersFigure 5. Validation accuracy versus the number of trained param-eters for SQuAD v1.1. Error bars indicate the s.e.m. across threeseeds, using the best hyperparameters.the heatmaps\u2019 diagonals show the performances of removingadapters from single layers, where largest performance dropis 2%. In contrast, when all of the adapters are removedfrom the network, the performance drops substantially: to37% on MNLI and 69% on CoLA \u2013 scores attained bypredicting the majority class. This indicates that althougheach adapter has a small influence on the overall network,the overall effect is large.Second, Figure 6 suggests that adapters on the lower layershave a smaller impact than the higher-layers. Removingthe adapters from the layers 0\u2212 4 on MNLI barely affectsperformance. This indicates that adapters perform wellbecause they automatically prioritize higher layers. Indeed,focusing on the upper layers is a popular strategy in fine-tuning (Howard & Ruder, 2018). One intuition is that thelower layers extract lower-level features that are sharedamong tasks, while the higher layers build features that areunique to different tasks. This relates to our observation thatfor some tasks, fine-tuning only the top layers outperformsfull fine-tuning, see Table 2.Next, we investigate the robustness of the adapter modulesto the number of neurons and initialization scale. In ourmain experiments the weights in the adapter module weredrawn from a zero-mean Gaussian with standard deviation10\u22122, truncated to two standard deviations. To analyze theimpact of the initialization scale on the performance, wetest standard deviations in the interval [10\u22127, 1]. Figure 6summarizes the results. We observe that on both datasets,the performance of adapters is robust for standard deviationsbelow 10\u22122. However, when the initialization is too large,performance degrades, more substantially on CoLA.To investigate robustness of adapters to the number of neu-rons, we re-examine the experimental data from Section 3.2.We find that the quality of the model across adapter sizes isstable, and a fixed adapter size across all the tasks could beused with small detriment to performance. For each adaptersize we calculate the mean validation accuracy across theeight classification tasks by selecting the optimal learningrate and number of epochs6. For adapter sizes 8, 64, and256, the mean validation accuracies are 86.2%, 85.8% and85.7%, respectively. This message is further corroboratedby Figures 4 and 5, which show a stable performance acrossa few orders of magnitude.Finally, we tried a number of extensions to the adapter\u2019sarchitecture that did not yield a significant boost in perfor-mance. We document them here for completeness. Weexperimented with (i) adding a batch/layer normalization tothe adapter, (ii) increasing the number of layers per adapter,(iii) different activation functions, such as tanh, (iv) insertingadapters only inside the attention layer, (v) adding adaptersin parallel to the main layers, and possibly with a multi-plicative interaction. In all cases we observed the resultingperformance to be similar to the bottleneck proposed inSection 2.1. Therefore, due to its simplicity and strong per-formance, we recommend the original adapter architecture.4. Related WorkPre-trained text representations Pre-trained textual rep-resentations are widely used to improve performance onNLP tasks. These representations are trained on large cor-pora (usually unsupervised), and fed as features to down-stream models. In deep networks, these features may also befine-tuned on the downstream task. Brown clusters, trainedon distributional information, are a classic example of pre-trained representations (Brown et al., 1992). Turian et al.(2010) show that pre-trained embeddings of words outper-form those trained from scratch. Since deep-learning be-came popular, word embeddings have been widely used, andmany training strategies have arisen (Mikolov et al., 2013;Pennington et al., 2014; Bojanowski et al., 2017). Embed-dings of longer texts, sentences and paragraphs, have alsobeen developed (Le & Mikolov, 2014; Kiros et al., 2015;Conneau et al., 2017; Cer et al., 2019).To encode context in these representations, features areextracted from internal representations of sequence models,such as MT systems (McCann et al., 2017), and BiLSTMlanguage models, as used in ELMo (Peters et al., 2018). Aswith adapters, ELMo exploits the layers other than the toplayer of a pre-trained network. However, this strategy onlyreads from the inner layers. In contrast, adapters write tothe inner layers, re-configuring the processing of featuresthrough the entire network.Fine-tuning Fine-tuning an entire pre-trained model hasbecome a popular alternative to features (Dai & Le, 2015;6 We treat here MNLIm and MNLImm as separate tasks. Forconsistency, for all datasets we use accuracy metric and excludethe regression STS-B task.Parameter-Efficient Transfer Learning for NLPMNLIm CoLA0 1 2 3 4 5 6 7 8 9 10 11Last ablated layer01234567891011First ablated layer\u221240\u221232\u221224\u221216\u2212800 1 2 3 4 5 6 7 8 9 10 11Last ablated layer01234567891011First ablated layer\u221212\u22129\u22126\u22123010-7 10-6 10-5 10-4 10-3 10-2 10-1\u00be657075808590Validation accuracy (%)MNLImCoLAFigure 6. Left, Center: Ablation of trained adapters from continuous layer spans. The heatmap shows the relative decrease in validationaccuracy to the fully trained adapted model. The y and x axes indicate the first and last layers ablated (inclusive), respectively. Thediagonal cells, highlighted in green, indicate ablation of a single layer\u2019s adapters. The cell in the top-right indicates ablation of all adapters.Cells in the lower triangle are meaningless, and are set to 0%, the best possible relative performance. Right: Performance of BERTBASEusing adapters with different initial weight magnitudes. The x-axis is the standard deviation of the initialization distribution.Howard & Ruder, 2018; Radford et al., 2018) In NLP, theupstream model is usually a neural language model (Ben-gio et al., 2003). Recent state-of-the-art results on ques-tion answering (Rajpurkar et al., 2016) and text classi-fication (Wang et al., 2018) have been attained by fine-tuning a Transformer network (Vaswani et al., 2017) with aMasked Language Model loss (Devlin et al., 2018). Perfor-mance aside, an advantage of fine-tuning is that it does notrequire task-specific model design, unlike representation-based transfer. However, vanilla fine-tuning does require anew set of network weights for every new task.Multi-task Learning Multi-task learning (MTL) involvestraining on tasks simultaneously. Early work shows thatsharing network parameters across tasks exploits task reg-ularities, yielding improved performance (Caruana, 1997).The authors share weights in lower layers of a network,and use specialized higher layers. Many NLP systems haveexploited MTL. Some examples include: text processingsystems (part of speech, chunking, named entity recogni-tion, etc.) (Collobert & Weston, 2008), multilingual mod-els (Huang et al., 2013), semantic parsing (Peng et al., 2017),machine translation (Johnson et al., 2017), and question an-swering (Choi et al., 2017). MTL yields a single modelto solve all problems. However, unlike our adapters, MTLrequires simultaneous access to the tasks during training.Continual Learning As an alternative to simultaneoustraining, continual, or lifelong, learning aims to learn from asequence of tasks (Thrun, 1998). However, when re-trained,deep networks tend to forget how to perform previous tasks;a challenge termed catastrophic forgetting (McCloskey &Cohen, 1989; French, 1999). Techniques have been pro-posed to mitigate forgetting (Kirkpatrick et al., 2017; Zenkeet al., 2017), however, unlike for adapters, the memory isimperfect. Progressive Networks avoid forgetting by instan-tiating a new network \u201ccolumn\u201d for each task (Rusu et al.,2016). However, the number of parameters grows linearlywith the number of tasks, since adapters are very small, ourmodels scale much more favorably.Transfer Learning in Vision Fine-tuning models pre-trained on ImageNet (Deng et al., 2009) is ubiquitous whenbuilding image recognition models (Yosinski et al., 2014;Huh et al., 2016). This technique attains state-of-the-art per-formance on many vision tasks, including classification (Ko-rnblith et al., 2018), fine-grained classifcation (Hermanset al., 2017), segmentation (Long et al., 2015), and de-tection (Girshick et al., 2014). In vision, convolutionaladapter modules have been studied (Rebuffi et al., 2017;2018; Rosenfeld & Tsotsos, 2018). These works performincremental learning in multiple domains by adding smallconvolutional layers to a ResNet (He et al., 2016) or VGGnet (Simonyan & Zisserman, 2014). Adapter size is lim-ited using 1\u00d7 1 convolutions, whilst the original networkstypically use 3 \u00d7 3. This yields 11% increase in overallmodel size per task. Since the kernel size cannot be furtherreduced other weight compression techniques must be usedto attain further savings. Our bottleneck adapters can bemuch smaller, and still perform well.Concurrent work explores similar ideas for BERT (Stickland& Murray, 2019). The authors introduce Projected Atten-tion Layers (PALs), small layers with a similar role to ouradapters. The main differences are i) Stickland & Murray(2019) use a different architecture, and ii) they perform mul-titask training, jointly fine-tuning BERT on all GLUE tasks.Sina Semnani (2019) perform an emprical comparison ofour bottleneck Adpaters and PALs on SQuAD v2.0 (Ra-jpurkar et al., 2018).ACKNOWLEDGMENTSWe would like to thank Andrey Khorlin, Lucas Beyer, No\u00e9Lutz, and Jeremiah Harmsen for useful comments and dis-cussions.Parameter-Efficient Transfer Learning for NLPReferencesAlmeida, T. A., Hidalgo, J. M. G., and Yamakami, A. Con-tributions to the Study of SMS Spam Filtering: NewCollection and Results. In Proceedings of the 11th ACMSymposium on Document Engineering. ACM, 2011.Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. Aneural probabilistic language model. Journal of MachineLearning Research, 2003.Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. En-riching word vectors with subword information. ACL,2017.Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D.,and Lai, J. C. Class-based n-gram models of naturallanguage. Computational Linguistics, 1992.Caruana, R. Multitask learning. Machine Learning, 1997.Cer, D., Yang, Y., Kong, S.-y., Hua, N., Limtiaco, N., John,R. S., Constant, N., Guajardo-Cespedes, M., Yuan, S.,Tar, C., et al. Universal sentence encoder. arXiv preprintarXiv:1803.11175, 2018.Cer, D., Yang, Y., Kong, S.-y., Hua, N., Limtiaco, N.,St. John, R., Constant, N., Guajardo-Cespedes, M., Yuan,S., Tar, C., Strope, B., and Kurzweil, R. Universal sen-tence encoder for english. In EMNLP, 2019.Chen, T., Lucic, M., Houlsby, N., and Gelly, S. On selfmodulation for generative adversarial networks. ICLR,2019.Choi, E., Hewlett, D., Uszkoreit, J., Polosukhin, I., Lacoste,A., and Berant, J. Coarse-to-fine question answering forlong documents. In ACL, 2017.Collobert, R. and Weston, J. A unified architecture fornatural language processing: Deep neural networks withmultitask learning. In ICML, 2008.Conneau, A., Kiela, D., Schwenk, H., Barrault, L., andBordes, A. Supervised learning of universal sentencerepresentations from natural language inference data. InEMNLP, 2017.Dai, A. M. and Le, Q. V. Semi-supervised sequence learning.In NIPS. 2015.De Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O.,and Courville, A. C. Modulating early visual processingby language. In NIPS, 2017.Deng, J., Dong, W., Socher, R., jia Li, L., Li, K., and Fei-fei,L. Imagenet: A large-scale hierarchical image database.In In CVPR, 2009.Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:Pre-training of deep bidirectional transformers for lan-guage understanding. arXiv preprint arXiv:1810.04805,2018.French, R. M. Catastrophic forgetting in connectionist net-works. Trends in cognitive sciences, 1999.Girshick, R., Donahue, J., Darrell, T., and Malik, J. Richfeature hierarchies for accurate object detection and se-mantic segmentation. In CVPR, 2014.He, K., Zhang, X., Ren, S., and Sun, J. Deep residuallearning for image recognition. In CVPR, 2016.Hermans, A., Beyer, L., and Leibe, B. In Defense of theTriplet Loss for Person Re-Identification. arXiv preprintarXiv:1703.07737, 2017.Howard, J. and Ruder, S. Universal language model fine-tuning for text classification. In ACL, 2018.Huang, J.-T., Li, J., Yu, D., Deng, L., and Gong, Y. Cross-language knowledge transfer using multilingual deep neu-ral network with shared hidden layers. In ICASSP, 2013.Huh, M., Agrawal, P., and Efros, A. A. What makesimagenet good for transfer learning? arXiv preprintarXiv:1608.08614, 2016.Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y.,Chen, Z., Thorat, N., Vi\u00e9gas, F., Wattenberg, M., Corrado,G., Hughes, M., and Dean, J. Google\u2019s multilingualneural machine translation system: Enabling zero-shottranslation. ACL, 2017.Kingma, D. and Ba, J. Adam: A method for stochasticoptimization. ICLR, 2014.Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des-jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T.,Grabska-Barwinska, A., et al. Overcoming catastrophicforgetting in neural networks. PNAS, 2017.Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun,R., Torralba, A., and Fidler, S. Skip-thought vectors. InNIPS. 2015.Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenetmodels transfer better? arXiv preprint arXiv:1805.08974,2018.Lang, K. Newsweeder: Learning to filter netnews. In ICML,1995.Le, Q. and Mikolov, T. Distributed representations of sen-tences and documents. In ICML, 2014.Lichman, M. UCI machine learning repository, 2013.Parameter-Efficient Transfer Learning for NLPLong, J., Shelhamer, E., and Darrell, T. Fully convolutionalnetworks for semantic segmentation. In CVPR, 2015.McCann, B., Bradbury, J., Xiong, C., and Socher, R.Learned in translation: Contextualized word vectors. InGuyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fer-gus, R., Vishwanathan, S., and Garnett, R. (eds.), NIPS.2017.McCloskey, M. and Cohen, N. J. Catastrophic interferencein connectionist networks: The sequential learning prob-lem. In Psychology of learning and motivation. 1989.Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., andDean, J. Distributed representations of words and phrasesand their compositionality. In NIPS. 2013.Peng, H., Thomson, S., and Smith, N. A. Deep multitasklearning for semantic dependency parsing. In ACL, 2017.Pennington, J., Socher, R., and Manning, C. Glove: Globalvectors for word representation. In EMNLP, 2014.Perez, E., Strub, F., de Vries, H., Dumoulin, V., andCourville, A. C. Film: Visual reasoning with a generalconditioning layer. AAAI, 2018.Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,Lee, K., and Zettlemoyer, L. Deep contextualized wordrepresentations. In NAACL, 2018.Radford, A., Narasimhan, K., Salimans, T., and Sutskever,I. Improving language understanding by genera-tive pre-training. URL https://s3-us-west-2. ama-zonaws. com/openai-assets/research-covers/language-unsupervised/language_ understanding_paper. pdf, 2018.Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:100,000+ questions for machine comprehension of text.In EMNLP, 2016.Rajpurkar, P., Jia, R., and Liang, P. Know what you don\u2019tknow: Unanswerable questions for squad. In ACL, 2018.Rebuffi, S., Vedaldi, A., and Bilen, H. Efficient parametriza-tion of multi-domain deep neural networks. In CVPR,2018.Rebuffi, S.-A., Bilen, H., and Vedaldi, A. Learning multiplevisual domains with residual adapters. In NIPS. 2017.Rosenfeld, A. and Tsotsos, J. K. Incremental learningthrough deep adaptation. IEEE transactions on patternanalysis and machine intelligence, 2018.Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H.,Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had-sell, R. Progressive neural networks. arXiv preprintarXiv:1606.04671, 2016.Simonyan, K. and Zisserman, A. Very deep convolutionalnetworks for large-scale image recognition. ICLR, 2014.Sina Semnani, Kaushik Sadagopan, F. T. BERT-A: Fine-tuning BERT with Adapters and Data Augmentation.http://web.stanford.edu/class/cs224n/reports/default/15848417.pdf,2019.Stickland, A. C. and Murray, I. BERT and PALs: ProjectedAttention Layers for Efficient Adaptation in Multi-TaskLearning. arXiv preprint arXiv:1902.02671, 2019.Thrun, S. Learning to learn. chapter Lifelong LearningAlgorithms. 1998.Turian, J., Ratinov, L., and Bengio, Y. Word representa-tions: A simple and general method for semi-supervisedlearning. In ACL, 2010.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-tion is all you need. In NIPS. 2017.Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., andBowman, S. R. Glue: A multi-task benchmark and analy-sis platform for natural language understanding. ICLR,2018.Wong, C., Houlsby, N., Lu, Y., and Gesmundo, A. Transferlearning with neural automl. In NeurIPS. 2018.Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How trans-ferable are features in deep neural networks? In Ghahra-mani, Z., Welling, M., Cortes, C., Lawrence, N. D., andWeinberger, K. Q. (eds.), NIPS. 2014.Zenke, F., Poole, B., and Ganguli, S. Continual learningthrough synaptic intelligence. ICML, 2017.Zoph, B. and Le, Q. V. Neural architecture search withreinforcement learning. In ICLR, 2017.Supplementary Material forParameter-Efficient Transfer Learning for NLPA. Additional Text Classification TasksDataset Train examples Validation examples Test examples Classes Avg text length Reference20 newsgroups 15076 1885 1885 20 1903 (Lang, 1995)Crowdflower airline 11712 1464 1464 3 104 crowdflower.comCrowdflower corporate messaging 2494 312 312 4 121 crowdflower.comCrowdflower disasters 8688 1086 1086 2 101 crowdflower.comCrowdflower economic news relevance 6392 799 800 2 1400 crowdflower.comCrowdflower emotion 32000 4000 4000 13 73 crowdflower.comCrowdflower global warming 3380 422 423 2 112 crowdflower.comCrowdflower political audience 4000 500 500 2 205 crowdflower.comCrowdflower political bias 4000 500 500 2 205 crowdflower.comCrowdflower political message 4000 500 500 9 205 crowdflower.comCrowdflower primary emotions 2019 252 253 18 87 crowdflower.comCrowdflower progressive opinion 927 116 116 3 102 crowdflower.comCrowdflower progressive stance 927 116 116 4 102 crowdflower.comCrowdflower US economic performance 3961 495 496 2 305 crowdflower.comCustomer complaint database 146667 18333 18334 157 1046 catalog.data.govNews aggregator dataset 338349 42294 42294 4 57 (Lichman, 2013)SMS spam collection 4459 557 558 2 81 (Almeida et al., 2011)Table 3. Statistics and references for the additional text classification tasks.Dataset Epochs (Fine-tune) Epochs (Adapters)20 newsgroups 50 50Crowdflower airline 50 20Crowdflower corporate messaging 100 50Crowdflower disasters 50 50Crowdflower economic news relevance 20 20Crowdflower emotion 20 20Crowdflower global warming 100 50Crowdflower political audience 50 20Crowdflower political bias 50 50Crowdflower political message 50 50Crowdflower primary emotions 100 100Crowdflower progressive opinion 100 100Crowdflower progressive stance 100 100Crowdflower US economic performance 100 20Customer complaint database 20 20News aggregator dataset 20 20SMS spam collection 50 20Table 4. Number of training epochs selected for the additional classification tasks.Parameter-Efficient Transfer Learning for NLPParameter Search Space1) Input embedding modules Refer to Table 62) Fine-tune input embedding module {True, False}3) Lowercase text {True, False}4) Remove non alphanumeric text {True, False}5) Use convolution {True, False}6) Convolution activation {relu, relu6, leaky relu, swish, sigmoid, tanh}7) Convolution batch norm {True, False}8) Convolution max ngram length {2, 3}9) Convolution dropout rate [0.0, 0.4]10) Convolution number of filters [50, 200]11) Convolution embedding dropout rate [0.0, 0.4]12) Number of hidden layers {0, 1, 2, 3, 5}13) Hidden layers size {64, 128, 256}14) Hidden layers activation {relu, relu6, leaky relu, swish, sigmoid, tanh}15) Hidden layers normalization {none, batch norm, layer norm}16) Hidden layers dropout rate {0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}17) Deep tower learning rate {0.001, 0.005, 0.01, 0.05, 0.1, 0.5}18) Deep tower regularization weight {0.0, 0.0001, 0.001, 0.01}19) Wide tower learning rate {0.001, 0.005, 0.01, 0.05, 0.1, 0.5}20) Wide tower regularization weight {0.0, 0.0001, 0.001, 0.01}21) Number of training samples {1e5, 2e5, 5e5, 1e6, 2e6}Table 5. The search space of baseline models for the additional text classification tasks.ID Dataset Embed Vocab. Training TensorFlow Hub Handlessize dim. size algorithm Prefix: https://tfhub.dev/google/(tokens)English-small 7B 50 982k Lang. model nnlm-en-dim50-with-normalization/1English-big 200B 128 999k Lang. model nnlm-en-dim128-with-normalization/1English-wiki-small 4B 250 1M Skipgram Wiki-words-250-with-normalization/1English-wiki-big 4B 500 1M Skipgram Wiki-words-500-with-normalization/1Universal-sentence-encoder - 512 - (Cer et al., 2018) universal-sentence-encoder/2Table 6. Options for text input embedding modules. These are pre-trained text embedding tables. We provide the handle for the modulesthat are publicly distributed via the TensorFlow Hub service (https://www.tensorflow.org/hub).B. Learning Rate RobustnessWe test the robustness of adapters and fine-tuning to the learning rate. We ran experiments with learning rates in the range[2 \u00b7 10\u22125, 10\u22123], and selected the best hyperparameters for each method at each learning rate. Figure 7 shows the results.Parameter-Efficient Transfer Learning for NLPDataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 2120 newsgroups Universal-sentence-encoder False True True False relu6 False 2 0.37 94 0.38 1 128 leaky relu batch norm 0.5 0.5 0 0.05 0.0001 1000000Crowdflower airline English-big False False False True leaky relu False 3 0.36 200 0.07 0 128 tanh layer norm 0.4 0.1 0.001 0.05 0.001 200000Crowdflower corporate messaging English-big False False True True tanh True 3 0.40 56 0.40 1 64 tanh batch norm 0.5 0.5 0.001 0.01 0 200000Crowdflower disasters Universal-sentence-encoder True True False True swish True 3 0.27 52 0.22 0 64 relu none 0.2 0.005 0.0001 0.005 0.01 500000Crowdflower economic news relevance Universal-sentence-encoder True True False False leaky relu False 2 0.27 63 0.04 3 128 swish layer norm 0.2 0.01 0.01 0.001 0 100000Crowdflower emotion Universal-sentence-encoder False True False False relu6 False 3 0.35 132 0.34 1 64 tanh none 0.05 0.05 0 0.05 0 200000Crowdflower global warming Universal-sentence-encoder False True True False swish False 3 0.39 200 0.36 1 128 leaky relu batch norm 0.4 0.05 0 0.001 0.001 1000000Crowdflower political audience English-small True False True True relu False 3 0.11 98 0.07 0 64 relu none 0.5 0.05 0.001 0.001 0 100000Crowdflower political bias English-big False True True False swish False 3 0.12 81 0.30 0 64 relu6 none 0 0.01 0 0.005 0.01 200000Crowdflower political message Universal-sentence-encoder False False True False swish True 2 0.36 57 0.35 0 64 tanh none 0.5 0.01 0.001 0.005 0 200000Crowdflower primary emotions English-big False True True True swish False 3 0.40 191 0.03 0 256 relu6 none 0.5 0.1 0.001 0.05 0 200000Crowdflower progressive opinion English-big True False True True relu6 False 3 0.40 199 0.28 0 128 relu batch norm 0.3 0.1 0.01 0.005 0.001 200000Crowdflower progressive stance Universal-sentence-encoder True False True False relu True 3 0.01 195 0.00 2 256 tanh layer norm 0.4 0.005 0 0.005 0.0001 500000Crowdflower us economic performance English-big True False True True tanh True 2 0.31 53 0.24 1 256 leaky relu batch norm 0.3 0.05 0.0001 0.001 0.0001 100000Customer complaint database English-big True False False False tanh False 2 0.03 69 0.10 1 256 leaky relu layer norm 0.1 0.05 0.0001 0.05 0.001 1000000News aggregator dataset Universal-sentence-encoder False True True False sigmoid True 2 0.00 156 0.29 3 256 relu batch norm 0.05 0.05 0 0.5 0.0001 1000000Sms spam collection English-wiki-small True True True True leaky relu False 3 0.20 54 0.00 1 128 leaky relu batch norm 0 0.1 0 0.05 0.01 1000000Table 7. Search space parameters (see Table 5) for the AutoML baseline models that were selected.10-4 10-3Learning rate707580859095f1 scoreAdaptersFine-tune top layersFigure 7. Best performing models at different learning rates. Error vars indicate the s.e.m. across three random seeds."