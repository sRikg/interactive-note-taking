Dataset \\pbox5cmNo BERT baseline \\pbox5cmBERT_(BASE) Fine-tune \\pbox5cmBERT_(BASE) Variable FT \\pbox5cmBERT_(BASE) Adapters 20 newsgroups 91.1 92.8\u2005\u00b1\u20050.1 92.8\u2005\u00b1\u20050.1 91.7\u2005\u00b1\u20050.2 Crowdflower airline 84.5 83.6\u2005\u00b1\u20050.3 84.0\u2005\u00b1\u20050.1 84.5\u2005\u00b1\u20050.2 Crowdflower corporate messaging 91.9 92.5\u2005\u00b1\u20050.5 92.4\u2005\u00b1\u20050.6 92.9\u2005\u00b1\u20050.3 Crowdflower disasters 84.9 85.3\u2005\u00b1\u20050.4 85.3\u2005\u00b1\u20050.4 84.1\u2005\u00b1\u20050.2 Crowdflower economic news relevance 81.1 82.1\u2005\u00b1\u20050.0 78.9\u2005\u00b1\u20052.8 82.5\u2005\u00b1\u20050.3 Crowdflower emotion 36.3 38.4\u2005\u00b1\u20050.1 37.6\u2005\u00b1\u20050.2 38.7\u2005\u00b1\u20050.1 Crowdflower global warming 82.7 84.2\u2005\u00b1\u20050.4 81.9\u2005\u00b1\u20050.2 82.7\u2005\u00b1\u20050.3 Crowdflower political audience 80.8 80.9\u2005\u00b1\u20050.3 80.7\u2005\u00b1\u20050.8 79.0\u2005\u00b1\u20050.5 Crowdflower political bias 76.8 75.2\u2005\u00b1\u20050.9 76.5\u2005\u00b1\u20050.4 75.9\u2005\u00b1\u20050.3 Crowdflower political message 43.8 38.9\u2005\u00b1\u20050.6 44.9\u2005\u00b1\u20050.6 44.1\u2005\u00b1\u20050.2 Crowdflower primary emotions 33.5 36.9\u2005\u00b1\u20051.6 38.2\u2005\u00b1\u20051.0 33.9\u2005\u00b1\u20051.4 Crowdflower progressive opinion 70.6 71.6\u2005\u00b1\u20050.5 75.9\u2005\u00b1\u20051.3 71.7\u2005\u00b1\u20051.1 Crowdflower progressive stance 54.3 63.8\u2005\u00b1\u20051.0 61.5\u2005\u00b1\u20051.3 60.6\u2005\u00b1\u20051.4 Crowdflower US economic performance 75.6 75.3\u2005\u00b1\u20050.1 76.5\u2005\u00b1\u20050.4 77.3\u2005\u00b1\u20050.1 Customer complaint database 54.5 55.9\u2005\u00b1\u20050.1 56.4\u2005\u00b1\u20050.1 55.4\u2005\u00b1\u20050.1 News aggregator dataset 95.2 96.3\u2005\u00b1\u20050.0 96.5\u2005\u00b1\u20050.0 96.2\u2005\u00b1\u20050.0 SMS spam collection 98.5 99.3\u2005\u00b1\u20050.2 99.3\u2005\u00b1\u20050.2 95.1\u2005\u00b1\u20052.2 Average 72.7 73.7 74.0 73.3 Total number of params \u2014 17\u00d7 9.9\u00d7 1.19\u00d7 Trained params/task \u2014 100% 52.9% 1.14% Table 2: Test accuracy for additional classification tasks. In these experiments we transfer from the BERT_(BASE) model. For each task and algorithm, the model with the best validation set accuracy is chosen. We report the mean test accuracy and s.e.m. across runs with different random seeds. To further validate that adapters yields compact, performant, models, we test on additional public text classification tasks. This suite contains a diverse set of tasks: the number of training examples ranges from 900 to 330k, the number of classes ranges from 2 to 157, and the average text length ranging from 57 to 1.9k characters. We supply statistics and references for all of the datasets in the appendix. For these datasets, we use a batch size of 32. The datasets are diverse, so we sweep a wide range of learning rates: {1\u2005\u22c5\u200510^(\u2005\u2212\u20055),\u20063\u2005\u22c5\u200510^(\u2005\u2212\u20055),\u20061\u2005\u22c5\u200510^(\u2005\u2212\u20054),\u20063\u2005\u22c5\u200510^(\u2005\u2212\u20053)}. Due to the large number of datasets, we manually select the number of training epochs from the set {20,\u200650,\u2006100} by inspecting the validation set learning curves, rather than sweep this parameter. We selected optimal values for both fine-tuning and adapters; the exact values are in the appendix. We test adapters sizes in {2,\u20064,\u20068,\u200616,\u200632,\u200664}. Since some of the datasets are small, fine-tuning the entire network may be sub-optimal. Therefore, we run an additional baseline: variable fine-tuning. For this, we fine-tune only the top n layers, and freeze the remainder. We sweep n\u2004\u2208\u2004{1,\u20062,\u20063,\u20065,\u20067,\u20069,\u200611,\u200612}. In these experiments, we use the BERT_(BASE) model with 12 layers, therefore variable fine-tuning becomes full fine-tuning when n\u2004=\u200412. [Figure 3: Accuracy versus the number of trained parameters, aggregated across tasks. We compare adapters of different sizes (orange) with fine-tuning the top n layers, for varying n (blue). The lines and shaded areas indicate the 20th, 50th, and 80th percentiles across tasks. For each task and algorithm, the best model is selected for each point along the curve. For GLUE, the validation set accuracy is reported (the test set accuracy requires submission to the GLUE server). For the additional tasks, we report the test set accuracies. To remove intra-task variance in scores, we normalize the score for each model and task by subtracting the performance of full fine-tuning on the corresponding task.] [Figure 4: Validation set accuracy versus number of trained parameters for three methods: (i) Adapter tuning with an adapter sizes 2^(n) for n\u2004=\u20040\u20269 (orange). (ii) Fine-tuning the top k layers for k\u2004=\u20041\u202612 (blue). (iii) Tuning the layer normalization parameters only (green). Error bars indicate \u2005\u00b1\u20051 s.e.m. across three random seeds.] Unlike the GLUE tasks, there is no comprehensive set of state-of-the-art numbers for this set of tasks. Therefore, to check that our BERT models (with and without adapters) are competitive, we collect our own benchmark performances. For this, we run a large-scale hyperparameter search over standard network topologies. Specifically, we run the single-task Neural AutoML algorithm, similar to\u00a0Zoph & Le (2017); Wong et\u00a0al. (2018). This algorithm searches over a space of feedforward and convolutional networks, stacked on top of pre-trained text embeddings modules publicly available via TensorFlow Hub\u2074\u20744https://www.tensorflow.org/hub. The TensorFlow Hub embeddings may be frozen or fine-tuned. The full search space is described in the appendix. For each task, we run AutoML for one week on CPUs, using 30 machines. In this time the algorithm explores over 10k models on average per task. We select the best final model for each task according to validation set accuracy. The results for the AutoML benchmark (\u201cno BERT baseline\u201d), fine-tuning, variable fine-tuning, and adapter-tuning are reported in Table\u00a02. The AutoML baseline demonstrates that the BERT models are competitive. This baseline explores thousands of models, yet the BERT models perform better on average. We see a pattern of results similar to GLUE. The performance of adapter-tuning is close to full fine-tuning (0.4% behind). Fine-tuning requires 17\u00d7 the number of parameters to BERT_(BASE) to solve all tasks. Variable fine-tuning performs slightly better than fine-tuning, whilst training fewer layers. The optimal setting of variable fine-tuning results in training 52% of the network on average per task, reducing the total to 9.9\u00d7 parameters. Adapters, however, offer a much more compact model. They introduce 1.14% new parameters per task, resulting in 1.19\u00d7 parameters for all 17 tasks.