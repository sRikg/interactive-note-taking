Fine-tuning an entire pre-trained model has become a popular alternative to features\u00a0(Dai & Le, 2015; Howard & Ruder, 2018; Radford et\u00a0al., 2018) In NLP, the upstream model is usually a neural language model\u00a0(Bengio et\u00a0al., 2003). Recent state-of-the-art results on question answering\u00a0(Rajpurkar et\u00a0al., 2016) and text classification\u00a0(Wang et\u00a0al., 2018) have been attained by fine-tuning a Transformer network\u00a0(Vaswani et\u00a0al., 2017) with a Masked Language Model loss\u00a0(Devlin et\u00a0al., 2018). Performance aside, an advantage of fine-tuning is that it does not require task-specific model design, unlike representation-based transfer. However, vanilla fine-tuning does require a new set of network weights for every new task.