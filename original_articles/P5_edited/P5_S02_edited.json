"Transfer from pre-trained models yields strong performance on many NLP tasks\u00a0(Dai & Le, 2015; Howard & Ruder, 2018; Radford et\u00a0al., 2018). Recently, BERT, a Transformer network trained on large text corpora with an unsupervised loss, attained state-of-the-art performance on text classification and extractive question answering datasets\u00a0(Devlin et\u00a0al., 2018). In this paper we consider the online setting, where tasks arrive in a stream. The goal is to build a system that performs well on all of them, but without training an entire new model for every new task. A high degree of sharing between tasks is particularly useful for applications such as cloud services, where models need to be trained to solve many tasks that arrive from customers in sequence. For this, we propose a transfer learning strategy that yields compact and extensible downstream models. Compact models are those that solve many tasks using a small number of additional parameters per task. Extensible models can be trained incrementally to solve new tasks, without forgetting previous ones. Our method yields a highly compact and extensible model without sacrificing performance. [Figure 1: Trade-off between accuracy and number of trained task-specific parameters, for adapter tuning and fine-tuning. The y-axis is normalized by the performance of full fine-tuning, details in Section\u00a03. The curves show the 20th, 50th, and 80th performance percentiles across nine tasks from the GLUE benchmark. Adapter-based tuning attains a similar performance to full fine-tuning with two orders of magnitude fewer trained parameters.] The two most common transfer learning techniques in NLP are feature-based transfer and fine-tuning. We present an alternative transfer method based on adapter modules\u00a0(Rebuffi et\u00a0al., 2017). Features-based transfer typically involves pre-training real-valued embeddings vectors. These embeddings may be at the word\u00a0(Mikolov et\u00a0al., 2013), sentence\u00a0(Cer et\u00a0al., 2019), or paragraph level\u00a0(Le & Mikolov, 2014). The embeddings are then fed to custom downstream models. Instead, fine-tuning involves copying the weights from a pre-trained network and tuning them on the downstream task. Recent work shows that fine-tuning often enjoys better performance than feature-based transfer\u00a0(Howard & Ruder, 2018). Both feature-based transfer and fine-tuning require a new set of weights for each task. Fine-tuning is more parameter efficient if the lower layers of a network are shared between tasks. However, our proposed adapter tuning method is even more parameter efficient. Figure\u00a01 demonstrates this trade-off. The figure shows that adapter tuning achieves a comparable quality to full fine-tuning. The x-axis shows the number of parameters trained per task; this corresponds to the marginal increase in the model size required to solve each additional task. Adapter-based tuning requires training two orders of magnitude fewer parameters to fine-tuning, while attaining similar performance. Adapters are new modules added between layers of a pre-trained network. Adapter-based tuning differs from feature-based transfer and fine-tuning in the following way. Consider a function (neural network) with parameters w: \u03d5_(w)(x). Feature-based transfer composes \u03d5_(w) with a new function, \u03c7_(v), to yield \u03c7_(v)(\u03d5_(w)(x)). Only the new, task-specific, parameters, v, are then trained. Fine-tuning involves adjusting the original parameters, w, for each new task, limiting compactness. For adapter tuning, a new function, \u03c8_(w,\u2006v)(x), is defined, where parameters w are copied over from pre-training. The initial parameters v\u2080 are set such that the new function resembles the original: \u03c8_(w,\u2006v\u2080)(x)\u2004\u2248\u2004\u03d5_(w)(x). During training, only v are tuned. For deep networks, defining \u03c8_(w,\u2006v) typically involves adding new layers to the original network, \u03d5_(w). If one chooses |v|\u2004\u226a\u2004|w|, the resulting model requires \u2004\u223c\u2004|w| parameters for many tasks. Since w is fixed, the model can be extended to new tasks without affecting previous ones. Adapter-based tuning relates to multi-task and continual learning. Multi-task learning also results in compact models. However, multi-task learning requires simultaneous access to all tasks, which adapter-based tuning does not require. Continual learning systems aim to learn from an endless stream of tasks. This paradigm is challenging because networks forget previous tasks after re-training\u00a0(McCloskey & Cohen, 1989; French, 1999). Adapters differ in that the tasks to not interact and the shared parameters are frozen. This means that the model has perfect memory of previous tasks using a small number of task-specific parameters. We demonstrate on a large and diverse set of text classification tasks that adapters yield parameter-efficient tuning for NLP. The key innovation is to design an effective adapter module and its integration with the base model. We propose a simple yet effective, bottleneck architecture. On the GLUE benchmark, our strategy almost matches the performance of the fully fine-tuned BERT, but uses only 3% task-specific parameters, while fine-tuning uses 100% task-specific parameters. We observe similar results on a further 17 public text datasets. In summary, adapter-based tuning yields a single, extensible, model that attains near state-of-the-art performance in text classification."
