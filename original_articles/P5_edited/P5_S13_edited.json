As an alternative to simultaneous training, continual, or lifelong, learning algorithms aim to learn from a sequence of tasks\u00a0(Thrun, 1998). However, when re-trained, deep networks tend to forget how to perform previous tasks; a challenge termed catastrophic forgetting\u00a0(McCloskey & Cohen, 1989; French, 1999). Techniques have been proposed to mitigate forgetting\u00a0(Kirkpatrick et\u00a0al., 2017; Zenke et\u00a0al., 2017), however, unlike for adapters, the memory is still imperfect. Progressive Networks avoid forgetting by instantiating a new network \u201ccolumn\u201d for each task\u00a0(Rusu et\u00a0al., 2016). However, the number of parameters grows linearly with the number of tasks, since adapters are very small, our models scale much more favorably.