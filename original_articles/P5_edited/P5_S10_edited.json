"Pre-trained textual representations are widely used to improve performance on NLP tasks. These representations are trained on large corpora (usually, but not always, unsupervised), and fed as features to downstream models. In deep networks, these features may also be fine-tuned on the downstream task. Brown clusters, trained on distributional information, are a classic example of pre-trained representations\u00a0(Brown et\u00a0al., 1992). Turian et\u00a0al. (2010) show that pre-trained embeddings of words outperform those trained from scratch. Since the deep-learning era, word embeddings have been widely used, and training strategies these have arisen\u00a0(Mikolov et\u00a0al., 2013; Pennington et\u00a0al., 2014; Bojanowski et\u00a0al., 2017). Embeddings of longer texts, sentences and paragraphs, have also been developed\u00a0(Le & Mikolov, 2014; Kiros et\u00a0al., 2015; Conneau et\u00a0al., 2017; Cer et\u00a0al., 2019). To encode context in these representations, features are extracted from internal representations of sequence models, such as MT systems\u00a0(McCann et\u00a0al., 2017), and BiLSTM language models, as used in ELMo\u00a0(Peters et\u00a0al., 2018). As with adapters, ELMo exploits the layers other than the top layer of a pre-trained network. However, this strategy only reads from the inner layers. In contrast, adapters write to the inner layers, re-configuring the processing of features through the entire network."
