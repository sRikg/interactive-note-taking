We show on many datasets that adapters achieve parameter efficient transfer for text classification. On the GLUE benchmark\u00a0(Wang et\u00a0al., 2018), adapter tuning is within 0.4% of full fine-tuning of BERT, but it adds only 3% of the number of parameters trained by fine-tuning. We confirm this result on a further 17 public classification tasks. We finish with an analysis that shows that adapter-based tuning automatically focuses on the higher layers and is robust to choices such as initialization scheme and number of neurons. 
