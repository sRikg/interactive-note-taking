"Regularizing and Optimizing LSTM Language ModelsarXiv:1708.02182v1  [cs.CL]  7 Aug 2017Regularizing and Optimizing LSTM Language ModelsStephen Merity 1 Nitish Shirish Keskar 1 Richard Socher 1AbstractRecurrent neural networks (RNNs), such as longshort-term memory networks (LSTMs), serve asa fundamental building block for many sequencelearning tasks, including machine translation,language modeling, and question answering. Inthis paper, we consider the specific problem ofword-level language modeling and investigatestrategies for regularizing and optimizing LSTM-based models. We propose the weight-droppedLSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regulariza-tion. Further, we introduce NT-ASGD, a vari-ant of the averaged stochastic gradient method,wherein the averaging trigger is determined us-ing a non-monotonic condition as opposed to be-ing tuned by the user. Using these and other reg-ularization strategies, we achieve state-of-the-artword level perplexities on two data sets: 57.3 onPenn Treebank and 65.8 on WikiText-2. In ex-ploring the effectiveness of a neural cache in con-junction with our proposed model, we achieve aneven lower state-of-the-art perplexity of 52.8 onPenn Treebank and 52.0 on WikiText-2.1. IntroductionEffective regularization techniques for deep learning havebeen the subject of much research in recent years. Giventhe over-parameterization of neural networks, general-ization performance crucially relies on the ability toregularize the models sufficiently. Strategies such asdropout (Srivastava et al., 2014) and batch normalization(Ioffe & Szegedy, 2015) have found great success and arenow ubiquitous in feed-forward and convolutional neuralnetworks. Na\u00efvely applying these approaches to the caseof recurrent neural networks (RNNs) has not been highlysuccessful however. Many recent works have hence beenfocused on the extension of these regularization strategiesto RNNs; we briefly discuss some of them below.1Salesforce Research, Palo Alto, USA. Correspondence to:Stephen Merity <smerity@salesforce.com>.A na\u00efve application of dropout (Srivastava et al., 2014)to an RNN\u2019s hidden state is ineffective as it disruptsthe RNN\u2019s ability to retain long term dependencies(Zaremba et al., 2014). Gal & Ghahramani (2016) proposeovercoming this problem by retaining the same dropoutmask across multiple time steps as opposed to samplinga new binary mask at each timestep. Another approachis to regularize the network through limiting updates tothe RNN\u2019s hidden state. One such approach is taken bySemeniuta et al. (2016) wherein the authors drop updatesto network units, specifically the input gates of the LSTM,in lieu of the units themselves. This is reminiscent of zone-out (Krueger et al., 2016) where updates to the hidden statemay fail to occur for randomly selected neurons.Instead of operating on the RNN\u2019s hidden states, one canregularize the network through restrictions on the recur-rent matrices as well. This can be done either throughrestricting the capacity of the matrix (Arjovsky et al.,2016; Wisdom et al., 2016; Jing et al., 2016) or throughelement-wise interactions (Balduzzi & Ghifary, 2016;Bradbury et al., 2016; Seo et al., 2016).Other forms of regularization explicitly act upon activa-tions such as batch normalization (Ioffe & Szegedy, 2015),recurrent batch normalization (Cooijmans et al., 2016), andlayer normalization (Ba et al., 2016). These all introduceadditional training parameters and can complicate the train-ing process while increasing the sensitivity of the model.In this work, we investigate a set of regularization strategiesthat are not only highly effective but which can also be usedwith no modification to existing LSTM implementations.The weight-dropped LSTM applies recurrent regulariza-tion through a DropConnect mask on the hidden-to-hiddenrecurrent weights. Other strategies include the use ofrandomized-length backpropagation through time (BPTT),embedding dropout, activation regularization (AR), andtemporal activation regularization (TAR).As no modifications are required of the LSTM implemen-tation these regularization strategies are compatible withblack box libraries, such as NVIDIA cuDNN, which canbe many times faster than na\u00efve LSTM implementations.Effective methods for training deep recurrent networkshave also been a topic of renewed interest. Once a modelhttp://arxiv.org/abs/1708.02182v1Regularizing and Optimizing LSTM Language Modelshas been defined, the training algorithm used is requiredto not only find a good minimizer of the loss function butalso converge to such a minimizer rapidly. The choice ofthe optimizer is even more important in the context of reg-ularized models since such strategies, especially the useof dropout, can impede the training process. Stochasticgradient descent (SGD), and its variants such as Adam(Kingma & Ba, 2014) and RMSprop (Tieleman & Hinton,2012) are amongst the most popular training methods.These methods iteratively reduce the training loss throughscaled (stochastic) gradient steps. In particular, Adam hasbeen found to be widely applicable despite requiring lesstuning of its hyperparameters. In the context of word-levellanguage modeling, past work has empirically found thatSGD outperforms other methods in not only the final lossbut also in the rate of convergence. This is in agreementwith recent evidence pointing to the insufficiency of adap-tive gradient methods (Wilson et al., 2017).Given the success of SGD, especially within the languagemodeling domain, we investigate the use of averaged SGD(ASGD) (Polyak & Juditsky, 1992) which is known to havesuperior theoretical guarantees. ASGD carries out itera-tions similar to SGD, but instead of returning the last iterateas the solution, returns an average of the iterates past a cer-tain, tuned, threshold T . This threshold T is typically tunedand has a direct impact on the performance of the method.We propose a variant of ASGD where T is determined onthe fly through a non-monotonic criterion and show that itachieves better training outcomes compared to SGD.2. Weight-dropped LSTMWe refer to the mathematical formulation of the LSTM,it = \u03c3(Wixt + Uiht\u22121)ft = \u03c3(Wfxt + Ufht\u22121)ot = \u03c3(Woxt + Uoht\u22121)c\u0303t = tanh(Wcxt + Ucht\u22121)ct = it \u2299 c\u0303t + ft \u2299+c\u0303t\u22121ht = ot \u2299 tanh(ct)where [W i,W f ,W o, U i, Uf , Uo] are weight matrices, xtis the vector input to the timestep t, ht is the current ex-posed hidden state, ct is the memory cell state, and \u2299 iselement-wise multiplication.Preventing overfitting within the recurrent connections ofan RNN has been an area of extensive research in languagemodeling. The majority of previous recurrent regulariza-tion techniques have acted on the hidden state vector ht\u22121,most frequently introducing a dropout operation betweentimesteps, or performing dropout on the update to the mem-ory state ct. These modifications to a standard LSTM pre-vent the use of black box RNN implementations that maybe many times faster due to low-level hardware-specific op-timizations.We propose the use of DropConnect (Wan et al., 2013)on the recurrent hidden to hidden weight matrices whichdoes not require any modifications to an RNN\u2019s formu-lation. As the dropout operation is applied once to theweight matrices, before the forward and backward pass,the impact on training speed is minimal and any standardRNN implementation can be used, including inflexible buthighly optimized black box LSTM implementations suchas NVIDIA\u2019s cuDNN LSTM.By performing DropConnect on the hidden-to-hiddenweight matrices [U i, Uf , Uo, U c] within the LSTM, we canprevent overfitting from occurring on the recurrent connec-tions of the LSTM. This regularization technique wouldalso be applicable to preventing overfitting on the recurrentweight matrices of other RNN cells.As the same weights are reused over multiple timesteps,the same individual dropped weights remain dropped forthe entirety of the forward and backward pass. The resultis similar to variational dropout, which applies the samedropout mask to recurrent connections within the LSTMby performing dropout on ht\u22121, except that the dropoutis applied to the recurrent weights. DropConnect couldalso be used on the non-recurrent weights of the LSTM[W i,W f ,W o] though our focus was on preventing over-fitting on the recurrent connection.3. OptimizationSGD is among the most popular methods for training deeplearning models across various modalities including com-puter vision, natural language processing, and reinforce-ment learning. The training of deep networks can be posedas a non-convex optimization problemminw1NN\u2211i=1fi(w),where fi is the loss function for the ith data point, w arethe weights of the network, and the expectation is takenover the data. Given a sequence of learning rates, \u03b3k, SGDiteratively takes steps of the formwk+1 = wk \u2212 \u03b3k\u2207\u0302f(wk), (1)where the subscript denotes the iteration number and the\u2207\u0302 denotes a stochastic gradient that may be computed on aminibatch of data points. SGD demonstrably performs wellin practice and also possesses several attractive theoreticalproperties such as linear convergence (Bottou et al., 2016),saddle point avoidance (Panageas & Piliouras, 2016) andRegularizing and Optimizing LSTM Language Modelsbetter generalization performance (Hardt et al., 2015). Forthe specific task of neural language modeling, tradition-ally SGD without momentum has been found to outperformother algorithms such as momentum SGD (Sutskever et al.,2013), Adam (Kingma & Ba, 2014), Adagrad (Duchi et al.,2011) and RMSProp (Tieleman & Hinton, 2012) by a sta-tistically significant margin.Motivated by this observation, we investigate averagedSGD (ASGD) to further improve the training process.ASGD has been analyzed in depth theoretically and manysurprising results have been shown including its asymp-totic second-order convergence (Polyak & Juditsky, 1992;Mandt et al., 2017). ASGD takes steps identical to equa-tion (1) but instead of returning the last iterate as the solu-tion, returns 1(K\u2212T+1)\u2211Ki=T wi, where K is the total num-ber of iterations and T < K is a user-specified averagingtrigger.Algorithm 1 Non-monotonically Triggered ASGD (NT-ASGD)Inputs: Initial pointw0, learning rate \u03b3, logging intervalL,non-monotone interval n.1: Initialize k \u2190 0, t\u2190 0, T \u2190 0, logs\u2190 []2: while stopping criterion not met do3: Compute stochastic gradient \u2207\u0302f(wk) and take SGDstep (1).4: if mod(k, L) = 0 and T = 0 then5: Compute validation perplexity v.6: if t > n and v > minl\u2208{t\u2212n,\u00b7\u00b7\u00b7 ,t}logs[l] then7: Set T \u2190 k8: end if9: Append v to logs10: t\u2190 t+ 111: end if12: end whilereturn\u2211ki=Twi(k\u2212T+1)Despite its theoretical appeal, ASGD has found limitedpractical use in training of deep networks. This may be inpart due to unclear tuning guidelines for the learning-rateschedule \u03b3k and averaging trigger T . If the averaging istriggered too soon, the efficacy of the method is impacted,and if it is triggered too late, many additional iterations maybe needed to converge to the solution. In this section, wedescribe a non-monotonically triggered variant of ASGD(NT-ASGD), which obviates the need for tuning T . Fur-ther, the algorithm uses a constant learning rate throughoutthe experiment and hence no further tuning is necessary forthe decay scheduling.Ideally, averaging needs to be triggered when the SGD it-erates converge to a steady-state distribution (Mandt et al.,2017). This is roughly equivalent to the convergence ofSGD to a neighborhood around a solution. In the case ofSGD, certain learning-rate reduction strategies such as thestep-wise strategy analogously reduce the learning rate bya fixed quantity at such a point. A common strategy em-ployed in language modeling is to reduce the learning ratesby a fixed proportion when the performance of the model\u2019sprimary metric (such as perplexity) worsens or stagnates.Along the same lines, one could make a triggering decisionbased on the performance of the model on the validationset. However, instead of averaging immediately after thevalidation metric worsens, we propose a non-monotoniccriterion that conservatively triggers the averaging whenthe validation metric fails to improve for multiple cycles;see Algorithm 1. Given that the choice of triggering is irre-versible, this conservatism ensures that the randomness oftraining does not play a major role in the decision. Anal-ogous strategies have also been proposed for learning-ratereduction in SGD (Keskar & Saon, 2015).While the algorithm introduces two additional hyperparam-eters, the logging interval L and non-monotone interval n,we found that setting L to be the number of iterations inan epoch and n = 5 worked well across various modelsand data sets. As such, we use this setting in all of our NT-ASGD experiments in the following section and demon-strate that it achieves better training outcomes as comparedto SGD.4. Extended regularization techniquesIn addition to the regularization and optimization tech-niques above, we explored additional regularization tech-niques that aimed to improve data efficiency during trainingand to prevent overfitting of the RNN model.4.1. Variable length backpropagation sequencesGiven a fixed sequence length that is used to break a dataset into fixed length batches, the data set is not efficientlyused. To illustrate this, imagine being given 100 elementsto perform backpropagation through with a fixed backprop-agation through time (BPTT) window of 10. Any elementdivisible by 10 will never have any elements to backpropinto, no matter how many times you may traverse the dataset. Indeed, the backpropagation window that each elementreceives is equal to i mod 10 where i is the element\u2019s in-dex. This is data inefficient, preventing 110of the data setfrom ever being able to improve itself in a recurrent fash-ion, and resulting in 810of the remaining elements receivingonly a partial backpropagation window compared to the fullpossible backpropagation window of length 10.To prevent such inefficient data usage, we randomly selectthe sequence length for the forward and backward pass intwo steps. First, we select the base sequence length to beRegularizing and Optimizing LSTM Language Modelsseq with probability p andseq2with probability 1\u2212p, wherep is a high value approaching 1. This spreads the start-ing point for the BPTT window beyond the base sequencelength. We then select the sequence length according toN (seq, s), where seq is the base sequence length and s isthe standard deviation. This jitters the starting point suchthat it doesn\u2019t always fall on a specific word divisible byseq orseq2. From these, the sequence length more efficientlyuses the data set, ensuring that when given enough epochsall the elements in the data set experience a full BPTT win-dow, while ensuring the average sequence length remainsaround the base sequence length for computational effi-ciency.During training, we rescale the learning rate dependingon the length of the resulting sequence compared to theoriginal specified sequence length. The rescaling step isnecessary as sampling arbitrary sequence lengths with afixed learning rate favors short sequences over longer ones.This linear scaling rule has been noted as important fortraining large scale minibatch SGD without loss of accu-racy (Goyal et al., 2017) and is a component of unbiasedtruncated backpropagation through time (Tallec & Ollivier,2017).4.2. Variational dropoutIn standard dropout, a new binary dropout mask is sampledeach and every time the dropout function is called. Newdropout masks are sampled even if the given connectionis repeated, such as the input x0 to an LSTM at timestept = 0 receiving a different dropout mask than the inputx1 fed to the same LSTM at t = 1. A variant of this,variational dropout (Gal & Ghahramani, 2016), samples abinary dropout mask only once upon the first call and thento repeatedly use that locked dropout mask for all repeatedconnections within the forward and backward pass.While we propose using DropConnect rather than varia-tional dropout to regularize the hidden-to-hidden transitionwithin an RNN, we use variational dropout for all otherdropout operations, specifically using the same dropoutmask for all inputs and outputs of the LSTM within a givenforward and backward pass. Each example within the mini-batch uses a unique dropout mask, rather than a singledropout mask being used over all examples, ensuring di-versity in the elements dropped out.4.3. Embedding dropoutFollowing Gal & Ghahramani (2016), we employ embed-ding dropout. This is equivalent to performing dropout onthe embedding matrix at a word level, where the dropout isbroadcast across all the word vector\u2019s embedding. The re-maining non-dropped-out word embeddings are scaled by11\u2212pe where pe is the probability of embedding dropout. Asthe dropout occurs on the embedding matrix that is usedfor a full forward and backward pass, this means that alloccurrences of a specific word will disappear within thatpass, equivalent to performing variational dropout on theconnection between the one-hot embedding and the embed-ding lookup.4.4. Weight tyingWeight tying (Inan et al., 2016; Press & Wolf, 2016) sharesthe weights between the embedding and softmax layer, sub-stantially reducing the total parameter count in the model.The technique has theoretical motivation (Inan et al., 2016)and prevents the model from having to learn a one-to-onecorrespondence between the input and output, resulting insubstantial improvements to the standard LSTM languagemodel.4.5. Independent embedding size and hidden sizeIn most natural language processing tasks, both pre-trained and trained word vectors are of relatively lowdimensionality\u2014frequently between 100 and 400 dimen-sions in size. Most previous LSTM language models tiethe dimensionality of the word vectors to the dimensional-ity of the LSTM\u2019s hidden state. Even if reducing the wordembedding size was not beneficial in preventing overfit-ting, the easiest reduction in total parameters for a languagemodel is reducing the word vector size. To achieve this, thefirst and last LSTM layers are modified such that their in-put and output dimensionality respectively are equal to thereduced embedding size.4.6. Activation Regularization (AR) and TemporalActivation Regularization (TAR)L2-regularization is often used on the weights of the net-work to control the norm of the resulting model and reduceoverfitting. In addition, L2 decay can be used on the in-dividual unit activations and on the difference in outputsof an RNN at different time steps; these strategies labeledas activation regularization (AR) and temporal activationregularization (TAR) respectively (Merity et al., 2017). ARpenalizes activations that are significantly larger than 0 asa means of regularizing the network. Concretely, AR isdefined as\u03b1L2(m\u2299 ht)where m is the dropout mask, L2(\u00b7) = \u2016\u00b7\u20162, ht is the out-put of the RNN at timestep t, and \u03b1 is a scaling coefficient.TAR falls under the broad category of slowness regulariz-ers (Hinton, 1989; F\u00f6ldi\u00e1k, 1991; Luciw & Schmidhuber,2012; Jonschkowski & Brock, 2015) which penalize themodel from producing large changes in the hidden state.Regularizing and Optimizing LSTM Language ModelsUsing the notation from AR, TAR is defined as\u03b2 L2(ht \u2212 ht+1)where \u03b2 is a scaling coefficient. As in Merity et al. (2017),the AR and TAR loss are only applied to the output of thefinal RNN layer as opposed to being applied to all layers.5. Experiment DetailsFor evaluating the impact of these approaches, we performlanguage modeling over a preprocessed version of the PennTreebank (PTB) (Mikolov et al., 2010) and the WikiText-2(WT2) data set (Merity et al., 2016).PTB: The Penn Treebank data set has long been a centraldata set for experimenting with language modeling. Thedata set is heavily preprocessed and does not contain capitalletters, numbers, or punctuation. The vocabulary is alsocapped at 10,000 unique words, quite small in comparisonto most modern datasets, which results in a large numberof out of vocabulary (OoV) tokens.WT2: WikiText-2 is sourced from curated Wikipedia ar-ticles and is approximately twice the size of the PTB dataset. The text is tokenized and processed using the Mosestokenizer (Koehn et al., 2007), frequently used for machinetranslation, and features a vocabulary of over 30,000 words.Capitalization, punctuation, and numbers are retained inthis data set.All experiments use a three-layer LSTM model with 1150units in the hidden layer and an embedding of size 400. Theloss was averaged over all examples and timesteps. All em-bedding weights were uniformly initialized in the interval[\u22120.1, 0.1] and all other weights were initialized between[\u2212 1\u221aH, 1\u221aH], where H is the hidden size.For training the models, we use the NT-ASGD algorithmdiscussed in the previous section for 750 epochs with Lequivalent to one epoch and n = 5. We use a batch sizeof 80 for WT2 and 40 for PTB. Empirically, we found rel-atively large batch sizes (e.g., 40-80) performed better thansmaller sizes (e.g., 10-20) for NT-ASGD. After comple-tion, we run ASGD with T = 0 and hot-started w0 as afine-tuning step to further improve the solution. For thisfine-tuning step, we terminate the run using the same non-monotonic criterion detailed in Algorithm 1.We carry out gradient clipping with maximum norm 0.25and use an initial learning rate of 30 for all experiments. Weuse a random BPTT length which is N (70, 5) with proba-bility 0.95 and N (35, 5) with probability 0.05. The valuesused for dropout on the word vectors, the output betweenLSTM layers, the output of the final LSTM layer, and em-bedding dropout where (0.4, 0.3, 0.4, 0.1) respectively. Forthe weight-dropped LSTM, a dropout of 0.5 was applied tothe recurrent weight matrices. For WT2, we increase theinput dropout to 0.65 to account for the increased vocabu-lary size. For all experiments, we use AR and TAR valuesof 2 and 1 respectively, and tie the embedding and soft-max weights. These hyperparameters were chosen throughtrial and error and we expect further improvements may bepossible if a fine-grained hyperparameter search were to beconducted. In the results, we abbreviate our approach asAWD-LSTM for ASGD Weight-Dropped LSTM.6. Experimental AnalysisWe present the single-model perplexity results for both ourmodels (AWD-LSTM) and other competitive models in Ta-ble 1 and 2 for PTB and WT2 respectively. On both datasets we improve the state-of-the-art, with our vanilla LSTMmodel beating the state of the art by approximately 1 uniton PTB and 0.1 units on WT2.In comparison to other recent state-of-the-art models, ourmodel uses a vanilla LSTM. Zilly et al. (2016) propose therecurrent highway network, which extends the LSTM to al-low multiple hidden state updates per timestep. Zoph & Le(2016) use a reinforcement learning agent to generate anRNN cell tailored to the specific task of language model-ing, with the cell far more complex than the LSTM.Independently of our work, Melis et al. (2017) apply ex-tensive hyperparameter search to an LSTM based lan-guage modeling implementation, analyzing the sensitivityof RNN based language models to hyperparameters. Un-like our work, they use a modified LSTM, which caps theinput gate it to be min(1 \u2212 ft, it), use Adam with \u03b21 = 0rather than SGD or ASGD, use skip connections betweenLSTM layers, and use a black box hyperparameter tuner forexploring models and settings. Of particular interest is thattheir hyperparameters were tuned individually for each dataset compared to our work which shared almost all hyperpa-rameters between PTB and WT2, including the embeddingand hidden size for both data sets. Due to this, they usedless model parameters than our model and found shallowLSTMs of one or two layers worked best for WT2.Like our work, Melis et al. (2017) find that the underly-ing LSTM architecture can be highly effective comparedto complex custom architectures when well tuned hyperpa-rameters are used. The approaches used in our work andMelis et al. (2017) may be complementary and would beworth exploration.7. Pointer modelsIn past work, pointer based attention models have beenshown to be highly effective in improving language mod-eling (Merity et al., 2016; Grave et al., 2016). Given suchRegularizing and Optimizing LSTM Language ModelsModel Parameters Validation TestMikolov & Zweig (2012) - KN-5 2M\u2021 \u2212 141.2Mikolov & Zweig (2012) - KN5 + cache 2M\u2021 \u2212 125.7Mikolov & Zweig (2012) - RNN 6M\u2021 \u2212 124.7Mikolov & Zweig (2012) - RNN-LDA 7M\u2021 \u2212 113.7Mikolov & Zweig (2012) - RNN-LDA + KN-5 + cache 9M\u2021 \u2212 92.0Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4Gal & Ghahramani (2016) - Variational LSTM (medium) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1Gal & Ghahramani (2016) - Variational LSTM (medium, MC) 20M \u2212 78.6\u00b1 0.1Gal & Ghahramani (2016) - Variational LSTM (large) 66M 77.9\u00b1 0.3 75.2\u00b1 0.2Gal & Ghahramani (2016) - Variational LSTM (large, MC) 66M \u2212 73.4\u00b1 0.0Kim et al. (2016) - CharCNN 19M \u2212 78.9Merity et al. (2016) - Pointer Sentinel-LSTM 21M 72.4 70.9Grave et al. (2016) - LSTM \u2212 \u2212 82.3Grave et al. (2016) - LSTM + continuous cache pointer \u2212 \u2212 72.1Inan et al. (2016) - Variational LSTM (tied) + augmented loss 24M 75.7 73.2Inan et al. (2016) - Variational LSTM (tied) + augmented loss 51M 71.1 68.5Zilly et al. (2016) - Variational RHN (tied) 23M 67.9 65.4Zoph & Le (2016) - NAS Cell (tied) 25M \u2212 64.0Zoph & Le (2016) - NAS Cell (tied) 54M \u2212 62.4Melis et al. (2017) - 4-layer skip connection LSTM (tied) 24M 60.9 58.3AWD-LSTM - 3-layer LSTM (tied) 24M 60.0 57.3AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer 24M 53.9 52.8Table 1. Single model perplexity on validation and test sets for the Penn Treebank language modeling task. Parameter numbers with \u2021are estimates based upon our understanding of the model and with reference to Merity et al. (2016). Models noting tied use weight tyingon the embedding and softmax weights. Our model, AWD-LSTM, stands for ASGD Weight-Dropped LSTM.Model Parameters Validation TestInan et al. (2016) - Variational LSTM (tied) (h = 650) 28M 92.3 87.7Inan et al. (2016) - Variational LSTM (tied) (h = 650) + augmented loss 28M 91.5 87.0Grave et al. (2016) - LSTM \u2212 \u2212 99.3Grave et al. (2016) - LSTM + continuous cache pointer \u2212 \u2212 68.9Melis et al. (2017) - 1-layer LSTM (tied) 24M 69.3 65.9Melis et al. (2017) - 2-layer skip connection LSTM (tied) 24M 69.1 65.9AWD-LSTM - 3-layer LSTM (tied) 33M 68.6 65.8AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer 33M 53.8 52.0Table 2. Single model perplexity over WikiText-2. Models noting tied use weight tying on the embedding and softmax weights. Ourmodel, AWD-LSTM, stands for ASGD Weight-Dropped LSTM.Regularizing and Optimizing LSTM Language Modelssubstantial improvements to the underlying neural lan-guage model, it remained an open question as to how ef-fective pointer augmentation may be, especially when im-provements such as weight tying may act in mutually ex-clusive ways.The neural cache model (Grave et al., 2016) can be addedon top of a pre-trained language model at negligible cost.The neural cache stores the previous hidden states in mem-ory cells and then uses a simple convex combination ofthe probability distributions suggested by the cache and thelanguage model for prediction. The cache model has threehyperparameters: the memory size (window) for the cache,the coefficient of the combination (which determines howthe two distributions are mixed), and the flatness of thecache distribution. All of these are tuned on the validationset once a trained language model has been obtained andrequire no training by themselves, making it quite inexpen-sive to use. The tuned values for these hyperparameterswere (2000, 0.1, 1.0) for PTB and (3785, 0.1279, 0.662)for WT2 respectively.In Tables 1 and 2, we show that the model further improvesthe perplexity of the language model by as much as 6 per-plexity points for PTB and 11 points for WT2. While thisis smaller than the gains reported in Grave et al. (2016),which used an LSTM without weight tying, this is still asubstantial drop. Given the simplicity of the neural cachemodel, and the lack of any trained components, these re-sults suggest that existing neural language models remainfundamentally lacking, failing to capture long term depen-dencies or remember recently seen words effectively.To understand the impact the pointer had on the model,specifically the validation set perplexity, we detail the con-tribution that each word has on the cache model\u2019s overallperplexity in Table 3. We compute the sum of the total dif-ference in the loss function value (i.e., log perplexity) be-tween the LSTM-only and LSTM-with-cache models forthe target words in the validation portion of the WikiText-2data set. We present results for the sum of the difference asopposed to the mean since the latter undesirably overem-phasizes infrequently occurring words for which the cachehelps significantly and ignores frequently occurring wordsfor which the cache provides modest improvements that cu-mulatively make a strong contribution.The largest cumulative gain is in improving the handlingof <unk> tokens, though this is over 11540 instances. Thesecond best improvement, approximately one fifth the gaingiven by the <unk> tokens, is for Meridian, yet this wordonly occurs 161 times. This indicates the cache still helpssignificantly even for relatively rare words, further demon-strated by Churchill, Blythe, or Sonic. The cache is notbeneficial when handling frequent word categories, such aspunctuation or stop words, for which the language model isWord Count \u2206loss Word Count \u2206loss. 7632 -696.45 <unk> 11540 5047.34, 9857 -687.49 Meridian 161 1057.78of 5816 -365.21 Churchill 137 849.43= 2884 -342.01 - 67 682.15to 4048 -283.10 Blythe 97 554.95in 4178 -222.94 Sonic 75 543.85<eos> 3690 -216.42 Richmond 101 429.18and 5251 -215.38 Starr 74 416.52the 12481 -209.97 Australian 234 366.36a 3381 -149.78 Pagan 54 365.19\" 2540 -127.99 Asahi 39 316.24that 1365 -118.09 Japanese 181 295.97by 1252 -113.05 Hu 43 285.58was 2279 -107.95 Hedgehog 29 266.48) 1101 -94.74 Burma 35 263.65with 1176 -93.01 29 92 260.88for 1215 -87.68 Mississippi 72 241.59on 1485 -81.55 German 108 241.23as 1338 -77.05 mill 67 237.76at 879 -59.86 Cooke 33 231.11Table 3. The sum total difference in loss (log perplexity) that agiven word results in over all instances in the validation data setof WikiText-2 when the continuous cache pointer is introduced.The right column contains the words with the twenty best im-provements (i.e., where the cache was advantageous), and the leftcolumn the twenty most deteriorated (i.e., where the cache wasdisadvantageous).likely well suited. These observations motivate the designof a cache framework that is more aware of the relativestrengths of the two models.8. Model Ablation AnalysisIn Table 4, we present the values of validation and test-ing perplexity for different variants of our best-performingLSTM model. Each variant removes a form of optimizationor regularization.The first two variants deal with the optimization of the lan-guage models while the rest deal with the regularization.For the model using SGD with learning rate reduced by 2using the same nonmonotonic fashion, there is a signifi-cant degradation in performance. This stands as empiricalevidence regarding the benefit of averaging of the iterates.Using a monotonic criterion instead also hampered perfor-mance. Similarly, the removal of the fine-tuning step ex-pectedly also degrades the performance. This step helpsimprove the estimate of the minimizer by resetting thememory of the previous experiment. While this process offine-tuning can be repeated multiple times, we found littlebenefit in repeating it more than once.The removal of regularization strategies paints a similarpicture; the inclusion of all of the proposed strategiesRegularizing and Optimizing LSTM Language ModelsPTB WT2Model Validation Test Validation TestAWD-LSTM (tied) 60.0 57.3 68.6 65.8\u2013 fine-tuning 60.7 58.8 69.1 66.0\u2013 NT-ASGD 66.3 63.7 73.3 69.7\u2013 variable sequence lengths 61.3 58.9 69.3 66.2\u2013 embedding dropout 65.1 62.7 71.1 68.1\u2013 weight decay 63.7 61.0 71.9 68.7\u2013 AR/TAR 62.7 60.3 73.2 70.1\u2013 full sized embedding 68.0 65.6 73.7 70.7\u2013 weight-dropping 71.1 68.9 78.4 74.9Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.was pivotal in ensuring state-of-the-art performance. Themost extreme perplexity jump was in removing the hidden-to-hidden LSTM regularization provided by the weight-dropped LSTM. Without such hidden-to-hidden regular-ization, perplexity rises substantially, up to 11 points.This is in line with previous work showing the neces-sity of recurrent regularization in state-of-the-art models(Gal & Ghahramani, 2016; Inan et al., 2016).We also experiment with static sequence lengths which wehad hypothesized would lead to inefficient data usage. Thisalso worsens the performance by approximately one per-plexity unit. Next, we experiment with reverting to match-ing the sizes of the embedding vectors and the hiddenstates. This significantly increases the number of param-eters in the network (to 43M in the case of PTB and 70Mfor WT2) and leads to degradation by almost 8 perplexitypoints, which we attribute to overfitting in the word em-beddings. While this could potentially be improved withmore aggressive regularization, the computational over-head involved with substantially larger embeddings likelyoutweighs any advantages. Finally, we experiment with theremoval of embedding dropout, AR/TAR and weight decay.In all of the cases, the model suffers a perplexity increaseof 2\u20136 points which we hypothesize is due to insufficientregularization in the network.9. ConclusionIn this work, we discuss regularization and optimizationstrategies for neural language models. We propose theweight-dropped LSTM, a strategy that uses a DropConnectmask on the hidden-to-hidden weight matrices, as a meansto prevent overfitting across the recurrent connections. Fur-ther, we investigate the use of averaged SGD with a non-monontonic trigger for training language models and showthat it outperforms SGD by a significant margin. We in-vestigate other regularization strategies including the useof variable BPTT length and achieve a new state-of-the-artperplexity on the PTB and WikiText-2 data sets. Our mod-els outperform custom-built RNN cells and complex reg-ularization strategies that preclude the possibility of usingoptimized libraries such as the NVIDIA cuDNN LSTM.Finally, we explore the use of a neural cache in conjunc-tion with our proposed model and show that this furtherimproves the performance, thus attaining an even lowerstate-of-the-art perplexity. While the regularization and op-timization strategies proposed are demonstrated on the taskof language modeling, we anticipate that they would begenerally applicable across other sequence learning tasks.ReferencesArjovsky, M., Shah, A., and Bengio, Y. Unitary evolutionrecurrent neural networks. In International Conferenceon Machine Learning, pp. 1120\u20131128, 2016.Ba, J., Kiros, J., and Hinton, G. E. Layer normalization.CoRR, abs/1607.06450, 2016.Balduzzi, D. and Ghifary, M. Strongly-typed recurrent neu-ral networks. arXiv preprint arXiv:1602.02218, 2016.Bottou, L., Curtis, F. E., and Nocedal, J. Optimizationmethods for large-scale machine learning. arXiv preprintarXiv:1606.04838, 2016.Bradbury, J., Merity, S., Xiong, C., and Socher, R.Quasi-Recurrent Neural Networks. arXiv preprintarXiv:1611.01576, 2016.Cooijmans, T., Ballas, N., Laurent, C., and Courville, A. C.Recurrent batch normalization. CoRR, abs/1603.09025,2016.Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradientmethods for online learning and stochastic optimization.Regularizing and Optimizing LSTM Language ModelsJournal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.F\u00f6ldi\u00e1k, P. Learning invariance from transformation se-quences. Neural Computation, 3(2):194\u2013200, 1991.Gal, Y. and Ghahramani, Z. A theoretically grounded appli-cation of dropout in recurrent neural networks. In NIPS,2016.Goyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P.,Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He,K. Accurate, large minibatch sgd: Training imagenet in1 hour. arXiv preprint arXiv:1706.02677, 2017.Grave, E., Joulin, A., and Usunier, N. Improving neurallanguage models with a continuous cache. arXiv preprintarXiv:1612.04426, 2016.Hardt, M., Recht, B., and Singer, Y. Train faster, generalizebetter: Stability of stochastic gradient descent. arXivpreprint arXiv:1509.01240, 2015.Hinton, G. E. Connectionist learning procedures. Artificialintelligence, 40(1-3):185\u2013234, 1989.Inan, H., Khosravi, K., and Socher, R. Tying Word Vectorsand Word Classifiers: A Loss Framework for LanguageModeling. arXiv preprint arXiv:1611.01462, 2016.Ioffe, S. and Szegedy, C. Batch normalization: Accelerat-ing deep network training by reducing internal covariateshift. In ICML, 2015.Jing, L., Shen, Y., Dubc\u030cek, T., Peurifoy, J., Skirlo, S.,Tegmark, M., and Soljac\u030cic\u0301, M. Tunable Efficient Uni-tary Neural Networks (EUNN) and their application toRNN. arXiv preprint arXiv:1612.05231, 2016.Jonschkowski, R. and Brock, O. Learning state represen-tations with robotic priors. Auton. Robots, 39:407\u2013428,2015.Keskar, N. and Saon, G. A nonmonotone learning ratestrategy for sgd training of deep neural networks. InAcoustics, Speech and Signal Processing (ICASSP),2015 IEEE International Conference on, pp. 4974\u20134978.IEEE, 2015.Kim, Y., Jernite, Y., Sontag, D., and Rush, A. M. Character-aware neural language models. In Thirtieth AAAI Con-ference on Artificial Intelligence, 2016.Kingma, D. and Ba, J. Adam: A method for stochasticoptimization. arXiv preprint arXiv:1412.6980, 2014.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C.,Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst,E. Moses: Open source toolkit for statistical machinetranslation. In ACL, 2007.Krueger, D., Maharaj, T., Kram\u00e1r, J., Pezeshki, M., Bal-las, N., Ke, N., Goyal, A., Bengio, Y., Larochelle, H.,Courville, A., et al. Zoneout: Regularizing RNNss byrandomly preserving hidden activations. arXiv preprintarXiv:1606.01305, 2016.Luciw, M. and Schmidhuber, J. Low complexity proto-value function learning from sensory observations withincremental slow feature analysis. Artificial Neural Net-works and Machine Learning\u2013ICANN 2012, pp. 279\u2013287, 2012.Mandt, S., Hoffman, M. D., and Blei, D. M. Stochastic gra-dient descent as approximate bayesian inference. arXivpreprint arXiv:1704.04289, 2017.Melis, G., Dyer, C., and Blunsom, P. On the State of theArt of Evaluation in Neural Language Models. arXivpreprint arXiv:1707.05589, 2017.Merity, S., Xiong, C., Bradbury, J., and Socher, R.Pointer Sentinel Mixture Models. arXiv preprintarXiv:1609.07843, 2016.Merity, S., McCann, B., and Socher, R. Revisiting acti-vation regularization for language rnns. arXiv preprintarXiv:1708.01009, 2017.Mikolov, T. and Zweig, G. Context dependent recurrentneural network language model. SLT, 12:234\u2013239, 2012.Mikolov, T., Karafi\u00e1t, M., Burget, L., Cernock\u00fd, J., andKhudanpur, S. Recurrent neural network based languagemodel. In INTERSPEECH, 2010.Panageas, I. and Piliouras, G. Gradient descent convergesto minimizers: The case of non-isolated critical points.CoRR, abs/1605.00405, 2016.Polyak, B. and Juditsky, A. Acceleration of stochastic ap-proximation by averaging. SIAM Journal on Control andOptimization, 30(4):838\u2013855, 1992.Press, O. and Wolf, L. Using the output embed-ding to improve language models. arXiv preprintarXiv:1608.05859, 2016.Semeniuta, S., Severyn, A., and Barth, E. Recurrentdropout without memory loss. In COLING, 2016.Seo, M., Min, S., Farhadi, A., and Hajishirzi, H. Query-Reduction Networks for Question Answering. arXivpreprint arXiv:1606.04582, 2016.Regularizing and Optimizing LSTM Language ModelsSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,and Salakhutdinov, R. Dropout: a simple way to preventneural networks from overfitting. Journal of MachineLearning Research, 15:1929\u20131958, 2014.Sutskever, I., Martens, J., Dahl, G., and Hinton, G. Onthe importance of initialization and momentum in deeplearning. In International conference on machine learn-ing, pp. 1139\u20131147, 2013.Tallec, C. and Ollivier, Y. Unbiasing truncated backprop-agation through time. arXiv preprint arXiv:1705.08209,2017.Tieleman, T. and Hinton, G. Lecture 6.5-rmsprop: Dividethe gradient by a running average of its recent magni-tude. COURSERA: Neural networks for machine learn-ing, 4(2):26\u201331, 2012.Wan, L., Zeiler, M., Zhang, S., LeCun, Y, and Fergus, R.Regularization of neural networks using dropconnect. InProceedings of the 30th international conference on ma-chine learning (ICML-13), pp. 1058\u20131066, 2013.Wilson, A. C, Roelofs, R., Stern, M., Srebro, N., and Recht,B. The marginal value of adaptive gradient methodsin machine learning. arXiv preprint arXiv:1705.08292,2017.Wisdom, S., Powers, T., Hershey, J., Le Roux, J., and Atlas,L. Full-capacity unitary recurrent neural networks. InAdvances in Neural Information Processing Systems, pp.4880\u20134888, 2016.Zaremba, W., Sutskever, I., and Vinyals, O. Recur-rent neural network regularization. arXiv preprintarXiv:1409.2329, 2014.Zilly, J. G., Srivastava, R. K., Koutn\u00edk, J., and Schmid-huber, J. Recurrent highway networks. arXiv preprintarXiv:1607.03474, 2016.Zoph, B. and Le, Q. V. Neural architecture search with re-inforcement learning. arXiv preprint arXiv:1611.01578,2016."