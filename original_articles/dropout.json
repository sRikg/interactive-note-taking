"Journal of Machine Learning Research 15 (2014) 1929-1958 Submitted 11/13; Published 6/14Dropout: A Simple Way to Prevent Neural Networks fromOverfittingNitish Srivastava nitish@cs.toronto.eduGeoffrey Hinton hinton@cs.toronto.eduAlex Krizhevsky kriz@cs.toronto.eduIlya Sutskever ilya@cs.toronto.eduRuslan Salakhutdinov rsalakhu@cs.toronto.eduDepartment of Computer ScienceUniversity of Toronto10 Kings College Road, Rm 3302Toronto, Ontario, M5S 3G4, Canada.Editor: Yoshua BengioAbstractDeep neural nets with a large number of parameters are very powerful machine learningsystems. However, overfitting is a serious problem in such networks. Large networks are alsoslow to use, making it difficult to deal with overfitting by combining the predictions of manydifferent large neural nets at test time. Dropout is a technique for addressing this problem.The key idea is to randomly drop units (along with their connections) from the neuralnetwork during training. This prevents units from co-adapting too much. During training,dropout samples from an exponential number of different \u201cthinned\u201d networks. At test time,it is easy to approximate the effect of averaging the predictions of all these thinned networksby simply using a single unthinned network that has smaller weights. This significantlyreduces overfitting and gives major improvements over other regularization methods. Weshow that dropout improves the performance of neural networks on supervised learningtasks in vision, speech recognition, document classification and computational biology,obtaining state-of-the-art results on many benchmark data sets.Keywords: neural networks, regularization, model combination, deep learning1. IntroductionDeep neural networks contain multiple non-linear hidden layers and this makes them veryexpressive models that can learn very complicated relationships between their inputs andoutputs. With limited training data, however, many of these complicated relationshipswill be the result of sampling noise, so they will exist in the training set but not in realtest data even if it is drawn from the same distribution. This leads to overfitting and manymethods have been developed for reducing it. These include stopping the training as soon asperformance on a validation set starts to get worse, introducing weight penalties of variouskinds such as L1 and L2 regularization and soft weight sharing (Nowlan and Hinton, 1992).With unlimited computation, the best way to \u201cregularize\u201d a fixed-sized model is toaverage the predictions of all possible settings of the parameters, weighting each setting byc\u00a92014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov(a) Standard Neural Net (b) After applying dropout.Figure 1: Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right:An example of a thinned net produced by applying dropout to the network on the left.Crossed units have been dropped.its posterior probability given the training data. This can sometimes be approximated quitewell for simple or small models (Xiong et al., 2011; Salakhutdinov and Mnih, 2008), but wewould like to approach the performance of the Bayesian gold standard using considerablyless computation. We propose to do this by approximating an equally weighted geometricmean of the predictions of an exponential number of learned models that share parameters.Model combination nearly always improves the performance of machine learning meth-ods. With large neural networks, however, the obvious idea of averaging the outputs ofmany separately trained nets is prohibitively expensive. Combining several models is mosthelpful when the individual models are different from each other and in order to makeneural net models different, they should either have different architectures or be trainedon different data. Training many different architectures is hard because finding optimalhyperparameters for each architecture is a daunting task and training each large networkrequires a lot of computation. Moreover, large networks normally require large amounts oftraining data and there may not be enough data available to train different networks ondifferent subsets of the data. Even if one was able to train many different large networks,using them all at test time is infeasible in applications where it is important to respondquickly.Dropout is a technique that addresses both these issues. It prevents overfitting andprovides a way of approximately combining exponentially many different neural networkarchitectures efficiently. The term \u201cdropout\u201d refers to dropping out units (hidden andvisible) in a neural network. By dropping a unit out, we mean temporarily removing it fromthe network, along with all its incoming and outgoing connections, as shown in Figure 1.The choice of which units to drop is random. In the simplest case, each unit is retained witha fixed probability p independent of other units, where p can be chosen using a validationset or can simply be set at 0.5, which seems to be close to optimal for a wide range ofnetworks and tasks. For the input units, however, the optimal probability of retention isusually closer to 1 than to 0.5.1930DropoutPresent withprobability pw-(a) At training timeAlwayspresentpw-(b) At test timeFigure 2: Left: A unit at training time that is present with probability p and is connected to unitsin the next layer with weights w. Right: At test time, the unit is always present andthe weights are multiplied by p. The output at test time is same as the expected outputat training time.Applying dropout to a neural network amounts to sampling a \u201cthinned\u201d network fromit. The thinned network consists of all the units that survived dropout (Figure 1b). Aneural net with n units, can be seen as a collection of 2n possible thinned neural networks.These networks all share weights so that the total number of parameters is still O(n2), orless. For each presentation of each training case, a new thinned network is sampled andtrained. So training a neural network with dropout can be seen as training a collection of 2nthinned networks with extensive weight sharing, where each thinned network gets trainedvery rarely, if at all.At test time, it is not feasible to explicitly average the predictions from exponentiallymany thinned models. However, a very simple approximate averaging method works well inpractice. The idea is to use a single neural net at test time without dropout. The weightsof this network are scaled-down versions of the trained weights. If a unit is retained withprobability p during training, the outgoing weights of that unit are multiplied by p at testtime as shown in Figure 2. This ensures that for any hidden unit the expected output (underthe distribution used to drop units at training time) is the same as the actual output attest time. By doing this scaling, 2n networks with shared weights can be combined intoa single neural network to be used at test time. We found that training a network withdropout and using this approximate averaging method at test time leads to significantlylower generalization error on a wide variety of classification problems compared to trainingwith other regularization methods.The idea of dropout is not limited to feed-forward neural nets. It can be more generallyapplied to graphical models such as Boltzmann Machines. In this paper, we introducethe dropout Restricted Boltzmann Machine model and compare it to standard RestrictedBoltzmann Machines (RBM). Our experiments show that dropout RBMs are better thanstandard RBMs in certain respects.This paper is structured as follows. Section 2 describes the motivation for this idea.Section 3 describes relevant previous work. Section 4 formally describes the dropout model.Section 5 gives an algorithm for training dropout networks. In Section 6, we present ourexperimental results where we apply dropout to problems in different domains and compareit with other forms of regularization and model combination. Section 7 analyzes the effect ofdropout on different properties of a neural network and describes how dropout interacts withthe network\u2019s hyperparameters. Section 8 describes the Dropout RBM model. In Section 9we explore the idea of marginalizing dropout. In Appendix A we present a practical guide1931Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinovfor training dropout nets. This includes a detailed analysis of the practical considerationsinvolved in choosing hyperparameters when training dropout networks.2. MotivationA motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al.,2010). Sexual reproduction involves taking half the genes of one parent and half of theother, adding a very small amount of random mutation, and combining them to produce anoffspring. The asexual alternative is to create an offspring with a slightly mutated copy ofthe parent\u2019s genes. It seems plausible that asexual reproduction should be a better way tooptimize individual fitness because a good set of genes that have come to work well togethercan be passed on directly to the offspring. On the other hand, sexual reproduction is likelyto break up these co-adapted sets of genes, especially if these sets are large and, intuitively,this should decrease the fitness of organisms that have already evolved complicated co-adaptations. However, sexual reproduction is the way most advanced organisms evolved.One possible explanation for the superiority of sexual reproduction is that, over the longterm, the criterion for natural selection may not be individual fitness but rather mix-abilityof genes. The ability of a set of genes to be able to work well with another random set ofgenes makes them more robust. Since a gene cannot rely on a large set of partners to bepresent at all times, it must learn to do something useful on its own or in collaboration witha small number of other genes. According to this theory, the role of sexual reproductionis not just to allow useful new genes to spread throughout the population, but also tofacilitate this process by reducing complex co-adaptations that would reduce the chance ofa new gene improving the fitness of an individual. Similarly, each hidden unit in a neuralnetwork trained with dropout must learn to work with a randomly chosen sample of otherunits. This should make each hidden unit more robust and drive it towards creating usefulfeatures on its own without relying on other hidden units to correct its mistakes. However,the hidden units within a layer will still learn to do different things from each other. Onemight imagine that the net would become robust against dropout by making many copiesof each hidden unit, but this is a poor solution for exactly the same reason as replica codesare a poor way to deal with a noisy channel.A closely related, but slightly different motivation for dropout comes from thinkingabout successful conspiracies. Ten conspiracies each involving five people is probably abetter way to create havoc than one big conspiracy that requires fifty people to all playtheir parts correctly. If conditions do not change and there is plenty of time for rehearsal, abig conspiracy can work well, but with non-stationary conditions, the smaller the conspiracythe greater its chance of still working. Complex co-adaptations can be trained to work wellon a training set, but on novel test data they are far more likely to fail than multiple simplerco-adaptations that achieve the same thing.3. Related WorkDropout can be interpreted as a way of regularizing a neural network by adding noise toits hidden units. The idea of adding noise to the states of units has previously been used inthe context of Denoising Autoencoders (DAEs) by Vincent et al. (2008, 2010) where noise1932Dropoutis added to the input units of an autoencoder and the network is trained to reconstruct thenoise-free input. Our work extends this idea by showing that dropout can be effectivelyapplied in the hidden layers as well and that it can be interpreted as a form of modelaveraging. We also show that adding noise is not only useful for unsupervised featurelearning but can also be extended to supervised learning problems. In fact, our method canbe applied to other neuron-based architectures, for example, Boltzmann Machines. While5% noise typically works best for DAEs, we found that our weight scaling procedure appliedat test time enables us to use much higher noise levels. Dropping out 20% of the input unitsand 50% of the hidden units was often found to be optimal.Since dropout can be seen as a stochastic regularization technique, it is natural toconsider its deterministic counterpart which is obtained by marginalizing out the noise. Inthis paper, we show that, in simple cases, dropout can be analytically marginalized outto obtain deterministic regularization methods. Recently, van der Maaten et al. (2013)also explored deterministic regularizers corresponding to different exponential-family noisedistributions, including dropout (which they refer to as \u201cblankout noise\u201d). However, theyapply noise to the inputs and only explore models with no hidden layers. Wang and Manning(2013) proposed a method for speeding up dropout by marginalizing dropout noise. Chenet al. (2012) explored marginalization in the context of denoising autoencoders.In dropout, we minimize the loss function stochastically under a noise distribution.This can be seen as minimizing an expected loss function. Previous work of Globerson andRoweis (2006); Dekel et al. (2010) explored an alternate setting where the loss is minimizedwhen an adversary gets to pick which units to drop. Here, instead of a noise distribution,the maximum number of units that can be dropped is fixed. However, this work also doesnot explore models with hidden units.4. Model DescriptionThis section describes the dropout neural network model. Consider a neural network withL hidden layers. Let l \u2208 {1, . . . , L} index the hidden layers of the network. Let z(l) denotethe vector of inputs into layer l, y(l) denote the vector of outputs from layer l (y(0) = x isthe input). W (l) and b(l) are the weights and biases at layer l. The feed-forward operationof a standard neural network (Figure 3a) can be described as (for l \u2208 {0, . . . , L \u2212 1} andany hidden unit i)z(l+1)i = w(l+1)i yl + b(l+1)i ,y(l+1)i = f(z(l+1)i ),where f is any activation function, for example, f(x) = 1/ (1 + exp(\u2212x)).With dropout, the feed-forward operation becomes (Figure 3b)r(l)j \u223c Bernoulli(p),y\u0303(l) = r(l) \u2217 y(l),z(l+1)i = w(l+1)i y\u0303l + b(l+1)i ,y(l+1)i = f(z(l+1)i ).1933Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov+1y(l)1y(l)2y(l)3z(l+1)i y(l+1)iw(l+1)ib(l+1)if(a) Standard network+1y\u0303(l)1y\u0303(l)2y\u0303(l)3z(l+1)i y(l+1)iy(l)1y(l)2y(l)3r(l)1r(l)2r(l)3w(l+1)ib(l+1)if(b) Dropout networkFigure 3: Comparison of the basic operations of a standard and dropout network.Here \u2217 denotes an element-wise product. For any layer l, r(l) is a vector of independentBernoulli random variables each of which has probability p of being 1. This vector issampled and multiplied element-wise with the outputs of that layer, y(l), to create thethinned outputs y\u0303(l). The thinned outputs are then used as input to the next layer. Thisprocess is applied at each layer. This amounts to sampling a sub-network from a largernetwork. For learning, the derivatives of the loss function are backpropagated through thesub-network. At test time, the weights are scaled as W(l)test = pW(l) as shown in Figure 2.The resulting neural network is used without dropout.5. Learning Dropout NetsThis section describes a procedure for training dropout neural nets.5.1 BackpropagationDropout neural networks can be trained using stochastic gradient descent in a manner simi-lar to standard neural nets. The only difference is that for each training case in a mini-batch,we sample a thinned network by dropping out units. Forward and backpropagation for thattraining case are done only on this thinned network. The gradients for each parameter areaveraged over the training cases in each mini-batch. Any training case which does not use aparameter contributes a gradient of zero for that parameter. Many methods have been usedto improve stochastic gradient descent such as momentum, annealed learning rates and L2weight decay. Those were found to be useful for dropout neural networks as well.One particular form of regularization was found to be especially useful for dropout\u2014constraining the norm of the incoming weight vector at each hidden unit to be upperbounded by a fixed constant c. In other words, if w represents the vector of weights incidenton any hidden unit, the neural network was optimized under the constraint ||w||2 \u2264 c. Thisconstraint was imposed during optimization by projecting w onto the surface of a ball ofradius c, whenever w went out of it. This is also called max-norm regularization since itimplies that the maximum value that the norm of any weight can take is c. The constant1934Dropoutc is a tunable hyperparameter, which is determined using a validation set. Max-normregularization has been previously used in the context of collaborative filtering (Srebro andShraibman, 2005). It typically improves the performance of stochastic gradient descenttraining of deep neural nets, even when no dropout is used.Although dropout alone gives significant improvements, using dropout along with max-norm regularization, large decaying learning rates and high momentum provides a significantboost over just using dropout. A possible justification is that constraining weight vectorsto lie inside a ball of fixed radius makes it possible to use a huge learning rate without thepossibility of weights blowing up. The noise provided by dropout then allows the optimiza-tion process to explore different regions of the weight space that would have otherwise beendifficult to reach. As the learning rate decays, the optimization takes shorter steps, therebydoing less exploration and eventually settles into a minimum.5.2 Unsupervised PretrainingNeural networks can be pretrained using stacks of RBMs (Hinton and Salakhutdinov, 2006),autoencoders (Vincent et al., 2010) or Deep Boltzmann Machines (Salakhutdinov and Hin-ton, 2009). Pretraining is an effective way of making use of unlabeled data. Pretrainingfollowed by finetuning with backpropagation has been shown to give significant performanceboosts over finetuning from random initializations in certain cases.Dropout can be applied to finetune nets that have been pretrained using these tech-niques. The pretraining procedure stays the same. The weights obtained from pretrainingshould be scaled up by a factor of 1/p. This makes sure that for each unit, the expectedoutput from it under random dropout will be the same as the output during pretraining.We were initially concerned that the stochastic nature of dropout might wipe out the in-formation in the pretrained weights. This did happen when the learning rates used duringfinetuning were comparable to the best learning rates for randomly initialized nets. How-ever, when the learning rates were chosen to be smaller, the information in the pretrainedweights seemed to be retained and we were able to get improvements in terms of the finalgeneralization error compared to not using dropout when finetuning.6. Experimental ResultsWe trained dropout neural networks for classification problems on data sets in differentdomains. We found that dropout improved generalization performance on all data setscompared to neural networks that did not use dropout. Table 1 gives a brief description ofthe data sets. The data sets are\u2022 MNIST : A standard toy data set of handwritten digits.\u2022 TIMIT : A standard speech benchmark for clean speech recognition.\u2022 CIFAR-10 and CIFAR-100 : Tiny natural images (Krizhevsky, 2009).\u2022 Street View House Numbers data set (SVHN) : Images of house numbers collected byGoogle Street View (Netzer et al., 2011).\u2022 ImageNet : A large collection of natural images.\u2022 Reuters-RCV1 : A collection of Reuters newswire articles.1935Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov\u2022 Alternative Splicing data set: RNA features for predicting alternative gene splicing(Xiong et al., 2011).We chose a diverse set of data sets to demonstrate that dropout is a general techniquefor improving neural nets and is not specific to any particular application domain. In thissection, we present some key results that show the effectiveness of dropout. A more detaileddescription of all the experiments and data sets is provided in Appendix B.Data Set Domain Dimensionality Training Set Test SetMNIST Vision 784 (28 \u00d7 28 grayscale) 60K 10KSVHN Vision 3072 (32 \u00d7 32 color) 600K 26KCIFAR-10/100 Vision 3072 (32 \u00d7 32 color) 60K 10KImageNet (ILSVRC-2012) Vision 65536 (256 \u00d7 256 color) 1.2M 150KTIMIT Speech 2520 (120-dim, 21 frames) 1.1M frames 58K framesReuters-RCV1 Text 2000 200K 200KAlternative Splicing Genetics 1014 2932 733Table 1: Overview of the data sets used in this paper.6.1 Results on Image Data SetsWe used five image data sets to evaluate dropout\u2014MNIST, SVHN, CIFAR-10, CIFAR-100and ImageNet. These data sets include different image types and training set sizes. Modelswhich achieve state-of-the-art results on all of these data sets use dropout.6.1.1 MNISTMethodUnitTypeArchitectureError%Standard Neural Net (Simard et al., 2003) Logistic 2 layers, 800 units 1.60SVM Gaussian kernel NA NA 1.40Dropout NN Logistic 3 layers, 1024 units 1.35Dropout NN ReLU 3 layers, 1024 units 1.25Dropout NN + max-norm constraint ReLU 3 layers, 1024 units 1.06Dropout NN + max-norm constraint ReLU 3 layers, 2048 units 1.04Dropout NN + max-norm constraint ReLU 2 layers, 4096 units 1.01Dropout NN + max-norm constraint ReLU 2 layers, 8192 units 0.95Dropout NN + max-norm constraint (Goodfellowet al., 2013)Maxout2 layers, (5 \u00d7 240)units0.94DBN + finetuning (Hinton and Salakhutdinov, 2006) Logistic 500-500-2000 1.18DBM + finetuning (Salakhutdinov and Hinton, 2009) Logistic 500-500-2000 0.96DBN + dropout finetuning Logistic 500-500-2000 0.92DBM + dropout finetuning Logistic 500-500-2000 0.79Table 2: Comparison of different models on MNIST.The MNIST data set consists of 28 \u00d7 28 pixel handwritten digit images. The task isto classify the images into 10 digit classes. Table 2 compares the performance of dropoutwith other techniques. The best performing neural networks for the permutation invariant1936Dropoutsetting that do not use dropout or unsupervised pretraining achieve an error of about1.60% (Simard et al., 2003). With dropout the error reduces to 1.35%. Replacing logisticunits with rectified linear units (ReLUs) (Jarrett et al., 2009) further reduces the error to1.25%. Adding max-norm regularization again reduces it to 1.06%. Increasing the size ofthe network leads to better results. A neural net with 2 layers and 8192 units per layergets down to 0.95% error. Note that this network has more than 65 million parameters andis being trained on a data set of size 60,000. Training a network of this size to give goodgeneralization error is very hard with standard regularization methods and early stopping.Dropout, on the other hand, prevents overfitting, even in this case. It does not even needearly stopping. Goodfellow et al. (2013) showed that results can be further improved to0.94% by replacing ReLU units with maxout units. All dropout nets use p = 0.5 for hiddenunits and p = 0.8 for input units. More experimental details can be found in Appendix B.1.Dropout nets pretrained with stacks of RBMs and Deep Boltzmann Machines also giveimprovements as shown in Table 2. DBM\u2014pretrained dropout nets achieve a test error of0.79% which is the best performance ever reported for the permutation invariant setting.We note that it possible to obtain better results by using 2-D spatial information andaugmenting the training set with distorted versions of images from the standard trainingset. We demonstrate the effectiveness of dropout in that setting on more interesting datasets.0 200000 400000 600000 800000 1000000Number of weight updates1.01.52.02.5Classification Error %With dropoutWithout dropout@R@@RFigure 4: Test error for different architectureswith and without dropout. The net-works have 2 to 4 hidden layers eachwith 1024 to 2048 units.In order to test the robustness ofdropout, classification experiments weredone with networks of many different ar-chitectures keeping all hyperparameters, in-cluding p, fixed. Figure 4 shows the testerror rates obtained for these different ar-chitectures as training progresses. Thesame architectures trained with and with-out dropout have drastically different testerrors as seen as by the two separate clus-ters of trajectories. Dropout gives a hugeimprovement across all architectures, with-out using hyperparameters that were tunedspecifically for each architecture.6.1.2 Street View House NumbersThe Street View House Numbers (SVHN)Data Set (Netzer et al., 2011) consists ofcolor images of house numbers collected byGoogle Street View. Figure 5a shows some examples of images from this data set. Thepart of the data set that we use in our experiments consists of 32\u00d7 32 color images roughlycentered on a digit in a house number. The task is to identify that digit.For this data set, we applied dropout to convolutional neural networks (LeCun et al.,1989). The best architecture that we found has three convolutional layers followed by 2fully connected hidden layers. All hidden units were ReLUs. Each convolutional layer was1937Srivastava, Hinton, Krizhevsky, Sutskever and SalakhutdinovMethod Error %Binary Features (WDCH) (Netzer et al., 2011) 36.7HOG (Netzer et al., 2011) 15.0Stacked Sparse Autoencoders (Netzer et al., 2011) 10.3KMeans (Netzer et al., 2011) 9.4Multi-stage Conv Net with average pooling (Sermanet et al., 2012) 9.06Multi-stage Conv Net + L2 pooling (Sermanet et al., 2012) 5.36Multi-stage Conv Net + L4 pooling + padding (Sermanet et al., 2012) 4.90Conv Net + max-pooling 3.95Conv Net + max pooling + dropout in fully connected layers 3.02Conv Net + stochastic pooling (Zeiler and Fergus, 2013) 2.80Conv Net + max pooling + dropout in all layers 2.55Conv Net + maxout (Goodfellow et al., 2013) 2.47Human Performance 2.0Table 3: Results on the Street View House Numbers data set.followed by a max-pooling layer. Appendix B.2 describes the architecture in more detail.Dropout was applied to all the layers of the network with the probability of retaining a hid-den unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the different layers of the network (goingfrom input to convolutional layers to fully connected layers). Max-norm regularization wasused for weights in both convolutional and fully connected layers. Table 3 compares theresults obtained by different methods. We find that convolutional nets outperform othermethods. The best performing convolutional nets that do not use dropout achieve an errorrate of 3.95%. Adding dropout only to the fully connected layers reduces the error to 3.02%.Adding dropout to the convolutional layers as well further reduces the error to 2.55%. Evenmore gains can be obtained by using maxout units.The additional gain in performance obtained by adding dropout in the convolutionallayers (3.02% to 2.55%) is worth noting. One may have presumed that since the convo-lutional layers don\u2019t have a lot of parameters, overfitting is not a problem and thereforedropout would not have much effect. However, dropout in the lower layers still helps be-cause it provides noisy inputs for the higher fully connected layers which prevents themfrom overfitting.6.1.3 CIFAR-10 and CIFAR-100The CIFAR-10 and CIFAR-100 data sets consist of 32 \u00d7 32 color images drawn from 10and 100 categories respectively. Figure 5b shows some examples of images from this dataset. A detailed description of the data sets, input preprocessing, network architectures andother experimental details is given in Appendix B.3. Table 4 shows the error rate obtainedby different methods on these data sets. Without any data augmentation, Snoek et al.(2012) used Bayesian hyperparameter optimization to obtained an error rate of 14.98% onCIFAR-10. Using dropout in the fully connected layers reduces that to 14.32% and addingdropout in every layer further reduces the error to 12.61%. Goodfellow et al. (2013) showedthat the error is further reduced to 11.68% by replacing ReLU units with maxout units. OnCIFAR-100, dropout reduces the error from 43.48% to 37.20% which is a huge improvement.No data augmentation was used for either data set (apart from the input dropout).1938Dropout(a) Street View House Numbers (SVHN) (b) CIFAR-10Figure 5: Samples from image data sets. Each row corresponds to a different category.Method CIFAR-10 CIFAR-100Conv Net + max pooling (hand tuned) 15.60 43.48Conv Net + stochastic pooling (Zeiler and Fergus, 2013) 15.13 42.51Conv Net + max pooling (Snoek et al., 2012) 14.98 -Conv Net + max pooling + dropout fully connected layers 14.32 41.26Conv Net + max pooling + dropout in all layers 12.61 37.20Conv Net + maxout (Goodfellow et al., 2013) 11.68 38.57Table 4: Error rates on CIFAR-10 and CIFAR-100.6.1.4 ImageNetImageNet is a data set of over 15 million labeled high-resolution images belonging to roughly22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annualcompetition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) hasbeen held. A subset of ImageNet with roughly 1000 images in each of 1000 categories isused in this challenge. Since the number of categories is rather large, it is conventional toreport two error rates: top-1 and top-5, where the top-5 error rate is the fraction of testimages for which the correct label is not among the five labels considered most probable bythe model. Figure 6 shows some predictions made by our model on a few test images.ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, somost of our experiments were performed on this data set. Table 5 compares the performanceof different methods. Convolutional nets with dropout outperform other methods by a largemargin. The architecture and implementation details are described in detail in Krizhevskyet al. (2012).1939Srivastava, Hinton, Krizhevsky, Sutskever and SalakhutdinovFigure 6: Some ImageNet test cases with the 4 most probable labels as predicted by our model.The length of the horizontal bars is proportional to the probability assigned to the labelsby the model. Pink indicates ground truth.Model Top-1 Top-5Sparse Coding (Lin et al., 2010) 47.1 28.2SIFT + Fisher Vectors (Sanchez and Perronnin, 2011) 45.7 25.7Conv Net + dropout (Krizhevsky et al., 2012) 37.5 17.0Table 5: Results on the ILSVRC-2010 test set.ModelTop-1(val)Top-5(val)Top-5(test)SVM on Fisher Vectors of Dense SIFT and Color Statistics - - 27.3Avg of classifiers over FVs of SIFT, LBP, GIST and CSIFT - - 26.2Conv Net + dropout (Krizhevsky et al., 2012) 40.7 18.2 -Avg of 5 Conv Nets + dropout (Krizhevsky et al., 2012) 38.1 16.4 16.4Table 6: Results on the ILSVRC-2012 validation/test set.Our model based on convolutional nets and dropout won the ILSVRC-2012 competition.Since the labels for the test set are not available, we report our results on the test set forthe final submission and include the validation set results for different variations of ourmodel. Table 6 shows the results from the competition. While the best methods based onstandard vision features achieve a top-5 error rate of about 26%, convolutional nets withdropout achieve a test error of about 16% which is a staggering difference. Figure 6 showssome examples of predictions made by our model. We can see that the model makes veryreasonable predictions, even when its best guess is not correct.6.2 Results on TIMITNext, we applied dropout to a speech recognition task. We use the TIMIT data set whichconsists of recordings from 680 speakers covering 8 major dialects of American Englishreading ten phonetically-rich sentences in a controlled noise-free environment. Dropoutneural networks were trained on windows of 21 log-filter bank frames to predict the labelof the central frame. No speaker dependent operations were performed. Appendix B.4describes the data preprocessing and training details. Table 7 compares dropout neural1940Dropoutnets with other models. A 6-layer net gives a phone error rate of 23.4%. Dropout furtherimproves it to 21.8%. We also trained dropout nets starting from pretrained weights. A4-layer net pretrained with a stack of RBMs get a phone error rate of 22.7%. With dropout,this reduces to 19.7%. Similarly, for an 8-layer net the error reduces from 20.5% to 19.7%.Method Phone Error Rate%NN (6 layers) (Mohamed et al., 2010) 23.4Dropout NN (6 layers) 21.8DBN-pretrained NN (4 layers) 22.7DBN-pretrained NN (6 layers) (Mohamed et al., 2010) 22.4DBN-pretrained NN (8 layers) (Mohamed et al., 2010) 20.7mcRBM-DBN-pretrained NN (5 layers) (Dahl et al., 2010) 20.5DBN-pretrained NN (4 layers) + dropout 19.7DBN-pretrained NN (8 layers) + dropout 19.7Table 7: Phone error rate on the TIMIT core test set.6.3 Results on a Text Data SetTo test the usefulness of dropout in the text domain, we used dropout networks to train adocument classifier. We used a subset of the Reuters-RCV1 data set which is a collection ofover 800,000 newswire articles from Reuters. These articles cover a variety of topics. Thetask is to take a bag of words representation of a document and classify it into 50 disjointtopics. Appendix B.5 describes the setup in more detail. Our best neural net which didnot use dropout obtained an error rate of 31.05%. Adding dropout reduced the error to29.62%. We found that the improvement was much smaller compared to that for the visionand speech data sets.6.4 Comparison with Bayesian Neural NetworksDropout can be seen as a way of doing an equally-weighted averaging of exponentially manymodels with shared weights. On the other hand, Bayesian neural networks (Neal, 1996) arethe proper way of doing model averaging over the space of neural network structures andparameters. In dropout, each model is weighted equally, whereas in a Bayesian neuralnetwork each model is weighted taking into account the prior and how well the model fitsthe data, which is the more correct approach. Bayesian neural nets are extremely useful forsolving problems in domains where data is scarce such as medical diagnosis, genetics, drugdiscovery and other computational biology applications. However, Bayesian neural nets areslow to train and difficult to scale to very large network sizes. Besides, it is expensive toget predictions from many large nets at test time. On the other hand, dropout neural netsare much faster to train and use at test time. In this section, we report experiments thatcompare Bayesian neural nets with dropout neural nets on a small data set where Bayesianneural networks are known to perform well and obtain state-of-the-art results. The aim isto analyze how much does dropout lose compared to Bayesian neural nets.The data set that we use (Xiong et al., 2011) comes from the domain of genetics. Thetask is to predict the occurrence of alternative splicing based on RNA features. Alternativesplicing is a significant cause of cellular diversity in mammalian tissues. Predicting the1941Srivastava, Hinton, Krizhevsky, Sutskever and SalakhutdinovMethod Code Quality (bits)Neural Network (early stopping) (Xiong et al., 2011) 440Regression, PCA (Xiong et al., 2011) 463SVM, PCA (Xiong et al., 2011) 487Neural Network with dropout 567Bayesian Neural Network (Xiong et al., 2011) 623Table 8: Results on the Alternative Splicing Data Set.occurrence of alternate splicing in certain tissues under different conditions is important forunderstanding many human diseases. Given the RNA features, the task is to predict theprobability of three splicing related events that biologists care about. The evaluation metricis Code Quality which is a measure of the negative KL divergence between the target andthe predicted probability distributions (higher is better). Appendix B.6 includes a detaileddescription of the data set and this performance metric.Table 8 summarizes the performance of different models on this data set. Xiong et al.(2011) used Bayesian neural nets for this task. As expected, we found that Bayesian neuralnets perform better than dropout. However, we see that dropout improves significantlyupon the performance of standard neural nets and outperforms all other methods. Thechallenge in this data set is to prevent overfitting since the size of the training set is small.One way to prevent overfitting is to reduce the input dimensionality using PCA. Thereafter,standard techniques such as SVMs or logistic regression can be used. However, with dropoutwe were able to prevent overfitting without the need to do dimensionality reduction. Thedropout nets are very large (1000s of hidden units) compared to a few tens of units in theBayesian network. This shows that dropout has a strong regularizing effect.6.5 Comparison with Standard RegularizersSeveral regularization methods have been proposed for preventing overfitting in neural net-works. These include L2 weight decay (more generally Tikhonov regularization (Tikhonov,1943)), lasso (Tibshirani, 1996), KL-sparsity and max-norm regularization. Dropout canbe seen as another way of regularizing neural networks. In this section we compare dropoutwith some of these regularization methods using the MNIST data set.The same network architecture (784-1024-1024-2048-10) with ReLUs was trained us-ing stochastic gradient descent with different regularizations. Table 9 shows the results.The values of different hyperparameters associated with each kind of regularization (decayconstants, target sparsity, dropout rate, max-norm upper bound) were obtained using avalidation set. We found that dropout combined with max-norm regularization gives thelowest generalization error.7. Salient FeaturesThe experiments described in the previous section provide strong evidence that dropoutis a useful technique for improving neural networks. In this section, we closely examinehow dropout affects a neural network. We analyze the effect of dropout on the quality offeatures produced. We see how dropout affects the sparsity of hidden unit activations. We1942DropoutMethod Test Classification error %L2 1.62L2 + L1 applied towards the end of training 1.60L2 + KL-sparsity 1.55Max-norm 1.35Dropout + L2 1.25Dropout + Max-norm 1.05Table 9: Comparison of different regularization methods on MNIST.also see how the advantages obtained from dropout vary with the probability of retainingunits, size of the network and the size of the training set. These observations give someinsight into why dropout works so well.7.1 Effect on Features(a) Without dropout (b) Dropout with p = 0.5.Figure 7: Features learned on MNIST with one hidden layer autoencoders having 256 rectifiedlinear units.In a standard neural network, the derivative received by each parameter tells it how itshould change so the final loss function is reduced, given what all other units are doing.Therefore, units may change in a way that they fix up the mistakes of the other units.This may lead to complex co-adaptations. This in turn leads to overfitting because theseco-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit,dropout prevents co-adaptation by making the presence of other hidden units unreliable.Therefore, a hidden unit cannot rely on other specific units to correct its mistakes. It mustperform well in a wide variety of different contexts provided by the other hidden units. Toobserve this effect directly, we look at the first level features learned by neural networkstrained on visual tasks with and without dropout.1943Srivastava, Hinton, Krizhevsky, Sutskever and SalakhutdinovFigure 7a shows features learned by an autoencoder on MNIST with a single hiddenlayer of 256 rectified linear units without dropout. Figure 7b shows the features learned byan identical autoencoder which used dropout in the hidden layer with p = 0.5. Both au-toencoders had similar test reconstruction errors. However, it is apparent that the featuresshown in Figure 7a have co-adapted in order to produce good reconstructions. Each hiddenunit on its own does not seem to be detecting a meaningful feature. On the other hand, inFigure 7b, the hidden units seem to detect edges, strokes and spots in different parts of theimage. This shows that dropout does break up co-adaptations, which is probably the mainreason why it leads to lower generalization errors.7.2 Effect on Sparsity(a) Without dropout (b) Dropout with p = 0.5.Figure 8: Effect of dropout on sparsity. ReLUs were used for both models. Left: The histogramof mean activations shows that most units have a mean activation of about 2.0. Thehistogram of activations shows a huge mode away from zero. Clearly, a large fraction ofunits have high activation. Right: The histogram of mean activations shows that mostunits have a smaller mean mean activation of about 0.7. The histogram of activationsshows a sharp peak at zero. Very few units have high activation.We found that as a side-effect of doing dropout, the activations of the hidden unitsbecome sparse, even when no sparsity inducing regularizers are present. Thus, dropout au-tomatically leads to sparse representations. To observe this effect, we take the autoencoderstrained in the previous section and look at the sparsity of hidden unit activations on a ran-dom mini-batch taken from the test set. Figure 8a and Figure 8b compare the sparsity forthe two models. In a good sparse model, there should only be a few highly activated unitsfor any data case. Moreover, the average activation of any unit across data cases shouldbe low. To assess both of these qualities, we plot two histograms for each model. For eachmodel, the histogram on the left shows the distribution of mean activations of hidden unitsacross the minibatch. The histogram on the right shows the distribution of activations ofthe hidden units.Comparing the histograms of activations we can see that fewer hidden units have highactivations in Figure 8b compared to Figure 8a, as seen by the significant mass away from1944Dropoutzero for the net that does not use dropout. The mean activations are also smaller for thedropout net. The overall mean activation of hidden units is close to 2.0 for the autoencoderwithout dropout but drops to around 0.7 when dropout is used.7.3 Effect of Dropout RateDropout has a tunable hyperparameter p (the probability of retaining a unit in the network).In this section, we explore the effect of varying this hyperparameter. The comparison isdone in two situations.1. The number of hidden units is held constant.2. The number of hidden units is changed so that the expected number of hidden unitsthat will be retained after dropout is held constant.In the first case, we train the same network architecture with different amounts ofdropout. We use a 784-2048-2048-2048-10 architecture. No input dropout was used. Fig-ure 9a shows the test error obtained as a function of p. If the architecture is held constant,having a small p means very few units will turn on during training. It can be seen that thishas led to underfitting since the training error is also high. We see that as p increases, theerror goes down. It becomes flat when 0.4 \u2264 p \u2264 0.8 and then increases as p becomes closeto 1.0.0 0.2 0.4 0.6 0.8 1.0Probability of retaining a unit (p)0.00.51.01.52.02.53.03.5Classification Error %Test ErrorTraining Error(a) Keeping n fixed.0.0 0.2 0.4 0.6 0.8 1.0Probability of retaining a unit (p)0.00.51.01.52.02.53.0Classification Error %Test ErrorTraining Error(b) Keeping pn fixed.Figure 9: Effect of changing dropout rates on MNIST.Another interesting setting is the second case in which the quantity pn is held constantwhere n is the number of hidden units in any particular layer. This means that networksthat have small p will have a large number of hidden units. Therefore, after applyingdropout, the expected number of units that are present will be the same across differentarchitectures. However, the test networks will be of different sizes. In our experiments,we set pn = 256 for the first two hidden layers and pn = 512 for the last hidden layer.Figure 9b shows the test error obtained as a function of p. We notice that the magnitudeof errors for small values of p has reduced by a lot compared to Figure 9a (for p = 0.1 it fellfrom 2.7% to 1.7%). Values of p that are close to 0.6 seem to perform best for this choiceof pn but our usual default value of 0.5 is close to optimal.1945Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov7.4 Effect of Data Set SizeOne test of a good regularizer is that it should make it possible to get good generalizationerror from models with a large number of parameters trained on small data sets. Thissection explores the effect of changing the data set size when dropout is used with feed-forward networks. Huge neural networks trained in the standard way overfit massively onsmall data sets. To see if dropout can help, we run classification experiments on MNISTand vary the amount of data given to the network.102 103 104 105Dataset size051015202530Classification Error %With dropoutWithout dropoutFigure 10: Effect of varying data set size.The results of these experiments areshown in Figure 10. The network was givendata sets of size 100, 500, 1K, 5K, 10Kand 50K chosen randomly from the MNISTtraining set. The same network architec-ture (784-1024-1024-2048-10) was used forall data sets. Dropout with p = 0.5 was per-formed at all the hidden layers and p = 0.8at the input layer. It can be observed thatfor extremely small data sets (100, 500)dropout does not give any improvements.The model has enough parameters that itcan overfit on the training data, even withall the noise coming from dropout. As thesize of the data set is increased, the gainfrom doing dropout increases up to a point and then declines. This suggests that for anygiven architecture and dropout rate, there is a \u201csweet spot\u201d corresponding to some amountof data that is large enough to not be memorized in spite of the noise but not so large thatoverfitting is not a problem anyways.7.5 Monte-Carlo Model Averaging vs. Weight Scaling0 20 40 60 80 100 120Number of samples used for Monte-Carlo averaging (k)1.001.051.101.151.201.251.301.35Test Classification error %Monte-Carlo Model AveragingApproximate averaging by weight scalingFigure 11: Monte-Carlo model averaging vs.weight scaling.The efficient test time procedure that wepropose is to do an approximate model com-bination by scaling down the weights of thetrained neural network. An expensive butmore correct way of averaging the modelsis to sample k neural nets using dropout foreach test case and average their predictions.As k \u2192\u221e, this Monte-Carlo model averagegets close to the true model average. It is in-teresting to see empirically how many sam-ples k are needed to match the performanceof the approximate averaging method. Bycomputing the error for different values of kwe can see how quickly the error rate of thefinite-sample average approaches the errorrate of the true model average.1946DropoutWe again use the MNIST data set and do classification by averaging the predictionsof k randomly sampled neural networks. Figure 11 shows the test error rate obtained fordifferent values of k. This is compared with the error obtained using the weight scalingmethod (shown as a horizontal line). It can be seen that around k = 50, the Monte-Carlomethod becomes as good as the approximate method. Thereafter, the Monte-Carlo methodis slightly better than the approximate method but well within one standard deviation ofit. This suggests that the weight scaling method is a fairly good approximation of the truemodel average.8. Dropout Restricted Boltzmann MachinesBesides feed-forward neural networks, dropout can also be applied to Restricted BoltzmannMachines (RBM). In this section, we formally describe this model and show some resultsto illustrate its key properties.8.1 Model DescriptionConsider an RBM with visible units v \u2208 {0, 1}D and hidden units h \u2208 {0, 1}F . It definesthe following probability distributionP (h,v; \u03b8) =1Z(\u03b8)exp(v>Wh + a>h + b>v).Where \u03b8 = {W,a,b} represents the model parameters and Z is the partition function.Dropout RBMs are RBMs augmented with a vector of binary random variables r \u2208{0, 1}F . Each random variable rj takes the value 1 with probability p, independent ofothers. If rj takes the value 1, the hidden unit hj is retained, otherwise it is dropped fromthe model. The joint distribution defined by a Dropout RBM can be expressed asP (r,h,v; p, \u03b8) = P (r; p)P (h,v|r; \u03b8),P (r; p) =F\u220fj=1prj (1\u2212 p)1\u2212rj ,P (h,v|r; \u03b8) =1Z \u2032(\u03b8, r)exp(v>Wh + a>h + b>v)F\u220fj=1g(hj , rj),g(hj , rj) = 1(rj = 1) + 1(rj = 0)1(hj = 0).Z \u2032(\u03b8, r) is the normalization constant. g(hj , rj) imposes the constraint that if rj = 0,hj must be 0. The distribution over h, conditioned on v and r is factorialP (h|r,v) =F\u220fj=1P (hj |rj ,v),P (hj = 1|rj ,v) = 1(rj = 1)\u03c3(bj +\u2211iWijvi).1947Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov(a) Without dropout (b) Dropout with p = 0.5.Figure 12: Features learned on MNIST by 256 hidden unit RBMs. The features are ordered by L2norm.The distribution over v conditioned on h is same as that of an RBMP (v|h) =D\u220fi=1P (vi|h),P (vi = 1|h) = \u03c3\uf8eb\uf8edai +\u2211jWijhj\uf8f6\uf8f8 .Conditioned on r, the distribution over {v,h} is same as the distribution that an RBMwould impose, except that the units for which rj = 0 are dropped from h. Therefore, theDropout RBM model can be seen as a mixture of exponentially many RBMs with sharedweights each using a different subset of h.8.2 Learning Dropout RBMsLearning algorithms developed for RBMs such as Contrastive Divergence (Hinton et al.,2006) can be directly applied for learning Dropout RBMs. The only difference is that r isfirst sampled and only the hidden units that are retained are used for training. Similar todropout neural networks, a different r is sampled for each training case in every minibatch.In our experiments, we use CD-1 for training dropout RBMs.8.3 Effect on FeaturesDropout in feed-forward networks improved the quality of features by reducing co-adaptations.This section explores whether this effect transfers to Dropout RBMs as well.Figure 12a shows features learned by a binary RBM with 256 hidden units. Figure 12bshows features learned by a dropout RBM with the same number of hidden units. Features1948Dropout(a) Without dropout (b) Dropout with p = 0.5.Figure 13: Effect of dropout on sparsity. Left: The activation histogram shows that a large num-ber of units have activations away from zero. Right: A large number of units haveactivations close to zero and very few units have high activation.learned by the dropout RBM appear qualitatively different in the sense that they seem tocapture features that are coarser compared to the sharply defined stroke-like features in thestandard RBM. There seem to be very few dead units in the dropout RBM relative to thestandard RBM.8.4 Effect on SparsityNext, we investigate the effect of dropout RBM training on sparsity of the hidden unitactivations. Figure 13a shows the histograms of hidden unit activations and their means ona test mini-batch after training an RBM. Figure 13b shows the same for dropout RBMs.The histograms clearly indicate that the dropout RBMs learn much sparser representationsthan standard RBMs even when no additional sparsity inducing regularizer is present.9. Marginalizing DropoutDropout can be seen as a way of adding noise to the states of hidden units in a neuralnetwork. In this section, we explore the class of models that arise as a result of marginalizingthis noise. These models can be seen as deterministic versions of dropout. In contrast tostandard (\u201cMonte-Carlo\u201d) dropout, these models do not need random bits and it is possibleto get gradients for the marginalized loss functions. In this section, we briefly explore thesemodels.Deterministic algorithms have been proposed that try to learn models that are robust tofeature deletion at test time (Globerson and Roweis, 2006). Marginalization in the contextof denoising autoencoders has been explored previously (Chen et al., 2012). The marginal-ization of dropout noise in the context of linear regression was discussed in Srivastava (2013).Wang and Manning (2013) further explored the idea of marginalizing dropout to speed-uptraining. van der Maaten et al. (2013) investigated different input noise distributions and1949Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinovthe regularizers obtained by marginalizing this noise. Wager et al. (2013) describes howdropout can be seen as an adaptive regularizer.9.1 Linear RegressionFirst we explore a very simple case of applying dropout to the classical problem of linearregression. Let X \u2208 RN\u00d7D be a data matrix of N data points. y \u2208 RN be a vector oftargets. Linear regression tries to find a w \u2208 RD that minimizes||y \u2212Xw||2.When the input X is dropped out such that any input dimension is retained withprobability p, the input can be expressed as R\u2217X where R \u2208 {0, 1}N\u00d7D is a random matrixwith Rij \u223c Bernoulli(p) and \u2217 denotes an element-wise product. Marginalizing the noise,the objective function becomesminimizewER\u223cBernoulli(p)[||y \u2212 (R \u2217X)w||2].This reduces tominimizew||y \u2212 pXw||2 + p(1\u2212 p)||\u0393w||2,where \u0393 = (diag(X>X))1/2. Therefore, dropout with linear regression is equivalent, inexpectation, to ridge regression with a particular form for \u0393. This form of \u0393 essentiallyscales the weight cost for weight wi by the standard deviation of the ith dimension of thedata. If a particular data dimension varies a lot, the regularizer tries to squeeze its weightmore.Another interesting way to look at this objective is to absorb the factor of p into w.This leads to the following formminimizew||y \u2212Xw\u0303||2 +1\u2212 pp||\u0393w\u0303||2,where w\u0303 = pw. This makes the dependence of the regularization constant on p explicit.For p close to 1, all the inputs are retained and the regularization constant is small. Asmore dropout is done (by decreasing p), the regularization constant grows larger.9.2 Logistic Regression and Deep NetworksFor logistic regression and deep neural nets, it is hard to obtain a closed form marginalizedmodel. However, Wang and Manning (2013) showed that in the context of dropout appliedto logistic regression, the corresponding marginalized model can be trained approximately.Under reasonable assumptions, the distributions over the inputs to the logistic unit and overthe gradients of the marginalized model are Gaussian. Their means and variances can becomputed efficiently. This approximate marginalization outperforms Monte-Carlo dropoutin terms of training time and generalization performance.However, the assumptions involved in this technique become successively weaker as morelayers are added. Therefore, the results are not directly applicable to deep networks.1950DropoutData Set Architecture Bernoulli dropout Gaussian dropoutMNIST 2 layers, 1024 units each 1.08 \u00b1 0.04 0.95 \u00b1 0.04CIFAR-10 3 conv + 2 fully connected layers 12.6 \u00b1 0.1 12.5 \u00b1 0.1Table 10: Comparison of classification error % with Bernoulli and Gaussian dropout. For MNIST,the Bernoulli model uses p = 0.5 for the hidden units and p = 0.8 for the input units.For CIFAR-10, we use p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) going from the input layer to thetop. The value of \u03c3 for the Gaussian dropout models was set to be\u221a1\u2212pp. Results wereaveraged over 10 different random seeds.10. Multiplicative Gaussian NoiseDropout involves multiplying hidden activations by Bernoulli distributed random variableswhich take the value 1 with probability p and 0 otherwise. This idea can be generalizedby multiplying the activations with random variables drawn from other distributions. Werecently discovered that multiplying by a random variable drawn from N (1, 1) works justas well, or perhaps better than using Bernoulli noise. This new form of dropout amountsto adding a Gaussian distributed random variable with zero mean and standard deviationequal to the activation of the unit. That is, each hidden activation hi is perturbed tohi + hir where r \u223c N (0, 1), or equivalently hir\u2032 where r\u2032 \u223c N (1, 1). We can generalizethis to r\u2032 \u223c N (1, \u03c32) where \u03c3 becomes an additional hyperparameter to tune, just like pwas in the standard (Bernoulli) dropout. The expected value of the activations remainsunchanged, therefore no weight scaling is required at test time.In this paper, we described dropout as a method where we retain units with probability pat training time and scale down the weights by multiplying them by a factor of p at test time.Another way to achieve the same effect is to scale up the retained activations by multiplyingby 1/p at training time and not modifying the weights at test time. These methods areequivalent with appropriate scaling of the learning rate and weight initializations at eachlayer.Therefore, dropout can be seen as multiplying hi by a Bernoulli random variable rb thattakes the value 1/p with probability p and 0 otherwise. E[rb] = 1 and V ar[rb] = (1\u2212 p)/p.For the Gaussian multiplicative noise, if we set \u03c32 = (1 \u2212 p)/p, we end up multiplyinghi by a random variable rg, where E[rg] = 1 and V ar[rg] = (1 \u2212 p)/p. Therefore, bothforms of dropout can be set up so that the random variable being multiplied by has thesame mean and variance. However, given these first and second order moments, rg has thehighest entropy and rb has the lowest. Both these extremes work well, although preliminaryexperimental results shown in Table 10 suggest that the high entropy case might workslightly better. For each layer, the value of \u03c3 in the Gaussian model was set to be\u221a1\u2212ppusing the p from the corresponding layer in the Bernoulli model.11. ConclusionDropout is a technique for improving neural networks by reducing overfitting. Standardbackpropagation learning builds up brittle co-adaptations that work for the training databut do not generalize to unseen data. Random dropout breaks up these co-adaptations by1951Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinovmaking the presence of any particular hidden unit unreliable. This technique was foundto improve the performance of neural nets in a wide variety of application domains includ-ing object classification, digit recognition, speech recognition, document classification andanalysis of computational biology data. This suggests that dropout is a general techniqueand is not specific to any domain. Methods that use dropout achieve state-of-the-art re-sults on SVHN, ImageNet, CIFAR-100 and MNIST. Dropout considerably improved theperformance of standard neural nets on other data sets as well.This idea can be extended to Restricted Boltzmann Machines and other graphical mod-els. The central idea of dropout is to take a large model that overfits easily and repeatedlysample and train smaller sub-models from it. RBMs easily fit into this framework. We de-veloped Dropout RBMs and empirically showed that they have certain desirable properties.One of the drawbacks of dropout is that it increases training time. A dropout networktypically takes 2-3 times longer to train than a standard neural network of the same ar-chitecture. A major cause of this increase is that the parameter updates are very noisy.Each training case effectively tries to train a different random architecture. Therefore, thegradients that are being computed are not gradients of the final architecture that will beused at test time. Therefore, it is not surprising that training takes a long time. However,it is likely that this stochasticity prevents overfitting. This creates a trade-off between over-fitting and training time. With more training time, one can use high dropout and suffer lessoverfitting. However, one way to obtain some of the benefits of dropout without stochas-ticity is to marginalize the noise to obtain a regularizer that does the same thing as thedropout procedure, in expectation. We showed that for linear regression this regularizer isa modified form of L2 regularization. For more complicated models, it is not obvious how toobtain an equivalent regularizer. Speeding up dropout is an interesting direction for futurework.AcknowledgmentsThis research was supported by OGS, NSERC and an Early Researcher Award.Appendix A. A Practical Guide for Training Dropout NetworksNeural networks are infamous for requiring extensive hyperparameter tuning. Dropoutnetworks are no exception. In this section, we describe heuristics that might be useful forapplying dropout.A.1 Network SizeIt is to be expected that dropping units will reduce the capacity of a neural network. Ifn is the number of hidden units in any layer and p is the probability of retaining a unit,then instead of n hidden units, only pn units will be present after dropout, in expectation.Moreover, this set of pn units will be different each time and the units are not allowed tobuild co-adaptations freely. Therefore, if an n-sized layer is optimal for a standard neuralnet on any given task, a good dropout net should have at least n/p units. We found this tobe a useful heuristic for setting the number of hidden units in both convolutional and fullyconnected networks.1952DropoutA.2 Learning Rate and MomentumDropout introduces a significant amount of noise in the gradients compared to standardstochastic gradient descent. Therefore, a lot of gradients tend to cancel each other. Inorder to make up for this, a dropout net should typically use 10-100 times the learning ratethat was optimal for a standard neural net. Another way to reduce the effect the noise isto use a high momentum. While momentum values of 0.9 are common for standard nets,with dropout we found that values around 0.95 to 0.99 work quite a lot better. Using highlearning rate and/or momentum significantly speed up learning.A.3 Max-norm RegularizationThough large momentum and learning rate speed up learning, they sometimes cause thenetwork weights to grow very large. To prevent this, we can use max-norm regularization.This constrains the norm of the vector of incoming weights at each hidden unit to be boundby a constant c. Typical values of c range from 3 to 4.A.4 Dropout RateDropout introduces an extra hyperparameter\u2014the probability of retaining a unit p. Thishyperparameter controls the intensity of dropout. p = 1, implies no dropout and low valuesof p mean more dropout. Typical values of p for hidden units are in the range 0.5 to 0.8.For input layers, the choice depends on the kind of input. For real-valued inputs (imagepatches or speech frames), a typical value is 0.8. For hidden layers, the choice of p is coupledwith the choice of number of hidden units n. Smaller p requires big n which slows downthe training and leads to underfitting. Large p may not produce enough dropout to preventoverfitting.Appendix B. Detailed Description of Experiments and Data Sets.This section describes the network architectures and training details for the experimentalresults reported in this paper. The code for reproducing these results can be obtained fromhttp://www.cs.toronto.edu/~nitish/dropout. The implementation is GPU-based. Weused the excellent CUDA libraries\u2014cudamat (Mnih, 2009) and cuda-convnet (Krizhevskyet al., 2012) to implement our networks.B.1 MNISTThe MNIST data set consists of 60,000 training and 10,000 test examples each representinga 28\u00d728 digit image. We held out 10,000 random training images for validation. Hyperpa-rameters were tuned on the validation set such that the best validation error was producedafter 1 million weight updates. The validation set was then combined with the training setand training was done for 1 million weight updates. This net was used to evaluate the per-formance on the test set. This way of using the validation set was chosen because we foundthat it was easy to set up hyperparameters so that early stopping was not required at all.Therefore, once the hyperparameters were fixed, it made sense to combine the validationand training sets and train for a very long time.1953http://www.cs.toronto.edu/~nitish/dropoutSrivastava, Hinton, Krizhevsky, Sutskever and SalakhutdinovThe architectures shown in Figure 4 include all combinations of 2, 3, and 4 layer networkswith 1024 and 2048 units in each layer. Thus, there are six architectures in all. For all thearchitectures (including the ones reported in Table 2), we used p = 0.5 in all hidden layersand p = 0.8 in the input layer. A final momentum of 0.95 and weight constraints with c = 2was used in all the layers.To test the limits of dropout\u2019s regularization power, we also experimented with 2 and 3layer nets having 4096 and 8192 units. 2 layer nets gave improvements as shown in Table 2.However, the three layer nets performed slightly worse than 2 layer ones with the samelevel of dropout. When we increased dropout, performance improved but not enough tooutperform the 2 layer nets.B.2 SVHNThe SVHN data set consists of approximately 600,000 training images and 26,000 testimages. The training set consists of two parts\u2014A standard labeled training set and anotherset of labeled examples that are easy. A validation set was constructed by taking examplesfrom both the parts. Two-thirds of it were taken from the standard set (400 per class) andone-third from the extra set (200 per class), a total of 6000 samples. This same processis used by Sermanet et al. (2012). The inputs were RGB pixels normalized to have zeromean and unit variance. Other preprocessing techniques such as global or local contrastnormalization or ZCA whitening did not give any noticeable improvements.The best architecture that we found uses three convolutional layers each followed bya max-pooling layer. The convolutional layers have 96, 128 and 256 filters respectively.Each convolutional layer has a 5 \u00d7 5 receptive field applied with a stride of 1 pixel. Eachmax pooling layer pools 3 \u00d7 3 regions at strides of 2 pixels. The convolutional layers arefollowed by two fully connected hidden layers having 2048 units each. All units use therectified linear activation function. Dropout was applied to all the layers of the networkwith the probability of retaining the unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for thedifferent layers of the network (going from input to convolutional layers to fully connectedlayers). In addition, the max-norm constraint with c = 4 was used for all the weights. Amomentum of 0.95 was used in all the layers. These hyperparameters were tuned using avalidation set. Since the training set was quite large, we did not combine the validationset with the training set for final training. We reported test error of the model that hadsmallest validation error.B.3 CIFAR-10 and CIFAR-100The CIFAR-10 and CIFAR-100 data sets consists of 50,000 training and 10,000 test imageseach. They have 10 and 100 image categories respectively. These are 32 \u00d7 32 color images.We used 5,000 of the training images for validation. We followed the procedure similarto MNIST, where we found the best hyperparameters using the validation set and thencombined it with the training set. The images were preprocessed by doing global contrastnormalization in each color channel followed by ZCA whitening. Global contrast normal-ization means that for image and each color channel in that image, we compute the meanof the pixel intensities and subtract it from the channel. ZCA whitening means that wemean center the data, rotate it onto its principle components, normalize each component1954Dropoutand then rotate it back. The network architecture and dropout rates are same as that forSVHN, except the learning rates for the input layer which had to be set to smaller values.B.4 TIMITThe open source Kaldi toolkit (Povey et al., 2011) was used to preprocess the data into log-filter banks. A monophone system was trained to do a forced alignment and to get labels forspeech frames. Dropout neural networks were trained on windows of 21 consecutive framesto predict the label of the central frame. No speaker dependent operations were performed.The inputs were mean centered and normalized to have unit variance.We used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers.Max-norm constraint with c = 4 was used in all the layers. A momentum of 0.95 with ahigh learning rate of 0.1 was used. The learning rate was decayed as \ufffd0(1 + t/T )\u22121. ForDBN pretraining, we trained RBMs using CD-1. The variance of each input unit for theGaussian RBM was fixed to 1. For finetuning the DBN with dropout, we found that inorder to get the best results it was important to use a smaller learning rate (about 0.01).Adding max-norm constraints did not give any improvements.B.5 ReutersThe Reuters RCV1 corpus contains more than 800,000 documents categorized into 103classes. These classes are arranged in a tree hierarchy. We created a subset of this data setconsisting of 402,738 articles and a vocabulary of 2000 words comprising of 50 categoriesin which each document belongs to exactly one class. The data was split into equal sizedtraining and test sets. We tried many network architectures and found that dropout gaveimprovements in classification accuracy over all of them. However, the improvement wasnot as significant as that for the image and speech data sets. This might be explained bythe fact that this data set is quite big (more than 200,000 training examples) and overfittingis not a very serious problem.B.6 Alternative SplicingThe alternative splicing data set consists of data for 3665 cassette exons, 1014 RNA featuresand 4 tissue types derived from 27 mouse tissues. For each input, the target consists of 4softmax units (one for tissue type). Each softmax unit has 3 states (inc, exc, nc) which areof the biological importance. For each softmax unit, the aim is to predict a distribution overthese 3 states that matches the observed distribution from wet lab experiments as closelyas possible. The evaluation metric is Code Quality which is defined as|data points|\u2211i=1\u2211t\u2208tissue types\u2211s\u2208{inc, exc, nc}psi,t log(qst (ri)p\u0304s),where, psi,t is the target probability for state s and tissue type t in input i; qst (ri) is thepredicted probability for state s in tissue type t for input ri and p\u0304s is the average of psi,tover i and t.A two layer dropout network with 1024 units in each layer was trained on this data set.A value of p = 0.5 was used for the hidden layer and p = 0.7 for the input layer. Max-normregularization with high decaying learning rates was used. Results were averaged across thesame 5 folds used by Xiong et al. (2011).1955Srivastava, Hinton, Krizhevsky, Sutskever and SalakhutdinovReferencesM. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders fordomain adaptation. In Proceedings of the 29th International Conference on MachineLearning, pages 767\u2013774. ACM, 2012.G. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton. Phone recognition with the mean-covariance restricted Boltzmann machine. In Advances in Neural Information ProcessingSystems 23, pages 469\u2013477, 2010.O. Dekel, O. Shamir, and L. Xiao. Learning to classify with missing and corrupted features.Machine Learning, 81(2):149\u2013178, 2010.A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature deletion. InProceedings of the 23rd International Conference on Machine Learning, pages 353\u2013360.ACM, 2006.I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks.In Proceedings of the 30th International Conference on Machine Learning, pages 1319\u20131327. ACM, 2013.G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.Science, 313(5786):504 \u2013 507, 2006.G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets.Neural Computation, 18:1527\u20131554, 2006.K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stagearchitecture for object recognition? In Proceedings of the International Conference onComputer Vision (ICCV\u201909). IEEE, 2009.A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report,University of Toronto, 2009.A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolu-tional neural networks. In Advances in Neural Information Processing Systems 25, pages1106\u20131114, 2012.Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D.Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computa-tion, 1(4):541\u2013551, 1989.Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, Z. Li, M.-H. Tsai, X. Zhou,T. Huang, and T. Zhang. Imagenet classification: fast descriptor coding and large-scalesvm training. Large scale visual recognition challenge, 2010.A. Livnat, C. Papadimitriou, N. Pippenger, and M. W. Feldman. Sex, mixability, andmodularity. Proceedings of the National Academy of Sciences, 107(4):1452\u20131457, 2010.V. Mnih. CUDAMat: a CUDA-based matrix class for Python. Technical Report UTMLTR 2009-004, Department of Computer Science, University of Toronto, November 2009.1956DropoutA. Mohamed, G. E. Dahl, and G. E. Hinton. Acoustic modeling using deep belief networks.IEEE Transactions on Audio, Speech, and Language Processing, 2010.R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., 1996.Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits innatural images with unsupervised feature learning. In NIPS Workshop on Deep Learningand Unsupervised Feature Learning 2011, 2011.S. J. Nowlan and G. E. Hinton. Simplifying neural networks by soft weight-sharing. NeuralComputation, 4(4), 1992.D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann,P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The KaldiSpeech Recognition Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognitionand Understanding. IEEE Signal Processing Society, 2011.R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Proceedings of the Inter-national Conference on Artificial Intelligence and Statistics, volume 5, pages 448\u2013455,2009.R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markovchain Monte Carlo. In Proceedings of the 25th International Conference on MachineLearning. ACM, 2008.J. Sanchez and F. Perronnin. High-dimensional signature compression for large-scale imageclassification. In Proceedings of the 2011 IEEE Conference on Computer Vision andPattern Recognition, pages 1665\u20131672, 2011.P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to housenumbers digit classification. In International Conference on Pattern Recognition (ICPR2012), 2012.P. Simard, D. Steinkraus, and J. Platt. Best practices for convolutional neural networks ap-plied to visual document analysis. In Proceedings of the Seventh International Conferenceon Document Analysis and Recognition, volume 2, pages 958\u2013962, 2003.J. Snoek, H. Larochelle, and R. Adams. Practical Bayesian optimization of machine learningalgorithms. In Advances in Neural Information Processing Systems 25, pages 2960\u20132968,2012.N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18thannual conference on Learning Theory, COLT\u201905, pages 545\u2013560. Springer-Verlag, 2005.N. Srivastava. Improving Neural Networks with Dropout. Master\u2019s thesis, University ofToronto, January 2013.R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the RoyalStatistical Society. Series B. Methodological, 58(1):267\u2013288, 1996.1957Srivastava, Hinton, Krizhevsky, Sutskever and SalakhutdinovA. N. Tikhonov. On the stability of inverse problems. Doklady Akademii Nauk SSSR, 39(5):195\u2013198, 1943.L. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with marginalizedcorrupted features. In Proceedings of the 30th International Conference on MachineLearning, pages 410\u2013418. ACM, 2013.P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robustfeatures with denoising autoencoders. In Proceedings of the 25th International Conferenceon Machine Learning, pages 1096\u20131103. ACM, 2008.P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoisingautoencoders: Learning useful representations in a deep network with a local denoisingcriterion. In Proceedings of the 27th International Conference on Machine Learning, pages3371\u20133408. ACM, 2010.S. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Advancesin Neural Information Processing Systems 26, pages 351\u2013359, 2013.S. Wang and C. D. Manning. Fast dropout training. In Proceedings of the 30th InternationalConference on Machine Learning, pages 118\u2013126. ACM, 2013.H. Y. Xiong, Y. Barash, and B. J. Frey. Bayesian prediction of tissue-regulated splicingusing RNA sequence and cellular context. Bioinformatics, 27(18):2554\u20132562, 2011.M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutionalneural networks. CoRR, abs/1301.3557, 2013.1958\tIntroduction\tMotivation\tRelated Work\tModel Description\tLearning Dropout Nets\tBackpropagation\tUnsupervised Pretraining\tExperimental Results\tResults on Image Data Sets\tMNIST\tStreet View House Numbers\tCIFAR-10 and CIFAR-100\tImageNet\tResults on TIMIT\tResults on a Text Data Set\tComparison with Bayesian Neural Networks\tComparison with Standard Regularizers\tSalient Features\tEffect on Features\tEffect on Sparsity\tEffect of Dropout Rate\tEffect of Data Set Size\tMonte-Carlo Model Averaging vs. Weight Scaling\tDropout Restricted Boltzmann Machines\tModel Description\tLearning Dropout RBMs\tEffect on Features\tEffect on Sparsity\tMarginalizing Dropout\tLinear Regression\tLogistic Regression and Deep Networks\tMultiplicative Gaussian Noise\tConclusion\tA Practical Guide for Training Dropout Networks\tNetwork Size\tLearning Rate and Momentum\tMax-norm Regularization\tDropout Rate\tDetailed Description of Experiments and Data Sets\tMNIST\tSVHN\tCIFAR-10 and CIFAR-100\tTIMIT\tReuters\tAlternative Splicing"