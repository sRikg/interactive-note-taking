"Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks\u00a0[15, 2]. The RNN model does not have a projection layer only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.  The complexity per training example of the RNN model is    -- -------------------- -- -----     (3)   -- -------------------- -- -----  where the word representations D have the same dimensionality as the hidden layer H. Again, the term H\u2005\u00d7\u2005V can be efficiently reduced to H\u2005\u00d7\u2005log\u2082(V) by using hierarchical softmax. Most of the complexity then comes from H\u2005\u00d7\u2005H."
