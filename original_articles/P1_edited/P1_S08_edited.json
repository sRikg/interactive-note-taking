"To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief\u00a0[6], including the feedforward NNLM and the new models proposed in this paper. The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad\u00a0[7]. Under this framework, it is common to use one hundred or more model replicas, each using many CPU cores at different machines in a data center."
