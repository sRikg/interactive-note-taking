"The probabilistic feedforward neural network language model has been proposed in [1]. It consists of input, projection, hidden and output layers. At the input layer,  previous words are encoded using 1-of- coding, where  is size of the vocabulary. The input layer is then projected to a projection layer  that has dimensionality , using a shared projection matrix. As only  inputs are active at any given time, composition of the projection layer is a relatively cheap operation.  The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense. For a common choice of , the size of the projection layer () might be 500 to 2000, while the hidden layer size  is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality . Thus, the computational complexity per each training example is     -- -------------------- -- ----- (3)   -- -------------------- -- -----  where the dominating term is . However, several practical solutions were proposed for avoiding it either using hierarchical versions of the softmax\u00a0[25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training\u00a0[4, 9]. With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log\u2082(V). Thus, most of the complexity is caused by the term N\u2005\u00d7\u2005D\u2005\u00d7\u2005H.  In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree. This follows previous observations that the frequency of words works well for obtaining classes in neural net language models\u00a0[16]. Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log\u2082(V) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log\u2082(Unigram_perplexity(V)). For example when the vocabulary size is one million words, this results in about two times speedup in evaluation. While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N\u2005\u00d7\u2005D\u2005\u00d7\u2005H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efficiency of the softmax normalization. "

