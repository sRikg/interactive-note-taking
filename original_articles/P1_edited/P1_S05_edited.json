"Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among wordsnull,  LDA moreover becomes computationally very expensive on large data sets.  Similar to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy, while minimizing the computational complexity.  For all the following models, the training complexity is proportional to where  is number of the training epochs,  is the number of the words in the training set and  is defined further for each model architecture. Common choice is  and  up to one billion. All models are trained using stochastic gradient descent and backpropagation [26]"
