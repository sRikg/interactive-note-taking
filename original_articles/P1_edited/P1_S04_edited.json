"Representation of words as continuous vectors has a long history\u00a0[10, 26, 8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in\u00a0[1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others.  Another interesting architecture of NNLM was presented in\u00a0[13, 14], where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.  It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications\u00a0[4, 5, 29]. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora\u00a0[4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison\u00b2\u00b22http://ronan.collobert.com/senna/ http://metaoptimize.com/projects/wordreprs/ http://www.fit.vutbr.cz/~imikolov/rnnlm/ http://ai.stanford.edu/~ehhuang/ . However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in\u00a0[13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used\u00a0[23]."
