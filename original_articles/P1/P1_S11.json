The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples. The training complexity of this architecture is proportional to    -- -------------------------- -- -----     Q = C x (D+ Dxlog2(V)) ------------------------- (5) where C is the maximum distance of the words. Thus, if we choose C=5, for each training word we will select randomly a number R in range <1;C>, and then use R words from history and R words from the future of the current word as correct labels. This will require us to do R\u2005\u00d7\u20052 word classifications, with the current word as input, and each of the R\u2005+\u2005R words as output. In the following experiments, we use C=10.