{"[1301.3781] Efficient Estimation of Word Representations in Vector Space\n\nEfficient Estimation of Word Representations in Vector Space\n\nTomas Mikolov\nGoogle Inc., Mountain View, CA\ntmikolov@google.com\n\\AndKai Chen\nGoogle Inc., Mountain View, CA\nkaichen@google.com\n\\ANDGreg Corrado\nGoogle Inc., Mountain View, CA\ngcorrado@google.com\n\\AndJeffrey Dean\nGoogle Inc., Mountain View, CA\njeff@google.com

\n\nAbstract\n\n

We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results\nare compared to the previously best performing techniques based on\ndifferent types of neural networks. We observe large improvements in\naccuracy at much lower computational cost, i.e. it takes less than a day\nto learn high quality word vectors from a 1.6 billion words data set.\nFurthermore, we show that these vectors provide state-of-the-art\nperformance on our test set for measuring syntactic and semantic word\nsimilarities.

\n\n1 Introduction\n\n

Many current NLP systems and techniques treat words as atomic units -\nthere is no notion of similarity between words, as these are represented\nas indices in a vocabulary. This choice has several good reasons -\nsimplicity, robustness and the observation that simple models trained on\nhuge amounts of data outperform complex systems trained on less data. An\nexample is the popular N-gram model used for statistical language\nmodeling - today, it is possible to train N-grams on virtually all\navailable data (trillions of words\u00a0[3]).\n\nHowever, the simple techniques are at their limits in many tasks. For\nexample, the amount of relevant in-domain data for automatic speech\nrecognition is limited - the performance is usually dominated by the\nsize of high quality transcribed speech data (often just millions of\nwords). In machine translation, the existing corpora for many languages\ncontain only a few billions of words or less. Thus, there are situations\nwhere simple scaling up of the basic techniques will not result in any\nsignificant progress, and we have to focus on more advanced techniques.\n\nWith progress of machine learning techniques in recent years, it has\nbecome possible to train more complex models on much larger data set,\nand they typically outperform the simple models. Probably the most\nsuccessful concept is to use distributed representations of words\u00a0[10].\nFor example, neural network based language models significantly\noutperform N-gram models\u00a0[1, 27, 17].

\n\n1.1 Goals of the Paper\n\n

The main goal of this paper is to introduce techniques that can be used\nfor learning high-quality word vectors from huge data sets with billions\nof words, and with millions of words in the vocabulary. As far as we\nknow, none of the previously proposed architectures has been\nsuccessfully trained on more than a few hundred of millions of words,\nwith a modest dimensionality of the word vectors between 50 - 100.\n\nWe use recently proposed techniques for measuring the quality of the\nresulting vector representations, with the expectation that not only\nwill similar words tend to be close to each other, but that words can\nhave multiple degrees of similarity\u00a0[20]. This has been observed earlier\nin the context of inflectional languages - for example, nouns can have\nmultiple word endings, and if we search for similar words in a subspace\nof the original vector space, it is possible to find words that have\nsimilar endings\u00a0[13, 14].\n\nSomewhat surprisingly, it was found that similarity of word\nrepresentations goes beyond simple syntactic regularities. Using a word\noffset technique where simple algebraic operations are performed on the\nword vectors, it was shown for example that vector(\u201dKing\u201d) -\nvector(\u201dMan\u201d) + vector(\u201dWoman\u201d) results in a vector that is closest to\nthe vector representation of the word Queen\u00a0[20].\n\nIn this paper, we try to maximize accuracy of these vector operations by\ndeveloping new model architectures that preserve the linear regularities\namong words. We design a new comprehensive test set for measuring both\nsyntactic and semantic regularities\u00b9\u00b91The test set is available at\nwww.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt, and show that many\nsuch regularities can be learned with high accuracy. Moreover, we\ndiscuss how training time and accuracy depends on the dimensionality of\nthe word vectors and on the amount of the training data.

\n\n1.2 Previous Work\n\n

Representation of words as continuous vectors has a long history\u00a0[10,\n26, 8]. A very popular model architecture for estimating neural network\nlanguage model (NNLM) was proposed in\u00a0[1], where a feedforward neural\nnetwork with a linear projection layer and a non-linear hidden layer was\nused to learn jointly the word vector representation and a statistical\nlanguage model. This work has been followed by many others.\n\nAnother interesting architecture of NNLM was presented in\u00a0[13, 14],\nwhere the word vectors are first learned using neural network with a\nsingle hidden layer. The word vectors are then used to train the NNLM.\nThus, the word vectors are learned even without constructing the full\nNNLM. In this work, we directly extend this architecture, and focus just\non the first step where the word vectors are learned using a simple\nmodel.\n\nIt was later shown that the word vectors can be used to significantly\nimprove and simplify many NLP applications\u00a0[4, 5, 29]. Estimation of the\nword vectors itself was performed using different model architectures\nand trained on various corpora\u00a0[4, 29, 23, 19, 9], and some of the\nresulting word vectors were made available for future research and\ncomparison\u00b2\u00b22http://ronan.collobert.com/senna/\nhttp://metaoptimize.com/projects/wordreprs/\nhttp://www.fit.vutbr.cz/~imikolov/rnnlm/\nhttp://ai.stanford.edu/~ehhuang/ . However, as far as we know, these\narchitectures were significantly more computationally expensive for\ntraining than the one proposed in\u00a0[13], with the exception of certain\nversion of log-bilinear model where diagonal weight matrices are\nused\u00a0[23].


\n\n2 Model Architectures\n\n

Many different types of models were proposed for estimating continuous\nrepresentations of words, including the well-known Latent Semantic\nAnalysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we\nfocus on distributed representations of words learned by neural\nnetworks, as it was previously shown that they perform significantly\nbetter than LSA for preserving linear regularities among words\u00a0[20, 31]": null, "\nLDA moreover becomes computationally very expensive on large data sets.\n\nSimilar to\u00a0[18], to compare different model architectures we define\nfirst the computational complexity of a model as the number of\nparameters that need to be accessed to fully train the model. Next, we\nwill try to maximize the accuracy, while minimizing the computational\ncomplexity.\n\nFor all the following models, the training complexity is proportional to\n\n -- -------------------- -- -----\n  O\u2004": "E\u2005\u00d7\u2005T\u2005\u00d7\u2005Q, (1)\n  -- -------------------- -- -----\n\nwhere  is number of the training epochs,  is the number of the words in the training set and  is defined further for each model architecture. Common choice is  and  up to one billion. All models are trained using stochastic gradient descent and backpropagation\u00a0[26].


\n\n2.1 Feedforward Neural Net Language Model (NNLM)\n\n

The probabilistic feedforward neural network language model has been proposed in [1]. It consists of input, projection, hidden and output layers. At the input layer,  previous words are encoded using 1-of- coding, where  is size of the vocabulary. The input layer is then projected to a projection layer  that has dimensionality , using a shared projection matrix. As only  inputs are active at any given time, composition of the projection layer is a relatively cheap operation.\n\nThe NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense. For a common choice of , the size of the projection layer () might be 500 to 2000, while the hidden layer size  is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality . Thus, the computational complexity per each training example is \n\n  -- -------------------- -- -----\n     Q\u2004": "N\u2005\u00d7\u2005D\u2005+\u2005N\u2005\u00d7\u2005D\u2005\u00d7\u2005H\u2005+\u2005H\u2005\u00d7\u2005V      (3)\n  -- -------------------- -- -----\n\nwhere the dominating term is . However, several practical solutions were proposed for avoiding it; O\u2004": null, " either using hierarchical versions of the\nsoftmax\u00a0[25, 23, 18], or avoiding normalized models completely by using\nmodels that are not normalized during training\u00a0[4, 9]. With binary tree\nrepresentations of the vocabulary, the number of output units that need\nto be evaluated can go down to around log\u2082(V). Thus, most of the\ncomplexity is caused by the term N\u2005\u00d7\u2005D\u2005\u00d7\u2005H.\n\nIn our models, we use hierarchical softmax where the vocabulary is\nrepresented as a Huffman binary tree. This follows previous observations\nthat the frequency of words works well for obtaining classes in neural\nnet language models\u00a0[16]. Huffman trees assign short binary codes to\nfrequent words, and this further reduces the number of output units that\nneed to be evaluated: while balanced binary tree would require log\u2082(V)\noutputs to be evaluated, the Huffman tree based hierarchical softmax\nrequires only about log\u2082(Unigram_perplexity(V)). For example when the\nvocabulary size is one million words, this results in about two times\nspeedup in evaluation. While this is not crucial speedup for neural\nnetwork LMs as the computational bottleneck is in the N\u2005\u00d7\u2005D\u2005\u00d7\u2005H term, we\nwill later propose architectures that do not have hidden layers and thus\ndepend heavily on the efficiency of the softmax normalization.


\n\n2.2 Recurrent Neural Net Language Model (RNNLM)\n\n

Recurrent neural network based language model has been proposed to\novercome certain limitations of the feedforward NNLM, such as the need\nto specify the context length (the order of the model N), and because\ntheoretically RNNs can efficiently represent more complex patterns than\nthe shallow neural networks\u00a0[15, 2]. The RNN model does not have a\nprojection layer": null, " only input, hidden and output layer. What is special\nfor this type of model is the recurrent matrix that connects hidden\nlayer to itself, using time-delayed connections. This allows the\nrecurrent model to form some kind of short term memory, as information\nfrom the past can be represented by the hidden layer state that gets\nupdated based on the current input and the state of the hidden layer in\nthe previous time step.\n\nThe complexity per training example of the RNN model is\n\n  -- -------------------- -- -----\n     Q\u2004": "H\u2005\u00d7\u2005H\u2005+\u2005H\u2005\u00d7\u2005V,      (3)\n  -- -------------------- -- -----\n\nwhere the word representations D have the same dimensionality as the\nhidden layer H. Again, the term H\u2005\u00d7\u2005V can be efficiently reduced to\nH\u2005\u00d7\u2005log\u2082(V) by using hierarchical softmax. Most of the complexity then\ncomes from H\u2005\u00d7\u2005H.


\n\n2.3 Parallel Training of Neural Networks\n\n

To train models on huge data sets, we have implemented several models on\ntop of a large-scale distributed framework called DistBelief\u00a0[6],\nincluding the feedforward NNLM and the new models proposed in this\npaper. The framework allows us to run multiple replicas of the same\nmodel in parallel, and each replica synchronizes its gradient updates\nthrough a centralized server that keeps all the parameters. For this\nparallel training, we use mini-batch asynchronous gradient descent with\nan adaptive learning rate procedure called Adagrad\u00a0[7]. Under this\nframework, it is common to use one hundred or more model replicas, each\nusing many CPU cores at different machines in a data center.


\n\n3 New Log-linear Models\n\n

In this section, we propose two new model architectures for learning\ndistributed representations of words that try to minimize computational\ncomplexity. The main observation from the previous section was that most\nof the complexity is caused by the non-linear hidden layer in the model.\nWhile this is what makes neural networks so attractive, we decided to\nexplore simpler models that might not be able to represent the data as\nprecisely as neural networks, but can possibly be trained on much more\ndata efficiently.\n\nThe new architectures directly follow those proposed in our earlier\nwork\u00a0[13, 14], where it was found that neural network language model can\nbe successfully trained in two steps: first, continuous word vectors are\nlearned using simple model, and then the N-gram NNLM is trained on top\nof these distributed representations of words. While there has been\nlater substantial amount of work that focuses on learning word vectors,\nwe consider the approach proposed in\u00a0[13] to be the simplest one. Note\nthat related models have been proposed also much earlier\u00a0[26, 8].\n\n3.1 Continuous Bag-of-Words Model\n\nThe first proposed architecture is similar to the feedforward NNLM,\nwhere the non-linear hidden layer is removed and the projection layer is\nshared for all words (not just the projection matrix)", " thus, all words\nget projected into the same position (their vectors are averaged). We\ncall this architecture a bag-of-words model as the order of words in the\nhistory does not influence the projection. Furthermore, we also use\nwords from the future": null, " we have obtained the best performance on the task\nintroduced in the next section by building a log-linear classifier with\nfour future and four history words at the input, where the training\ncriterion is to correctly classify the current (middle) word. Training\ncomplexity is then\n\n  -- -------------------------- -- -----\n     Q\u2004": null, "\u2006C>, and then use R words from history and R words from the future\nof the current word as correct labels. This will require us to do R\u2005\u00d7\u20052\nword classifications, with the current word as input, and each of the\nR\u2005+\u2005R words as output. In the following experiments, we use C\u2004": null, " synonyms are thus counted as mistakes. This also means that\nreaching 100% accuracy is likely to be impossible, as the current models\ndo not have any input information about word morphology. However, we\nbelieve that usefulness of the word vectors for certain applications\nshould be positively correlated with this accuracy metric. Further\nprogress can be achieved by incorporating information about structure of\nwords, especially for the syntactic questions.\n\nTable 1: Examples of five types of semantic and nine types of syntactic\nquestions in the Semantic-Syntactic Word Relationship test set.\n\nType of relationship Word Pair 1 Word Pair 2 Common capital city Athens\nGreece Oslo Norway All capital cities Astana Kazakhstan Harare Zimbabwe\nCurrency Angola kwanza Iran rial City-in-state Chicago Illinois Stockton\nCalifornia Man-Woman brother sister grandson granddaughter Adjective to\nadverb apparent apparently rapid rapidly Opposite possibly impossibly\nethical unethical Comparative great greater tough tougher Superlative\neasy easiest lucky luckiest Present Participle think thinking read\nreading Nationality adjective Switzerland Swiss Cambodia Cambodian Past\ntense walking walked swimming swam Plural nouns mouse mice dollar\ndollars Plural verbs work works speak speaks\n\n4.2 Maximization of Accuracy\n\nWe have used a Google News corpus for training the word vectors. This\ncorpus contains about 6B tokens. We have restricted the vocabulary size\nto 1 million most frequent words. Clearly, we are facing time\nconstrained optimization problem, as it can be expected that both using\nmore data and higher dimensional word vectors will improve the accuracy.\nTo estimate the best choice of model architecture for obtaining as good\nas possible results quickly, we have first evaluated models trained on\nsubsets of the training data, with vocabulary restricted to the most\nfrequent 30k words. The results using the CBOW architecture with\ndifferent choice of word vector dimensionality and increasing amount of\nthe training data are shown in Table\u00a02.\n\nIt can be seen that after some point, adding more dimensions or adding\nmore training data provides diminishing improvements. So, we have to\nincrease both vector dimensionality and the amount of the training data\ntogether. While this observation might seem trivial, it must be noted\nthat it is currently popular to train word vectors on relatively large\namounts of data, but with insufficient size (such as 50 - 100). Given\nEquation\u00a04, increasing amount of training data twice results in about\nthe same increase of computational complexity as increasing vector size\ntwice.\n\nTable 2: Accuracy on subset of the Semantic-Syntactic Word Relationship\ntest set, using word vectors from the CBOW architecture with limited\nvocabulary. Only questions containing words from the most frequent 30k\nwords are used.\n\nDimensionality / Training words 24M 49M 98M 196M 391M 783M 50 13.4 15.7\n18.6 19.1 22.5 23.2 100 19.4 23.1 27.8 28.7 33.4 32.2 300 23.2 29.2 35.3\n38.6 43.7 45.9 600 24.0 30.1 36.5 40.8 46.6 50.4\n\nFor the experiments reported in Tables\u00a02 and\u00a04, we used three training\nepochs with stochastic gradient descent and backpropagation. We chose\nstarting learning rate 0.025 and decreased it linearly, so that it\napproaches zero at the end of the last training epoch.\n\n4.3 Comparison of Model Architectures\n\nFirst we compare different model architectures for deriving the word\nvectors using the same training data and using the same dimensionality\nof 640 of the word vectors. In the further experiments, we use full set\nof questions in the new Semantic-Syntactic Word Relationship test set,\ni.e. unrestricted to the 30k vocabulary. We also include results on a\ntest set introduced in\u00a0[20] that focuses on syntactic similarity between\nwords\u00b3\u00b33We thank Geoff Zweig for providing us the test set..\n\nThe training data consists of several LDC corpora and is described in\ndetail in\u00a0[18] (320M words, 82K vocabulary). We used these data to\nprovide a comparison to a previously trained recurrent neural network\nlanguage model that took about 8 weeks to train on a single CPU. We\ntrained a feedforward NNLM with the same number of 640 hidden units\nusing the DistBelief parallel training\u00a0[6], using a history of 8\nprevious words (thus, the NNLM has more parameters than the RNNLM, as\nthe projection layer has size 640\u2005\u00d7\u20058).\n\nIn Table\u00a03, it can be seen that the word vectors from the RNN (as used\nin\u00a0[20]) perform well mostly on the syntactic questions. The NNLM\nvectors perform significantly better than the RNN - this is not\nsurprising, as the word vectors in the RNNLM are directly connected to a\nnon-linear hidden layer. The CBOW architecture works better than the\nNNLM on the syntactic tasks, and about the same on the semantic one.\nFinally, the Skip-gram architecture works slightly worse on the\nsyntactic task than the CBOW model (but still better than the NNLM), and\nmuch better on the semantic part of the test than all the other models.\n\nTable 3: Comparison of architectures using models trained on the same\ndata, with 640-dimensional word vectors. The accuracies are reported on\nour Semantic-Syntactic Word Relationship test set, and on the syntactic\nrelationship test set of\u00a0[20]\n\nModel Semantic-Syntactic Word Relationship test set MSR Word Relatedness\nArchitecture Semantic Accuracy [%] Syntactic Accuracy [%] Test Set\u00a0[20]\nRNNLM 9 36 35 NNLM 23 53 47 CBOW 24 64 61 Skip-gram 55 59 56\n\nNext, we evaluated our models trained using one CPU only and compared\nthe results against publicly available word vectors. The comparison is\ngiven in Table\u00a04. The CBOW model was trained on subset of the Google\nNews data in about a day, while training time for the Skip-gram model\nwas about three days.\n\nTable 4: Comparison of publicly available word vectors on the\nSemantic-Syntactic Word Relationship test set, and word vectors from our\nmodels. Full vocabularies are used.\n\nModel Vector Training Accuracy [%] Dimensionality words Semantic\nSyntactic Total Collobert-Weston NNLM 50 660M 9.3 12.3 11.0 Turian NNLM\n50 37M 1.4 2.6 2.1 Turian NNLM 200 37M 1.4 2.2 1.8 Mnih NNLM 50 37M 1.8\n9.1 5.8 Mnih NNLM 100 37M 3.3 13.2 8.8 Mikolov RNNLM 80 320M 4.9 18.4\n12.7 Mikolov RNNLM 640 320M 8.6 36.5 24.6 Huang NNLM 50 990M 13.3 11.6\n12.3 Our NNLM 20 6B 12.9 26.4 20.3 Our NNLM 50 6B 27.9 55.8 43.2 Our\nNNLM 100 6B 34.2 64.5 50.8 CBOW 300 783M 15.5 53.1 36.1 Skip-gram 300\n783M 50.0 55.9 53.3\n\nFor experiments reported further, we used just one training epoch\n(again, we decrease the learning rate linearly so that it approaches\nzero at the end of training). Training a model on twice as much data\nusing one epoch gives comparable or better results than iterating over\nthe same data for three epochs, as is shown in Table\u00a05, and provides\nadditional small speedup.\n\nTable 5: Comparison of models trained for three epochs on the same data\nand models trained for one epoch. Accuracy is reported on the full\nSemantic-Syntactic data set.\n\nModel Vector Training Accuracy [%] Training time Dimensionality words\n[days] Semantic Syntactic Total 3 epoch CBOW 300 783M 15.5 53.1 36.1 1 3\nepoch Skip-gram 300 783M 50.0 55.9 53.3 3 1 epoch CBOW 300 783M 13.8\n49.9 33.6 0.3 1 epoch CBOW 300 1.6B 16.1 52.6 36.1 0.6 1 epoch CBOW 600\n783M 15.4 53.3 36.2 0.7 1 epoch Skip-gram 300 783M 45.6 52.2 49.2 1 1\nepoch Skip-gram 300 1.6B 52.2 55.1 53.8 2 1 epoch Skip-gram 600 783M\n56.7 54.5 55.5 2.5\n\n4.4 Large Scale Parallel Training of Models\n\nAs mentioned earlier, we have implemented various models in a\ndistributed framework called DistBelief. Below we report the results of\nseveral models trained on the Google News 6B data set, with mini-batch\nasynchronous gradient descent and the adaptive learning rate procedure\ncalled Adagrad\u00a0[7]. We used 50 to 100 model replicas during the\ntraining. The number of CPU cores is an estimate since the data center\nmachines are shared with other production tasks, and the usage can\nfluctuate quite a bit. Note that due to the overhead of the distributed\nframework, the CPU usage of the CBOW model and the Skip-gram model are\nmuch closer to each other than their single-machine implementations. The\nresult are reported in Table\u00a06.\n\nTable 6: Comparison of models trained using the DistBelief distributed\nframework. Note that training of NNLM with 1000-dimensional vectors\nwould take too long to complete.\n\nModel Vector Training Accuracy [%] Training time Dimensionality words\n[days x CPU cores] Semantic Syntactic Total NNLM 100 6B 34.2 64.5 50.8\n14 x 180 CBOW 1000 6B 57.3 68.9 63.7 2 x 140 Skip-gram 1000 6B 66.1 65.1\n65.6 2.5 x 125\n\n4.5 Microsoft Research Sentence Completion Challenge\n\nThe Microsoft Sentence Completion Challenge has been recently introduced\nas a task for advancing language modeling and other NLP techniques\u00a0[32].\nThis task consists of 1040 sentences, where one word is missing in each\nsentence and the goal is to select word that is the most coherent with\nthe rest of the sentence, given a list of five reasonable choices.\nPerformance of several techniques has been already reported on this set,\nincluding N-gram models, LSA-based model\u00a0[32], log-bilinear model\u00a0[24]\nand a combination of recurrent neural networks that currently holds the\nstate of the art performance of 55.4% accuracy on this benchmark\u00a0[19].\n\nWe have explored the performance of Skip-gram architecture on this task.\nFirst, we train the 640-dimensional model on 50M words provided in\u00a0[32].\nThen, we compute score of each sentence in the test set by using the\nunknown word at the input, and predict all surrounding words in a\nsentence. The final sentence score is then the sum of these individual\npredictions. Using the sentence scores, we choose the most likely\nsentence.\n\nA short summary of some previous results together with the new results\nis presented in Table\u00a07. While the Skip-gram model itself does not\nperform on this task better than LSA similarity, the scores from this\nmodel are complementary to scores obtained with RNNLMs, and a weighted\ncombination leads to a new state of the art result 58.9% accuracy (59.2%\non the development part of the set and 58.7% on the test part of the\nset).\n\nTable 7: Comparison and combination of models on the Microsoft Sentence\nCompletion Challenge.\n\nArchitecture Accuracy [%] 4-gram\u00a0[32] 39 Average LSA similarity\u00a0[32] 49\nLog-bilinear model\u00a0[24] 54.8 RNNLMs\u00a0[19] 55.4 Skip-gram 48.0 Skip-gram +\nRNNLMs 58.9\n\n5 Examples of the Learned Relationships\n\nTable\u00a08 shows words that follow various relationships. We follow the\napproach described above: the relationship is defined by subtracting two\nword vectors, and the result is added to another word. Thus for example,\nParis - France + Italy ": "Rome. As it can be seen, accuracy is quite\ngood, although there is clearly a lot of room for further improvements\n(note that using our accuracy metric that assumes exact match, the\nresults in Table\u00a08 would score only about 60%). We believe that word\nvectors trained on even larger data sets with larger dimensionality will\nperform significantly better, and will enable the development of new\ninnovative applications. Another way to improve accuracy is to provide\nmore than one example of the relationship. By using ten examples instead\nof one to form the relationship vector (we average the individual\nvectors together), we have observed improvement of accuracy of our best\nmodels by about 10% absolutely on the semantic-syntactic test.\n\nIt is also possible to apply the vector operations to solve different\ntasks. For example, we have observed good accuracy for selecting\nout-of-the-list words, by computing average vector for a list of words,\nand finding the most distant word vector. This is a popular type of\nproblems in certain human intelligence tests. Clearly, there is still a\nlot of discoveries to be made using these techniques.\n\nTable 8: Examples of the word pair relationships, using the best word\nvectors from Table\u00a04 (Skip-gram model trained on 783M words with 300\ndimensionality).\n\nRelationship Example 1 Example 2 Example 3 France - Paris Italy: Rome\nJapan: Tokyo Florida: Tallahassee big - bigger small: larger cold:\ncolder quick: quicker Miami - Florida Baltimore: Maryland Dallas: Texas\nKona: Hawaii Einstein - scientist Messi: midfielder Mozart: violinist\nPicasso: painter Sarkozy - France Berlusconi: Italy Merkel: Germany\nKoizumi: Japan copper - Cu zinc: Zn gold: Au uranium: plutonium\nBerlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: Barack\nMicrosoft - Windows Google: Android IBM: Linux Apple: iPhone Microsoft -\nBallmer Google: Yahoo IBM: McNealy Apple: Jobs Japan - sushi Germany:\nbratwurst France: tapas USA: pizza\n\n6 Conclusion\n\nIn this paper we studied the quality of vector representations of words\nderived by various models on a collection of syntactic and semantic\nlanguage tasks. We observed that it is possible to train high quality\nword vectors using very simple model architectures, compared to the\npopular neural network models (both feedforward and recurrent). Because\nof the much lower computational complexity, it is possible to compute\nvery accurate high dimensional word vectors from a much larger data set.\nUsing the DistBelief distributed framework, it should be possible to\ntrain the CBOW and Skip-gram models even on corpora with one trillion\nwords, for basically unlimited size of the vocabulary. That is several\norders of magnitude larger than the best previously published results\nfor similar models.\n\nAn interesting task where the word vectors have recently been shown to\nsignificantly outperform the previous state of the art is the\nSemEval-2012 Task 2\u00a0[11]. The publicly available RNN vectors were used\ntogether with other techniques to achieve over 50% increase in\nSpearman\u2019s rank correlation over the previous best result\u00a0[31]. The\nneural network based word vectors were previously applied to many other\nNLP tasks, for example sentiment analysis\u00a0[12] and paraphrase\ndetection\u00a0[28]. It can be expected that these applications can benefit\nfrom the model architectures described in this paper.\n\nOur ongoing work shows that the word vectors can be successfully applied\nto automatic extension of facts in Knowledge Bases, and also for\nverification of correctness of existing facts. Results from machine\ntranslation experiments also look very promising. In the future, it\nwould be also interesting to compare our techniques to Latent Relational\nAnalysis\u00a0[30] and others. We believe that our comprehensive test set\nwill help the research community to improve the existing techniques for\nestimating the word vectors. We also expect that high quality word\nvectors will become an important building block for future NLP\napplications.\n\n7 Follow-Up Work\n\nAfter the initial version of this paper was written, we published\nsingle-machine multi-threaded C++ code for computing the word vectors,\nusing both the continuous bag-of-words and skip-gram architectures\u2074\u20744The\ncode is available at https://code.google.com/p/word2vec/. The training\nspeed is significantly higher than reported earlier in this paper, i.e.\nit is in the order of billions of words per hour for typical\nhyperparameter choices. We also published more than 1.4 million vectors\nthat represent named entities, trained on more than 100 billion words.\nSome of our follow-up work will be published in an upcoming NIPS 2013\npaper\u00a0[21].\n\nReferences\n\n-   [1]  Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic\n    language model. Journal of Machine Learning Research, 3:1137-1155,\n    2003.\n-   [2]  Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI.\n    In: Large-Scale Kernel Machines, MIT Press, 2007.\n-   [3]  T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large\n    language models in machine translation. In Proceedings of the Joint\n    Conference on Empirical Methods in Natural Language Processing and\n    Computational Language Learning, 2007.\n-   [4]  R. Collobert and J. Weston. A Unified Architecture for Natural\n    Language Processing: Deep Neural Networks with Multitask Learning.\n    In International Conference on Machine Learning, ICML, 2008.\n-   [5]  R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu\n    and P. Kuksa. Natural Language Processing (Almost) from Scratch.\n    Journal of Machine Learning Research, 12:2493-2537, 2011.\n-   [6]  J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le,\n    M.Z. Mao, M.A. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng.,\n    Large Scale Distributed Deep Networks, NIPS, 2012.\n-   [7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient\n    methods for online learning and stochastic optimization. Journal of\n    Machine Learning Research, 2011.\n-   [8]  J. Elman. Finding Structure in Time. Cognitive Science, 14,\n    179-211, 1990.\n-   [9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng.\n    Improving Word Representations via Global Context and Multiple Word\n    Prototypes. In: Proc. Association for Computational Linguistics,\n    2012.\n-   [10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed\n    representations. In: Parallel distributed processing: Explorations\n    in the microstructure of cognition. Volume 1: Foundations, MIT\n    Press, 1986.\n-   [11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak.\n    Semeval-2012 task 2: Measuring degrees of relational similarity. In:\n    Proceedings of the 6th International Workshop on Semantic Evaluation\n    (SemEval 2012), 2012.\n-   [12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C.\n    Potts. Learning word vectors for sentiment analysis. In Proceedings\n    of ACL, 2011.\n-   [13]  T. Mikolov. Language Modeling for Speech Recognition in Czech,\n    Masters thesis, Brno University of Technology, 2007.\n-   [14]  T. Mikolov, J. Kopeck\u00fd, L. Burget, O. Glembek and J. \u010cernock\u00fd.\n    Neural network based language models for higly inflective languages,\n    In: Proc. ICASSP 2009.\n-   [15]  T. Mikolov, M. Karafi\u00e1t, L. Burget, J. \u010cernock\u00fd, S. Khudanpur.\n    Recurrent neural network based language model, In: Proceedings of\n    Interspeech, 2010.\n-   [16]  T. Mikolov, S. Kombrink, L. Burget, J. \u010cernock\u00fd, S. Khudanpur.\n    Extensions of recurrent neural network language model, In:\n    Proceedings of ICASSP 2011.\n-   [17]  T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. \u010cernock\u00fd.\n    Empirical Evaluation and Combination of Advanced Language Modeling\n    Techniques, In: Proceedings of Interspeech, 2011.\n-   [18]  T. Mikolov, A. Deoras, D. Povey, L. Burget, J. \u010cernock\u00fd.\n    Strategies for Training Large Scale Neural Network Language Models,\n    In: Proc. Automatic Speech Recognition and Understanding, 2011.\n-   [19]  T. Mikolov. Statistical Language Models based on Neural\n    Networks. PhD thesis, Brno University of Technology, 2012.\n-   [20]  T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in\n    Continuous Space Word Representations. NAACL HLT 2013.\n-   [21]  T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean.\n    Distributed Representations of Words and Phrases and their\n    Compositionality. Accepted to NIPS 2013.\n-   [22]  A. Mnih, G. Hinton. Three new graphical models for statistical\n    language modelling. ICML, 2007.\n-   [23]  A. Mnih, G. Hinton. A Scalable Hierarchical Distributed\n    Language Model. Advances in Neural Information Processing Systems\n    21, MIT Press, 2009.\n-   [24]  A. Mnih, Y.W. Teh. A fast and simple algorithm for training\n    neural probabilistic language models. ICML, 2012.\n-   [25]  F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network\n    Language Model. AISTATS, 2005.\n-   [26]  D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning\n    internal representations by back-propagating errors. Nature,\n    323:533.536, 1986.\n-   [27]  H. Schwenk. Continuous space language models. Computer Speech\n    and Language, vol. 21, 2007.\n-   [28]  R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D.\n    Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for\n    Paraphrase Detection. In NIPS, 2011.\n-   [29]  J. Turian, L. Ratinov, Y. Bengio. Word Representations: A\n    Simple and General Method for Semi-Supervised Learning. In: Proc.\n    Association for Computational Linguistics, 2010.\n-   [30]  P. D. Turney. Measuring Semantic Similarity by Latent\n    Relational Analysis. In: Proc. International Joint Conference on\n    Artificial Intelligence, 2005.\n-   [31]  A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining\n    Heterogeneous Models for Measuring Relational Similarity. NAACL HLT\n    2013.\n-   [32]  G. Zweig, C.J.C. Burges. The Microsoft Research Sentence\n    Completion Challenge, Microsoft Research Technical Report\n    MSR-TR-2011-129, 2011.\n\n\u25c4 [ar5iv homepage] Feeling\nlucky? Conversion\nreport Report\nan issue View\u00a0original\non\u00a0arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Jan 28 20:46:44 2022 by LaTeXML [[LOGO]]"}